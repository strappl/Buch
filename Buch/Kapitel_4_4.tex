%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neue Neoklassische Synthese}
\label{Neue Neoklassische Synthese}

Mit diesem Kapitel sind wir in der Gegenwart der Ökonomie angekommen. Man kann zwar durchaus argumentieren, dass die Makroökonomie nach der weltweiten Wirtschaftskrise ab 2008 und der immer noch praktizierten globalen Nullzinspolitik eine erneute "`Revolution"' nötig hätte, aber Stand 2022 ist die State-of-the-Art Mainstream-Ökonomie die \textit{Neue Neoklassische Synthese}. Der Begriff "`Neue Neoklassische Synthese"' ist (noch) nicht wirklich etabliert als Bezeichnung für den aktuellen wirtschaftswissenschaftlichen "`Mainstream"'. Meist spricht man stattdessen von "`Neu-Keynesianismus"' oder auch von "`Neoklassik"'\footnote{In Lehrbüchern wird häufig ohne Unterschied vom "`Neu-Keynesianismus"' gesprochen. Auch "`Neu-Keynesianismus der 2. Generation"', "`Neue Synthese"', "`Neue Keynesianische Synthese"'  kommen vor. Selten werden die Modelle auch als "`Neo-Wicksellianisch"' bezeichnet \parencite[S. 28]{Gali2007}, dies wegen der Ähnlichkeit zur Theorie von Wicksell, der Abweichungen vom natürlichen Gleichgewicht beschreibt}. Beide Begriffe sind aber nicht eindeutig. Um Unklarheiten zu vermeiden wird hier der etwas holprige, aber eindeutige und inhaltlich meiner Meinung nach passende Begriff "`Neue Neoklassische Synthese"' (oder schlicht "`Neue Synthese"') verwendet.

Ungefähr um 1990 versuchten Ökonomen, die nicht vom Streit zwischen Neu-Klassikern und Neu-Keynesianern vorbelastet waren, unvoreingenommen das beste aus beiden Welten zu übernehmen und zu einer "`Neuen Synthese"' zusammen zuführen. Die Abgrenzung zwischen "`Neu-Keynesianismus"' und "`Neuer Synthese"' ist hierbei sowohl inhaltlich als auch zeitlich verlaufend. Vor allem, weil viele Ökonomen, die uns im letzten Kapitel untergekommen sind, auch in diesem Kapitel die "`Hauptdarsteller"' sein werden. Es gibt aber auch durchaus Abspaltungen bei den Vertretern des "`Neu-Keynesianismus"': Paul Krugman und Joseph Stiglitz, zum Beispiel, lehnen viele Ansätze der "`Neuen Synthese"' weitgehend ab. Mankiw, Blanchard und David Romer sind schwieriger einer der beiden Schulen zuzuordnen, sie stehen für den Übergang von "`Neu-Keynesianismus"' zur "`Neuer Synthese"'. Ein zentraler Vertreter der "`Neuen Sythese"' (ohne Vergangenheit im "`Neu-Keynesianismus) ist Jordi Gali. Ein spezieller Vertreter ist John Taylor. Er begründete - gemeinsam mit \textcite{Phelps1968} und \textcite{Fischer1977} - den "`Neu-Keynesianismus"' mit \parencite{Taylor1977}, und auch für die "`Neue Synthese"' lieferte er einen der grundlegenden Beiträge \parencite{Taylor1993}. Es gibt heute in der Ökonomie nicht mehr jene klar abgrenzbaren, konkurrierenden Schulen, die die Wirtschaftsgeschichte des 20. Jahrhunderts geprägt haben: Neoklassiker vs. Keynesianer, Keynesianer vs. Monetaristen, Neu-Klassiker vs. Neu-Keynesianer. Vielmehr ist die gesamte Mainstream-Ökonomie unter einem sehr breiten Dach zusammengefasst. Was aber nicht bedeutet, dass unter diesem Dach alle einer Meinung sind, ganz im Gegenteil.\footnote{Neben diesem Mainstream, gibt es immer noch mehrere heterodoxe Schulen, die im Teil \ref{Heterodox} beschrieben werden.}

Passend dazu verschwammen auch die ideologischen Unterschiede zwischen den verschiedenen ökonomischen Gruppen. Konnte man bis in die 1990er Jahre hinein die ökonomischen Richtungen meist auch einer politischen Richtung zuweisen, ist dies heute nicht mehr möglich. Sozialdemokraten (Kontinental-Europa), Labour-Party (UK) und Demokraten (USA) waren fast ausschließlich dem (Neu-) Keynesianismus zugeneigt. Christ-Demokraten (Kontinental-Europa), Tories (UK) und Republikaner (USA) meist den Neoklassikern,  Monetaristen und Neuen Klassikern. Das Spektrum der Vertreter der Neuen Synthese reicht vom Erzliberalen John Taylor über den bekennenden Republikaner Mankiw bis zu Janet Yellen, die Finanzministerin im Kabinett des demokratischen US-Präsidenten Joe Biden ist. 

Bevor wir uns die rein ökonomischen Aspekte der "`Neuen neoklassischen Synthese"' im Detail ansehen, blicken wir auf das Umfeld. Welchen Herausforderungen waren die Volkswirtschaften Anfang der 1990er Jahre ausgesetzt? Politisch gesehen war natürlich der Zusammenbruch der Sowjetunion und damit des real existierenden Sozialismus dominierend. Die Marktwirtschaft, also die Grundlage fast aller in diesem Buch beschriebenen Ideen, hatte sich durchgesetzt. Der Kommunismus, der ohnehin nie so wie von Marx beschrieben praktiziert wurde, galt endgültig als gescheitert. In den westlichen Marktwirtschaften trat das Problem der Inflation in den Hintergrund. Dafür traten Probleme der Arbeitslosigkeit in den Vordergrund, in Europa stärker ausgeprägt als in den USA. Das Problem der steigenden Staatsschulden wurde zunehmend thematisiert, mit ein Grund warum Fiskalpolitik aus dem Fokus geriet. Wechselkurssysteme waren Anfang der 1990er zwar noch einmal ein Thema, als sich nacheinander mehrere europäische Zentralbanken dem Treiben von Spekulanten ausgesetzt sahen, die versuchten die fixierten Wechselkurse zu manipulieren. Doch mit der Schaffung der Europäischen Wirtschafts- und Währungsunion, die in der Einführung des Euros gipfelte, verlor diese Thematik an Bedeutung. Technologisch Begann mit den frühen 1990er Jahren das Zeitalter der Computer. Rechenmaschinen wurden für Haushalte und Kleinunternehmen leistbar und veränderten damit auch das zentrale Verwaltungssystem. Damit verbunden waren wesentliche Verbesserungen in der Datenverfügbarkeit und der Datenauswertung. Die in diesem Buch nicht gesondert behandelte Ökonometrie, sowie die empirische Wirtschaftswissenschaft erfuhr in dieser Zeit einen enormen Aufschwung. Das ist nicht unwesentlich bei der nun folgenden Betrachtung der Weiterentwicklung der Makroökonomie. Die modernen DSGE-Modelle, die im folgenden erläutert werden, sind nur numerisch und damit mit hohem Rechenaufwand zu lösen. 

Zurück zur Entwicklung der Ökonomie: Es gibt vor allem zwei Punkte, die sich Anfang der 1990er-Jahre entwickelt haben und die Makroökonomie und deren Wirtschaftspolitik seither eindeutig prägen und sehr wohl eine eindeutige Abgrenzung vom Neu-Keynesianismus und der Neuen Klassik ermöglichen:
\begin{enumerate}
	\item Erstens, in der makroökonomischen Theorie: die Zusammenführung der formal-mathematischen Real-Business-Cycle-Gleichgewichtsmodelle mit Elementen der Neu-Keynesianer.
	\item Zweitens, in der Wirtschaftspolitik: Die Dominanz der Bedeutung der Geldpolitik und der Aufstieg der Zentralbanken zum wichtigsten wirtschaftspolitischen Player.
\end{enumerate}



\section{Die Verwissenschaftlichung der Zentralbanken}

\subsection{Exkurs: Die Evolution der Zentralbanken}
Vom Verwalter des Goldstandards, zum Hüter der Wechselkurse, zum Spieler gegen Spekulanten, zur zentralen Player der Wirtschaftspolitik
HIER WEITER


Evolution: Geldmengenziele, hin zum impliziten und expliziten Inflation-targeting.

Kosten der Inflation, empirische Untersuchungen

Unabhängigkeit der Zentralbanken

Romer-Buch S. 637ff


\section{DSGE: Die Zweckehe zwischen "`Neuen Klassikern"' und "`Neu-Keynesianern"'}

Dieses Kapitel rechtfertigt den Begriff "`Neue Neoklassische \textit{Synthese}"' wie kein anderes. \textit{Die} wesentliche Erweiterung in der Makroökonomie in den 1990er Jahren, war die Entwicklung der ersten \textit{Dynamic, Stochastic, General Equilibrium}-Modelle (DSGE-Modelle). Wie wir im Kapitel \ref{Neue Makro} gelesen haben, wurde durch die Arbeiten von \textcite{Kydland1982, Plosser1983} die Methodik der Makroökonomie geradezu revolutioniert. Die dabei entwickelten Real-Business-Cycle-Modelle waren aus mathematisch-modelltheoretischer Sicht den in den 1970er Jahren vorherrschenden Keynesianischen und später auch Monetaristischen Totalmodellen, weit überlegen. RBC-Modelle sind walrasianische Gleichgewichtsmodelle, welche die damals neu entstandene Mikrofundierung der Makroökonomie umsetzten. Ihre mathematische Eleganz passte perfekt in die Zeit der Formalisierung der Ökonomie. Wenngleich die Lösung dieser Modelle, mangels Möglichkeit das Modell analytisch zu berechnen, ein paar "`Tricks"' erforderte. Die RBC-Modelle hatten nur einen Haken: Sie scheiterten grandios darin die Empirie gut abzubilden \parencite[S. 309]{Romer2019}. Eigentlich ein Ausschlusskriterium für ökonomische Modelle. Die Methodik galt aber dennoch als zukunftsträchtig. 

Als Antwort auf diese "`Neu-klassischen"'-Modelle, entwickelten die "`Neu-Keynesianer"' punktuell Ansätze um empirisch beobachtete ökonomische Auffälligkeiten zu erklären. Diese Elemente wurden im letzten Kapitel (\ref{cha: Neu Keynes}) beschrieben. Die "`Verehelichung"' zwischen der RBC-Methodik und Neu-Keynesianischen Lösungsansätzen führte schließlich zu den ersten DSGE-Modellen \parencite[S. 5]{Gali2015}. Konkret wurde das mathematische RBC-Framework um folgende Neu-Keynesianische Elemente erweitert:
\begin{itemize}
	\item "`Nominale Rigiditäten"'
	\item "`Monopolistische Konkurrenz"'
	\item "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"'   
\end{itemize}

Die Kombination der "`Neu-Keynesianischen"' Ideen mit dem "`Neu-Klassischen"' Modellansatz begründet damit den Namen "`Neue Neoklassische Synthese"'\footnote{Dies ist aber insofern etwas verwirrend, als die meisten Entwickler von DSGE-Modellen, diese als "`Neu-Keynesianisch"' bezeichnen. Z.B.: \textcite{Gali2015}, \textcite{Romer2019}. \textcite[S. 28]{Gali2007} meint richtigerweise, dass sich der Begriff "`Neu-Keynesianisch"' weitgehend durchgesetzt hat, räumt aber ein, dass dieser Begriff eigentlich unzureichend ist!}.

Das Zusammenführen der beiden Ansätze verlief weniger geradlinig und einfach, als im letzten Absatz dargestellt. So gibt es verschiedene Ansätze \parencite[S. 310]{Romer2019} wie die dynamische Preisanpassung modelliert wird, was wiederum sehr unterschiedliche Auswirkungen der mikroökonomischen Rigiditäten auf die makroökonomischen Ergebnisse verursacht. Die DSGE-Modelle wurden zudem laufend erweitert. Insbesondere die Anfang der 1990er Jahre ebenfalls neuen Erkenntnisse, wie Geldpolitik praktiziert wird (siehe Unterkapitel \ref{Taylor}), sowie die Berücksichtigung der "`Neuen Phillipskurve"' (siehe Unterkapitel \ref{NeuePhillips}) zur Modellierung von Arbeitslosigkeit, waren wesentliche Weiterentwicklungen des Grundmodells und sind heute wichtige Bausteine der DSGE-Modelle.

Konkret werden die drei soeben angeführten Neu-Keynesianischen Elemente durch zwei Annahmen eingeführt. Erstens, jedes Unternehmen produziert ein spezielles Produkt, für das es den Preis \textit{setzt}. Damit ist die Annahme eines monopolistischen Konkurrenzmarktes erfüllt. Zweitens, nominale Preisrigiditäten werden modelliert, indem man annimmt, dass jedes Unternehmen nur zu einem bestimmten Zeitpunkt seine Preise an das Gleichgewicht anpassen kann \parencite[S. 52]{Gali2015}. Im Kapitel \ref{cha: Neu Keynes} wurden drei verschiedene Ansätze erwähnt, wie man dies simulieren kann. Jenen von \textcite{Fischer1977} und \textcite{Taylor1977}, jenen von \textcite{Taylor1979} und schließlich jenen von \textcite{Calvo1983}. Man kann heute sagen, dass sich der Ansatz von \textcite{Calvo1983} durchgesetzt hat. Dieser kann aus mathematischen Gründen am einfachsten in das DSGE-Framework integriert werden. Zur Erinnerung: Welches Unternehmen seinen Preis anpassen darf, wird hier durch eine Poisson-Verteilung, also durch einen Zufallsprozess, bestimmt. Die Annahme der "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"' ist durch die zwei eben genannten Elemente mitumfasst, denn die Preise sind im Ergebnis eben keine walrasianischen Gleichgewichtspreise mehr. Das heißt aber auch, dass sich auch der Geldwert nicht ständig an sein wahres Gleichgewicht anpasst. Diese Abweichung vom Gleichgewicht kann eben für Geldpolitik genutzt werden.

Die drei folgenden Unterkapitel stellen die "`Bausteine"' der "`Dynamisch Stochastischen General Equilibrium"' (DSGE) Modelle dar. Diese sind ursprünglich alleinstehend entwickelt worden. 

\subsection{Die Neu-Keynesianische IS-Kurve}
\label{NeueIS}

Das berühmte IS-LM-Modell hat bis heute in den einführenden VWL-Büchern überlebt. Vor allem als Instrument der kurzfristigen Gleichgewichtsanalyse. Natürlich gilt das Modell heute nicht mehr als zeitgemäß: Inflation spielt darin keine Rolle, des unterscheidet nicht zwischen realen und nominalen Zinsen. Von Mikrofundierung war zu seiner Entstehung im Jahr 1937 noch lange nicht die Rede. Mitte der 1990er Jahre stellten sich Ökonomen schließlich die Frage, wie man die aggregierte Nachfrage zukunftsorientiert darstellen könnte. Das Optimierungsverhalten der Haushalte ist schon aus dem Kapitel \ref{Arrow-Debreu} bekannt. Im Ramsey-Cass-Koopmans-Modell liefert Konsum positiven und Arbeit negativen Nutzen. Konsum kann mittels Realzinssatz von einer Periode in die nächste übertragen werden. \textcite{Kerr1996, McCallum1999} leiteten daraus eine zukunftsorientierte \textit{Neu Keynesianische IS-Kurve} ab. Dazu stellen sie fest, dass man unter der Prämisse rationaler Erwartungen von \textit{einem} repräsentativen Haushalt ausgeht. Im Gleichgewicht gilt nach wie vor, dass Konsum gleich Gesamtoutput (BIP) gilt. Die Ramsey-Cass-Koopmans-Konsumtheorie wird damit so umgestellt, dass man das aktuelle BIP durch das erwartete, zukünftige BIP erklärt. Zusätzlich wird jener Anteil am Einkommen, den man in spätere Perioden verschiebt und dafür Zinsen generiert, vom aktuellen BIP abgezogen. Je höher der Realzinssatz, desto höher wird der Anteil sein, den man erst in zukünftigen Perioden konsumiert. Das ist auch der entscheidende Punkt der Neu-Keynesianischen IS-Kurve: Es gibt einen negativen Zusammenhang zwischen BIP und Realzins und die Erklärung des Gesamtoutputs (BIP) ist zukunftsorientiert. Vom Ergebnis her unterscheiden sich die Neu-Keynesianische IS-Kurve und die traditionellen IS-Kurve auf den ersten Blick nur wenig: In beiden Fällen gibt es den negativen Zusammenhang zwischen BIP und Realzins. Von der Idee her \parencite[S. 241]{Romer2019} unterscheiden sich beide deutlich: Bei der Neu-Keynesianischen IS-Kurve reagiert der \textit{Konsum} negativ auf den Realzins, während die Investitionen in der Herleitung gar nicht vorkommen. Im traditionellen Modell ist es genau umgekehrt, \textit{IS} steht hierbei ja auch für \textit{I}nvestition und \textit{S}paren. 
Ein kleines Gedankenexperiment aus \textcite[S. 243]{Romer2019} soll vorweg andeuten, warum sich diese Neu-Keynesianische IS-Kurve so gut eignet, Nominale Rigiditäten (vgl. Kapitel \ref{Nominale Rigiditäten}) in ein Gleichgewichtsmodell zu implementieren. Erinnern sie sich an das traditionelle IS-LM-Modell. Die IS-Kurve hat eine negative Steigung, die LM-Kurve eine positive Steigung. (Zur Vorstellung: jeweils durchgehende 45-Grad-Linien eignen sich gut). Die LM-Kurve bildet die Geldpolitik ab, auf den Achsen sind der Zinssatz (y-Achse), bzw. der Output (x-Achse) abgebildet. Jetzt das Gedankenexperiment: Angenommen die Geldmenge wird schlagartig (in t=0) ausgeweitet und im kommenden Jahr (t=1) wieder auf das Ursprungsniveau zurückgefahren. Die neue IS-Kurve hängt vom \textit{zukünftigen (t=1)} Output ab. Dieser bleibt ja gleich, da die Geldmenge in Zukunft (t=1) wieder zurückgefahren wird. Allerdings hängt die IS-Kurve auch vom aktuellen (t=0) Zinssatz ab. Der fällt vorübergehend durch die vorübergehende Geldmengenausweitung. Im Resultat wirkt sich dieser Geldmengenschock kurzfristig, nämlich in (t=0) positiv auf den Output aus, mittelfristig (bereits in t=1) ist das ursprüngliche Gleichgewicht aber wieder hergestellt. Die Neu-Keynesianische IS-Kurve hat also Eigenschaften, mit denen man die Auswirkungen von Nominalen Rigiditäten darstellen kann. Konkret nämlich, dass Geldpolitik\footnote{Ebenso ist Fiskalpolitik kurzfristig wirksam: Das gleiche Gedankenexperiment liefert für temporär wirksame Staatsausgaben die selben Ergebnisse.} kurzfristig wirksam ist. Damit liegt mit der Neu-Keynesianischen IS-Kurve ein Modell vor, dass den Forderungen der Neuen Klassiker (vgl.: Kapitel \ref{Neue Makro}), siehe Lucas-Kritik, entspricht und gleichzeitig das Neu-Keynesianische Element der Nominalen Rigiditäten berücksichtigt.

\subsection{Die erneute Auferstehung der Phillips Kurve}
\label{NeuePhillips}
George Akerlof nannte die Phillips-Kurve in seiner Nobelpreis-Rede im Jahr 2001 "`probably the single most important macroeconomic relationship"' \parencite{Nobelpreis-Komitee2001}. Dabei ist die Phillips-Kurve seit jeher umstritten und ständigem Wandel unterworfen. Die ursprüngliche Phillips-Kurve postulierte einen \textit{langfristigen} und stabilen Zusammenhang zwischen Inflation und Arbeitslosigkeit. In Kapitel \ref{cha: Neu Keynes} führten wir die "`erwartungsgestützte Phillips-Kurve"' ein, basierend auf adaptiven Erwartungen gilt der postulierte Zusammenhang nur mehr in der kurzen Frist. Arbeitslosigkeit hängt also von der \textit{Inflation der Vorperiode} ab. In Kapitel \ref{Neue Makro} war eine Folge der Lucas-Kritik, dass der Zusammenhang zwischen Arbeitslosigkeit und Inflation von den \textit{aktuellen Inflationserwartungen} abhängt. Da alle Akteure rationale Erwartungen haben, existiert ein entsprechender Zusammenhang nicht, außer die Wirtschaftspolitik tätigt unvorhersehbare Maßnahmen. In den 1990er Jahren schließlich verwarf man die radikalen Ideen der "`Neu-Klassiker"' diesbezüglich weitgehend und es wurde eine "`Neu Keynesianische Phillips Kurve"' postuliert. Diese ist mikrofundiert, basiert auf rationalen Erwartungen ist aber rein zukunftsorientiert. Das heißt die akutelle Arbeitslosigkeit hängt darin von den \textit{aktuellen Erwartungen der zukünftigen Inflation} ab \parencite[S. 980]{Roberts1995}. Mittlerweile spricht man übrigens meist vom Zusammenhang zwischen Inflation und Output (also BIP), durch den empirisch gut abgesicherten Zusammenhang zwischen BIP und Arbeitslosigkeit ist dies allerdings nur eine simple Erweiterung.

Unter der Annahme rationaler Erwartungen, bricht der postulierte Zusammenhang eigentlich zusammen, durch nominale Rigiditäten und damit verbundenen Preisanpassungsprozessen, muss man die Phillips-Kurve aber wieder in Betracht ziehen. Im Kapitel \ref{cha: Neu Keynes} haben wir die Arbeit von \textcite{Calvo1983} bereits dazu herangezogen nominale Rigiditäten zu modellieren. Und aus dieser Arbeit wurde auch tatsächlich die "`Neu-Keynesianische Phillips Kurve"' direkt formal-mathematisch abgeleitet. Dies ist einigermaßen interessant, da die ursprüngliche Phillips-Kurve ja ausschließlich auf empirischen Beobachtungen basierte.

Historisch gesehen hat \textcite{Rotemberg1982} ein äquivalentes Konzept zur "`Neu-Keynesianische Phillips Kurve"' bereits vor \textcite{Calvo1983} entwickelt. In \textcite{Roberts1995} geht der Autor auf die beiden eben genannten Arbeiten direkt ein und leitete eine Formel ab, die er - erstmals ausdrücklich - "`Neu-Keynesianische Phillips Kurve"' nannte \parencite[S. 979]{Roberts1995}. 

Die genannten Unterschiede erscheinen gering, schließlich lautet die Hauptaussage immer noch, dass es einen kurzfristigen negativen Zusammenhang zwischen BIP (oder Arbeitslosigkeit) und Inflation gibt. Wenn wir die drei modernen Formen aber vergleichen, fällt auf, dass sich die Grundaussage wesentlich ändert: Bei der "`erwartungsgestützte Phillips-Kurve"' geht man davon aus, dass eine (aktuelle) sinkende Inflation mit niedrigeren BIP-Wachstumsraten verbunden ist. Nach den Neu-Klassikern wirkt sich eine Änderung der Inflationsraten auf die Realwirtschaft, also die BIP-Wachstumsraten real gar nicht aus. Bei der "`Neu-Keynesianische Phillips Kurve"' geht man davon aus, dass die sinkende Inflation ja eine reine zukunftsbezogene Erwartung ist, demnach lautet die Grundaussage, dass sich die Ökonomie in t0 in einer Hochkonjunkturphase befindet. Mit anderen Worten: Das Vorzeichen ist falsch, oder zumindest nicht so wie erwartet. Konkret passt dies nicht zusammen mit der empirisch und theoretisch recht gut abgesicherten Annahme der "`Inflationsträgheit"' (Inflation Inertia). Hierbei geht man davon aus, dass einer hohen Inflation nur mittels schmerzhafter, also wachstumshemmender Maßnahmen, beizukommen ist.
Diese Unzulänglichkeit der Neu-Keynesianischen Phillips-Kurve macht das Konzept insgesamt umstritten. \textcite{Gali1999} nahmen Anpassungen vor, indem sie nicht nur die Inflationserwartungen, sondern auch historische Inflationswerte einfließen ließen. Wobei empirische Ergebnisse der beiden Autoren in der Folge ergaben, dass die zukunftsgerichteten Erwartungswerte der Inflation bessere Ergebnisse liefern, also historische Werte. Der Output wurde zudem durch die Lohnquote der funktionalen Einkommensverteilung ersetzt. Diese steigt nämlich empirisch während Rezessionsphasen. Letztlich ein Kunstgriff, der die Theorie nicht wesentlich besser macht. \textcite{Rudd2005, Rudd2006} zeigten empirisch die Unzulänglichkeiten der "`Neu-keynesianischen Phillips-Kurve"' und die Gültigkeit der Inflationsträgheit. Unumstritten ist die faktische Unmöglichkeit den Output-Gap, also die Differenz zwischen BIP und potenziellem BIP, zu messen. Da dieser ein wichtiger Bestandteil der "`Neu-keynesianischen Phillips-Kurve"' ist, sind empirische Untersuchungen schwierig durchzuführen. Die Diskussion zur Gültigkeit der "`Neu-keynesianischen Phillips-Kurve"' ist also schwer zu lösen. Aber es ist doch "`State of the Art"', dass diese Gleichung ein Schwachpunkt der ursprünglichen DSGE-Modelle ist \textcite{Mankiw2002} und \textcite{Christiano2005} haben auch tatsächlich Modelle entwickelt, die erfolgreich Inflationsträgheit berücksichtigen, diese sind aber schwer in das übliche DSGE-Framework zu integrieren und haben auch andere Nachteile, wie wir im Unterkapitel \ref{ErweiterungDSGE} sehen werden. 

Insgesamt führt dies zu der etwas unbefriedigenden Situation, dass die "`Neu-keynesianische Phillips-Kurve"' empirisch problembehaftet ist, aufgrund ihrer Einfachheit und formaler Eleganz wird sie aber nach wie vor in DSGE-Modellen sehr häufig verwendet \parencite[S. 341]{Romer2019}.


\subsection{Taylor-Rule: Ein pragmatischer Zugang zur Geldpolitik}
\label{Taylor}
Die Mainstream-Ökonomie kam Anfang der 1990er Jahre weitgehend darin überein, dass Märkte in der Regel nicht vollkommen reibungslos funktionieren. Wäre dies der Fall wäre aktive Wirtschaftspolitik wirkungslos und Konjunkturschwankungen wären rein zufällig, wie im Real-Business-Cycle-Framework dargestellt. Wie \textcite[S. 823]{Akerlof1985} beschrieb, gingen die Neu-Keynesianer also davon aus, dass Wirtschaftspolitik im Allgemeinen und Geldpolitik im Speziellen \textit{nicht} wirkungslos sind.

Die Geldpolitik ist uns ja schon des öfteren in diesem Buch untergekommen. Die Keynesianer legten den Fokus auf Fiskalpolitik. Friedman machte die Geldpolitik, und insbesondere die Gültigkeit der Quantitätstheorie des Geldes, wieder salonfähig. Keynesianer und Monetaristen "`einigten"' sich schließlich darauf, dass ein Policy-Mix aus Fiskal- und Geldpolitik zu optimalen Ergebnissen führt. \textcite{Friedman1960} (vgl. Kapitel \ref{Monetarismus}) plädierte schließlich in der Geldpolitik für ein jährliches "`Mengenziel"'. Die Zentralbanken sollten die Geldmenge mit einer konstanten Rate wachsen lassen. Tatsächlich folgten Zentralbanken diesen Rat vor allem in den 1980er Jahren. Das Resultat dieser Geldmengensteuerung war aber in der Empirie wenig befriedigend. Offensichtlich änderte sich die Umlaufgeschwindigkeit des Geldes mit dem Effekt, dass Inflation und der kurzfristige Zinssatz nicht einem stabilen Pfad folgten, sondern im Gegenteil, stark schwankten. Zentralbanken gingen daher dazu über direkt ihre Zielgröße, nämlich den kurzfristigen Zinssatz, zu steuern. Dazu passen sie ihr Geldangebot so an, dass sich das Marktgleichgewicht bei entsprechender Geldnachfrage genau beim gewünschten, kurzfristigen Zinssatz\footnote{Was ist mit "`kurzfristigem Zinssatz"' gemeint? Hier kommt es oft zu Missverständnissen. Dies ist \textit{nicht} der Leitzins, oder ein anderer Zentralbankenzinssatz. Tatsächlich handelt es sich um den Zinssatz, zu dem sich Finanzinstitutionen "`über Nacht"' gegenseitig liquide Mittel leihen. In Europa entspricht dies also dem "`EONIA"' (bzw. ESTR), in den USA ist dies die "`Federal funds rate"'} einpendelt. Bleibt die Frage, \textit{welchen} kurzfristigen Zinssatz die Zentralbank wählen soll, um den gewünschten, stabilen Wachstumspfad zu erreichen? Und hier kommen die "`Zinssatz-Regeln"', die allerdings meist "`Taylor-Rule"' bezeichnet werden, ins Spiel.

Benannt sind diese nach \textit{John Brian Taylor}. Der Stanford-Professor ist ein sehr interessantes Individuum. Taylor ist eigentlich eher dem erzliberalem Spektrum zuzuordnen. Unter anderem war er Vorsitzender der Mont-Pelerin-Gesellschaft von 2018 bis 2020. Etwas im Widerspruch dazu steht aber, dass er einer der ersten Ökonomen war, der die Existenz und Bedeutung von Nominalen Rigiditäten beschrieb. Er wurde diesbezüglich schon in Kapitel \ref{cha: Neu Keynes} des öfteren zitiert. Er stellte sich damit eben \textit{gegen} die "`Neuen Klassiker"' - die ganz im Sinne des ökonomischen Liberalismus - postulierten, dass Wirtschaftspolitik wirkungslos sei. Anfang der 1990er Jahre schließlich trug er maßgeblich zum theoretischen Verständnis der modernen Zentralbankensteuerung bei. Was sich schließlich als zentraler Baustein in den Modellen der "`Neuen neoklassischen Synthese"' erweisen sollte.

Der bahnbrechende Artikel \textcite{Taylor1993} \textit{begründete} nicht die Zinssteuerung in Zentralbanken - tatsächlich waren viele Zentralbanken schon vor der Veröffentlichung des Artikels zur Zinssteuerung übergegangen - sondern versuchte eine formale Regel zu finden, wie Zentralbanken den kurzfristigen Zins bestimmen sollten. Er schlug tatsächlich eine sehr einfache Formel vor \parencite[S. 202]{Taylor1993}. Normalerweise wird auf die Darstellung von mathematischen Formeln in diesem Buch bewusst verzichtet, aber in diesem Fall hat die Darstellung auch für nicht mathematisch versierte Leser nur Vorteile\footnote{Die im Fließtext angeführte Formel ist tatsächlich jene die \textcite{Taylor1993} verwendete. Verallgemeinert wird sie heute meist (z.B.: \textcite[S. 609]{Romer2019}) so dargestellt: $i_t = r^n + \phi_{\pi}*(\pi_{t} - \pi*) + \phi_y * (ln Y_t - lnY^{n}_{t})$. Dadurch wird der lineare Zusammenhang zwischen Zielvariable und Inflation und der log-lineare Zusammenhang zwiscehn Zeilvariable und BIP-Abweichung deutlich gemacht. Außerdem werden die Konstanten Zielinflation \textit{p} = 2\% und Real-Zinssatz = 2\% durch Variable \textit{r} und $\pi*$ ersetzt, sowie  $\phi_{\pi}$ und $\phi_y$ statt fixer Werte}:

$$ r = p + 0,5y + 0,5 *(p-2) + 2 $$

Dabei ist \textit{r} eben die Zielvariable \textit{kurzfristiger Zinssatz}. \textit{p} bildet die Inflationsrate ab. Und \textit{y} ist die Abweichung des \textit{tatsächlichen realen BIP} vom \textit{potentiellen realen BIP}. Wenn das tatsächliche BIP unter dem potentiellen BIP liegt, so wird der Wert negativ, andernfalls positiv. Zur Erklärung: Taylor geht in seiner Arbeit davon aus, dass das BIP langfristig mit einer konstanten Wachstumsrate von ca. 2\% pro Jahr wächst. Dies ist konform mit allgemeinen Gleichgewichtstheorien und insbesondere auch mit Real Business Cycle-Modellen.


 Gehen wir zum Verständnis der Funktion dieser einfachen Zinsregel drei Beispiele durch:
\begin{enumerate}
	\item Inflation: \textit{p} = 4. Abweichung vom potentiellen BIP: \textit{y} = 4. \\
	Ergibt:	$ 4 + 0,5*4 + 0,5 *(4-2) + 2 = 9$ \\
	Die Zentralbank würde den Zinssatz also auf 9\%, bzw. real 5\% festlegen.
	\item Inflation: \textit{p} = 2. Abweichung vom potentiellen BIP: \textit{y} = 0. \\
	Ergibt:	$ 2 + 0,5*0 + 0,5 *(2-2) + 2 = 4$ \\
	Die Zentralbank würde den Zinssatz also auf 4\%, bzw. real 2\% festlegen.
	\item Inflation: \textit{p} = 1. Abweichung vom potentiellen BIP: \textit{y} = -4. \\
	Ergibt:	$ 1 + 0,5*-4 + 0,5 *(1-2) + 2 = 0,5$ \\
	Die Zentralbank würde den Zinssatz also auf 0,5\%, bzw. real -0,5\% festlegen.
	
\end{enumerate}

Im Modell von Taylor wird eine Zielinflation von 2\%, sowie ein stabiler Real-Zinssatz von 2\% implizit angenommen. 
Wann dann die Inflation tatsächlich 2\% beträgt und es auch zu keinen Abweichungen vom langfristigen Wachstumspfad (Das BIP wächst mit ca. 2\%; \textit{y = 0}) kommt, dann bleibt der Zinssatz bei 4\% und das BIP wächst, vorbehaltlich zukünftiger exogener Schocks, konstant entlang des langfristigen Wachstumspfads. Dieser Gleichgewichtszustand wird im zweiten Beispiel oben dargestellt.
Das erste Beispiel zeigt, was passieren sollte, wenn die Wirtschaft droht zu "`überhitzen"'. Dies ist dann der Fall wenn die Inflation höher ist als 2\% und/oder das potentielle BIP übertroffen wird. Hier sollte der sehr hohe kurzfristige Zinssatz von 9\% angestrebt werden. Das Geldangebot müsste dementsprechend im Verhältnis zur hohen Geldnachfrage - davon kann man aufgrund der Tatsache, dass Inflation wie auch BIP über den Zielwerten liegen ausgehen - recht gering gehalten werden.
Das dritte Beispiel zeigt schließlich, wie die Notenbank stimulierend eingreifen kann. Da möchte sie in der Regel, wenn das tatsächliche BIP hinter seinem Potential zurück geblieben ist und/oder die Inflation unter der angenommenen Zielrate von 2\% liegt. Ein Zinssatz von real unter 0\% sollte für steigenden Nachfrage nach Geld sorgen. Aufgrund des niedrigen angestrebten Nominalzinssatzes von 0,5\% wäre auch das zur Verfügung gestellte Angebot dementsprechend hoch.

Die Ursprungsversion der Taylor-Rule hat einige Ungenauigkeiten, denen sich Taylor auch bewusst war. Erstens geht Taylor davon aus, dass der natürliche, reale Zinssatz - bei dem die Ökonomie auf einem stabilen Wachstumskurs bleibt ohne zu überhitzen, bzw. nicht ausgelastet ist - langfristig konstant und bekannt ist. Beides ist zumindest fraglich. Zweitens, werden die Eingangsvariablen BIP, potentielles BIP und Inflation als bekannt angenommen, was vor allem beim potentiellen BIP alles andere als klar ist. Diese beiden Probleme können nie zur Gänze gelöst werden, aber durch den Fortschritt vor allem bei empirischen Arbeiten, können die Parameter heute zumindest weiter eingeschränkt werden als Anfang der 1990er Jahre. Ein Modell-theoretisches Problem ist drittens, die Berücksichtigung der Inflation: \textcite[S. 211]{Taylor1993} selbst beschrieb bereits, dass Inflation nicht einfach - wie in der von ihm formulierten Formel - als Vergangenheitswert berücksichtigt werden sollte, sondern stattdessen als Kombination aus durchschnittlichen Vergangenheitswerten und erwarteten Inflationsraten. Eine wichtige Erweiterung, die in späteren Zinsregeln Anwendung fand. Später werden wir sehen, dass in DSGE-Modellen alle Parameter sogar rein zukunftsorientiert sind, dementsprechend werden heute häufig ausschließlich die Inflations\textit{erwartungen} als Parameter für Zinsregeln verwendet. Eine bis heute häufig verwendete Umsetzung hierfür kommt von \textcite[S. 150ff]{Gali2000}. Eine vielbeachtete Arbeit zu Zinsregeln ist auch \textcite{Woodford2001}. Darin räumt der Autor modelltheoretische Bedenken, die Ende der 1990er Jahre aufgetreten sind, weitgehend aus. So wurden Zweifel aufgeworfen, ob eine reine Zinsregel dem Ziel ein stabiles Gleichgewicht zu erreichen überhaupt sinnvoll sein kann? So könnte es zum Beispiel sein, dass ein unvorhergesehener Anstieg der Inflationserwartungen dazu führt, dass die Zinsregel einen nominalen Zinssatz vorschlägt, der in einem  Realzinssatz resultiert, der niedriger ist als der Gleichgewichtszinssatz. Dieser Zinssatz würde nicht, wie gewünscht restriktiv wirkend, sondern expansiv. Das Ergebnis wären steigender Output, steigende Inflation und steigende Inflationserwartungen. Das Ergebnis wäre eine immer weiterführende, die Inflation stimulierende, Spirale, also das Gegenteil vom stabilen Gleichgewicht. Eine zweite kritische Frage ist, ob die verwendeten Bestimmungsfaktoren Inflation und BIP-Abweichung überhaupt als Zielfaktoren der Geldpolitik geeignet sind? \textcite{Woodford1999} zeigt für beide Fragen formal, dass Zinsregeln sehr wohl stabilisierend wirken, räumt aber ein, dass vor allem bei der BIP-Abweichung Probleme dahingehend auftauchen, welche zukunftsgerichteten Werte tatsächlich verwendet werden sollen. Michael Woodford ist übrigens jener Ökonom, der das Framework der "`Neuen Neoklassischen Synthese"' lieber "`Neo-Wicksellianische Schule"' nennt. Tatsächlich finden sich zwischen Taylor-Rule-Konzept und der Arbeit von Wicksell (vgl. Kapitel \ref{Austria}) interessante Parallelen, nämlich in der postulierten, stabilisierenden Wirkung des Zinssatzes. Tatsächlich hat schon \textcite{Wicksel1898} die Idee aufgebracht, dass die sich Ökonomie bei einem bestimmten natürlichen Zinssatz im Gleichgewicht befindet. Bei Abweichungen davon könnte man also mittels Erhöhung bzw. Reduktion des Zinssatzes gegensteuern. 

Wie zu Beginn dieses Kapitels bereits angedeutet, erfuhr die \textit{empirische} Wirtschaftsforschung in den 1990er Jahre einen enormen Aufschwung. So wurde auch die Taylor-Rule mittels Datenanalyse auf ihre empirische Robustheit überprüft. \textcite{Taylor1999} war auch hierbei einer der ersten. Er analysierte Zeitreihen für die USA, die zurück bis 1979 reichten. Die Arbeit ist auch deswegen interessant, weil Taylor darin explizit darauf hinweist, dass Zinsregeln nicht im Widerspruch zur Quantitätsgleichung des Geldes stehen (vgl. Kapitel \ref{Monetarismus}), sondern, im Gegenteil, sogar daraus abgeleitet werden können\parencite[S. 322f]{Taylor1999}. Ein zweiter Artikel - der in diesem Kapitel wegen seiner umfassenden theoretischen und empirischen Bedeutung noch öfter zitiert wird - ist jener von \textcite{Gali2000}. Die beiden Artikel kommen zum gleichen zentralen Ergebnis: In den 1960er und 1970er Jahren waren die realen kurzfristigen Zinssätze deutlich geringer als in den 1980er und 1990er Jahren. Zur Erinnerung der erstgenannte Zeitraum war geprägt von hohen Inflationsraten. Nachdem Paul Volcker 1979 Gouverneur der Fed wurde, führte er, zur Inflationsbekämpfung, eine restriktive Zinspolitik ein und folgte damit, retrospektiv betrachtet, einer Taylor-Rule. Tatsächlich waren die 1980er und 1990er Jahre geprägt von niedriger Inflation und recht stabilen Wachstumsraten.  Aus heutiger Sicht sind die Lobgesänge auf die sogenannte Volcker-Greenspan-Ära, auf die sich \textcite{Taylor1999} und \textcite{Gali2015}, in Anlehnung an die damaligen Fed-Vorsitzenden, beziehen, etwas vorsichtiger zu sehen. Folgte doch bereits im März 2000 mit dem Platzen der sogenannten "`Dot-Com"'-Blase eine große Finanzkrise\footnote{Anmerkung: Das Entstehen von Überbewertungen auf Finanzmärkten, wird häufig mit fehlerhafter Geldpolitik im Vorfeld in Verbindung gebracht}. Insgesamt werden die empirischen Ergebnisse - vor allem von Taylor selbst - als Bestätigung dafür gesehen, dass erstens, Geldpolitik dann besonders erfolgreich war, wenn implizit eine solche Zinsregel angewendet wurde und zweitens, die diese eine geeignete Regel-basierte Handlungsanleitung für Notenbanken ist.
Aus europäischer Sicht besonders interessant ist, dass \textcite{Gali1998} internationale Vergleiche angestellt haben. Sie kommen zu dem Schluss, dass neben den USA zumindest auch Japan und Deutschland implizit "`Inflation-Targeting"' betrieben haben, also einer (zukunftsgerichteten) Zinsregel folgten. Interessant ist auch deren Analyse bezüglich Frankreich, Großbritannien und Italien. Alle drei Staaten gehörten bis zu dessen Auflösung im Jahr 1992 dem Europäischen Währungssystem (EWS) an. Durch die wirtschaftliche Dominanz Deutschlands, mussten diese drei Länder Anfang der 1990er Jahre auf eine eigene Geldpolitik praktisch gänzlich verzichten. Tatsächlich sieht man vor allem für Frankreich und Italien, dass die traditionell eher hohen Inflationsraten ab Anfang der 1980er Jahre deutlich zurückgingen. Der Preis den die beiden Länder dafür bezahlen mussten waren aber sehr hohe nominale Zinssätze, die dazu führten, dass die Realzinssätze deutlich höher waren als in Deutschland. Laut Simulationen in \textcite[S. 23f]{Gali1998} lagen diese Zinssätze deutlich \textit{über} jenen, die eine Zinsregel ergeben hätte.

Taylor beteuerte übrigens in der Conclusion seines Artikels: "`Solche Regeln können und sollen nicht [stur] von [Notenbankern] befolgt werden"' \parencite[S. 213]{Taylor1993}. Später änderte Taylor seine Meinung und spricht sich seither dafür aus, dass Notenbanken ihre Geldpolitik stärker an Zinsregeln wie die Taylor-Rule binden sollten. Vertreter von Zentralbanken wollen sich ihre Autonomie hinsichtlich Geldpolitik natürlich nicht von Regeln einschränken lassen \parencite[S. 609]{Romer2019} und es entstand eine Debatte zwischen \textcite{Bernanke2015} und \textcite{Taylor2015} darüber. Taylor argumentierte, dass die Geldpolitik vor der "`Great Recession"' zu wenig restriktiv war und daher die Immobilienblase förderte. Auf jeden Fall zählt die Taylor-Rule - wenn auch in moderner, verbesserter Form - nach wie vor zum Werkzeug der Mainstream-Ökonomie. Nach der "`Great Recession"' reichte eine Orientierung an dieser Zinsregel aber nicht mehr aus um Geldpolitik durchzuführen, da die Zinsuntergrenze von 0\% nicht unterschritten werden kann, dazu aber später mehr.

Zweifelsohne aber veränderte die Taylor-Rule die Geldpolitik. Der Fokus wendete sich vollends von den Geldmengenzielen \parencite{Friedman1960} zu den Zinssatz-Regeln \parencite[S. 36]{Gali2007}. Heute wird zwar - beeinflusst durch zahlreiche theoretische \parencite{Woodford2001} und empirische \parencite{Gali2000, Taylor1999} Forschung - eine verallgemeinerte Form der Taylor-Rule verwendet, aber das grundlegende Prinzip ist das gleiche geblieben, weshalb man nach wie vor das gesamte Konzept als "`Taylor-Rule"' anstatt als "`Zinsregel"' bezeichnet. Außerdem spricht man heute übrigens meist vom "`Inflation-targeting"', also vom Erreichen einer festgelegten Zielinflation. Dies ist äquivalent mit der regelgebundenen Zinssteuerung \parencite{Taylor2006}, weil die Zinsregel als direkte Einflussgröße die Inflation umfasst. Moderne Notenbanken wie die EZB setzen sich eine Zielinflationsrate als Richtlinie dafür, ob ihre Wirtschaftspolitik erfolgreich war. Damit wurde das Inflation-Targeting zu einem zentralen Tool von Notenbanken. Interessant ist, dass die meisten Zentralbanken tatsächlich ein Inflationsziel von 2\% verwenden, diesen Wert hatte Taylor bereits in seinem Journal-Beitrag recht willkürlich gewählt hat \parencite[S. 202]{Taylor1993}. 

\subsection{\textit{Das} DSGE-Grundmodell}

In diesem Unterkapitel werden wir sehen, wie in den späten 1990er Jahre die verschiedenen nun dargestellten makroökonomischen Entwicklungen ineinander griffen. Im Rahmen der DSGE-Modelle wird die gesamte Ökonomie nämlich auf drei Player eingeschränkt, die mittels Gleichungen dargestellt werden. Jene für:

\begin{itemize}
	\item Haushalte: Die Aggregierte Nachfrage wird modelliert mittels "`Neu-Keynesianischer IS-Kurve"'.
	\item Unternehmen: Das Aggregierte Angebot wird mittels "`Neu-Keynesianischer Phillips-Kurve"' dargestellt.
	\item Zentralbank: Deren Verhalten ist nicht mikrofundiert, aber Regel-gebunden: Abgebildet durch eine "`Taylor-Rule"'.
\end{itemize}

Bevor wir uns ansehen wie diese Komponenten kombiniert werden, betrachten wir kurz, was das Ergebnis, ein DSGE-Modell, nicht umfasst: Wie man sieht werden Staatsausgaben und damit Fiskalpolitik, nicht explizit betrachtet. Vor allem letzteres ist natürlich ein krasser Bruch zu traditionellen Keynesianischen Modellen. Die Staatsausgaben sind im "`repräsentativen Haushalt"' mitumfasst, das heißt, steigende Staatsausgaben führen zu einem höheren BIP. Allerdings gibt es keinen Multiplikatoreffekt (wie in Kapitel \ref{Keynes}). Der Nachfrage-Multiplikator ist gleich eins. Fiskalpolitik hat in der Folge nur einen kurzfristigen und einmaligen positiven Effekt auf das BIP\footnote{Dies gilt zumindest solange der nominale Zins gesteuert werden kann. Zur Diskussion über die Rolle der Fiskalpolitik in der "`Neuen Synthese"' siehe Kapitel \ref{NeueFiskal}} \parencite{Woodford2011}. Anders ausgedrückt: Die Ricardianische Äquivalenz ist in DSGE-Modellen eine Standardannahme. 

Zurück zum Aufbau der DSGE-Modelle: Was können wir aus den Unterkapiteln \ref{NeueIS} bis \ref{Taylor} und den Grundlagen der "`Real-Business-Cycle"'-Modellen ableiten:

Der repräsentative Haushalt hat eine unendliche Lebenserwartung und optimiert sein Verhalten unter der Prämisse, dass Konsum einen positiven Nutzen generiert, zu tätigende Arbeit generiert hingegen einen negativen Nutzen. Außerdem wissen die Haushalte, dass es in der Zukunft zu zufälligen Schocks kommt. Diese sind ja integraler Bestandteil der Real-Business-Cycle-Methodik. Haushalte optimieren also ihren Nutzen, gegeben ihren Erwartungen und dem Wissen um zufällige zukünftige Schocks. Die Optimierungsaufgabe liegt darin, das Verhältnis aus Konsum und Arbeit für alle Zeitperioden festzulegen. Die Zukunft (zukünftige Werte), wird dabei - wie immer in der Ökonomie - abgezinst. Der Realzinssatz ist daher \textit{die} entscheidende Stellschraube bei der Optimierung. Ein steigender Realzinssatz sorgt dafür, dass Konsum aufgeschoben wird, mit dem Ergebnis, dass der aktuelle Konsum - und damit der Gesamtoutput (BIP) - in der aktuellen Periode sinkt. Dies wir repräsentiert durch die "`Neu-Keynesiansische IS-Kurve"'.

Unternehmen haben eine Produktionsfunktion, die von einheitlicher Technologie ausgeht (das heißt nur, dass kein Unternehmen einen Vorteil aus einem technischen Fortschritt generieren kann). Der Produktionsfaktor Arbeit muss zum Nominallohn zugekauft werden. Unternehmen können nur abhängig von einem Zufallsprozess ihre Preise an Marktgegebenheiten anpassen. Die Unternehmen produzieren jeweils ein einzigartiges Gut. Die Optimierungsaufgabe der Unternehmen besteht natürlich in der Gewinnmaximierung. Im Detail verlangt dies eine optimale Preisfestsetzung unter den Nebenbedingungen, dass Preisanpassungen erst wieder in einer zufällig zu bestimmenden Periode in der Zukunft möglich sein werden und in der Zwischenzeit sowohl Änderungen beim Nominalzinssatz, also auch bei Inflationsraten auftreten werden. Aufgrund der Annahme, dass monopolistische Konkurrenzmärkte vorherrschen, führt eine Abweichung des individuellen Preises eines Unternehmens vom Gleichgewichtspreis ja nicht zu einem totalen Rückgang des Gewinns. Unternehmen, die sich monopolistischer Konkurrenz ausgesetzt sehen, schlagen ja ohnehin immer ein "`Mark-up"' auf den Gleichgewichtspreis auf, um ihren Gewinn zu optimieren. Das heißt, Unternehmen setzen ihr Mark-up so, dass sie die Erwartungen ihrer zukünftigen Kostenänderung (durch Inflation) einpreisen und zusätzlich ihre Erwartungen der Dauer bis zur nächsten Preissetzung berücksichtigen. Dieser Preissetzungsmechanismus ist abgebildet durch die "`Neu-keynesianische Phillips-Kurve"'. Die Probleme, nämlich erstens, dass dies mathematisch herausfordernd zu modellieren ist und Annahmen getroffen werden müssen, und zweites, dass es unterschiedliche Ansätze gibt dies zu modellieren sind zwar bekannt, werden aber zumindest im Standardmodell vernachlässigt.

Als dritte Gleichung kommt jene der Geldpolitik ins Spiel. Wie wir bereits gesehen haben, sind für Haushalte und Unternehmen vor allem die Variablen Zinssatz und Inflation von Bedeutung bei ihrer Optimierungsaufgabe. Und an dieser Stelle kommt wieder die Taylor-Regel\parencite{Taylor1993} ins Spiel. Diese ist zwar nicht mikrofundiert, bestimmt aber zu jedem Zeitpunkt die resultierenden Handlungen der Zentralbanken. Es ist auch möglich, die Geldpolitik anders zu modellieren, aber im Ausgangsmodell hat sich das Konzept der Zinsregel etabliert. 

Aus den drei Gleichungen bilden wir jetzt ein DSGE-Modell:
\begin{itemize}
	\item Die Neu-Keynesianische IS-Kurve erklärt die Entwicklung des Outputs $Y$ durch den Real-Zinssatz $r_t$. Für diesen Real-Zinssatz haben wir eine Zinsregel in Form einer Taylor-Rule, welche den Term $r_t$ in der IS-Gleichung ersetzt.
	\item Die Formel für den Real-Zinssatz wiederum hängt ab vom erwarteten Output $E_t[y_{t+1}]$ und der erwarteten Inflationsrate $E_t[\pi_{t+1}]$ ab.
	\item Sowohl in der IS-Gleichung, als auch in der Real-Zinssatz-Gleichung ist außerdem ein Fehlerterm $u_t^{IS}$ bzw. $u_t^{MP}$, der zufällige Nachfrageschocks, bzw. Geldmengen-Schocks darstellt.
	\item Die Neu-Keynesianische Phillips-Kurve erklärt die Inflation $\pi_t$ durch erwartete Inflation $E_t[\pi_{t+1}]$ und aktuellem Output $y_t$. der abhängig vom Grad der nominalen Rigidität $\kappa$ ist.
	\item Dieser aktuelle Output kann wiederum durch die IS-Gleichung ersetzt werden, die wiederum die Gleichung des Real-Zinssatzes enthält.
	\item Die Neu-Keynesianische Phillips-Kurve enthält zusätzlich einen Fehlerterm für Inflationsschocks $u_t^{\pi}$.
\end{itemize}

Wie funktionieren solche Modelle nun? Zunächst muss festgehalten werden, dass es sich um mehrperiodige Gleichgewichtsmodelle handelt, das heißt, nach exogenen Schocks nehmen deren Auswirkungen mit fortlaufender Zeit ab und verschwinden schließlich ganz, womit das System wieder im Gleichgewicht ist. Die drei Stellschrauben sind die Fehlerterme. An diesen Stellen können die zufälligen Schocks simuliert werden: Der Fehlerterm der Zinsregel $u_t^{MP}$ simuliert hierbei einen geldpolitischen Schock. Durch das Gleichungssystem werden die verschiedenen Variablen durch diesen Schock unterschiedlich beeinflusst. Durch die intertemporäre Ausgestaltung des Systems werden diese Abweichungen vom Gleichgewicht über mehrere Perioden wirksam. Bei einem restriktiven, geldpolitischen Schock würde zum Beispiel die Inflation sinken, in weiterer Folge auch die Beschäftigung, sowie damit verbunden, die aggregierten Reallöhne. Der nominale Zinssatz würde steigen. Ebenso der Real-Zinssatz, welcher aufgrund des Rückgangs der Inflation sogar noch stärker steigen würde. Schließlich würde die Gesamtoutput-Entwicklung zurückgehen, also das BIP-Wachstum fallen und der Output-Gap steigen (im negativen Bereich), da das potenzielle BIP unverändert bleibt \parencite[S. 68f]{Gali2015}. Änderungen im Fehlerterm der Neu-Keynesianischen IS-Kurve $u_t^{IS}$ würden einen Nachfrage-Schock (IS-Schock) verursachen. Der Fehlerterm der Neu-Keynesianische Phillips-Kurve $u_t^{\pi}$ simuliert einen Inflationsschock\footnote{Auch ein Technologieschock kann über die Phillips-Kurve simuliert werden.}. Das vollständig definierte Modell gibt somit als Antwort auf auszuwählende Schocks für die verschiedenen Variablen zu verschiedenen Zeitpunkten Abweichungen vom Gleichgewicht an. 

Julio Rotemberg und Michael Woodford waren die ersten Ökonomen, die DSGE-ähnliche Modelle entwickelten. In \textcite{Rotemberg1993} implementierten die beiden das Konzept der Monopolistischen Konkurrenz in ein dynamisches Gleichgewichtsmodell. Als erstes vollständiges DSGE-Modell wird häufig \textcite{Yun1996} genannt, zum Beispiel in \textcite[S. 80]{Gali2015}. Tatsächlich veröffentlichte, der sonst recht unbekannt gebliebene Ökonom, schon Mitte der 1990er Jahre ein DSGE-Modell, das vom Aufbau her den heutigen Modellen schon sehr ähnlich ist. Monopolistische Konkurrenz, sowie Nominale Rigiditäten, die mittels den von \textcite{Calvo1983} vorgeschlagenen Prozess simuliert werden, sind bereits enthalten. Auch die Darstellung der Ergebnisse ist bis heute ähnlich geblieben. \textcite{Rotemberg1997} veröffentlichten noch eine Reihe weiterer DSGE-Paper. Interessant ist, dass \textcite{Woodford1996} bereits im Jahr 1996 mittels DSGE-Modell analysierte, dass sich in einer Währungsunion expansive Fiskalpolitik eines Mitgliedstaats auf die Preisstabilität der anderen Mitgliedsstaaten auswirkt. Er ging hierbei davon aus, dass die Ricardianische Äquivalenz nicht gültig ist. Das Ausgangsbeispiel war hierbei die Europäische Währungsunion und der Sinn und Zweck der Maastricht-Kriterien, die den potentielle Mitgliedsstaaten enge fiskalpolitische Grenzen auferlegte\footnote{So durfte ein jährliches Budgetdefizit von 3\% des BIP nicht überschritten werden und der Verschuldungsgrad musste unter 60\% des BIP liegen.}. Der Durchbruch der DSGE-Modell erfolgte schließlich mit der Jahrtausendwende. Als \textit{das} Standardmodell etablierte sich schließlich jenes Drei-Gleichungen-Modell, das \textcite{Gali2000} veröffentlichten. In dieser Zeit wurde das Modell auch schon ausgiebig auf seine empirische Robustheit getestet und diskutiert \parencite{Gali1999}. \textcite{Gali1998, Taylor1999} zum Beispiel publizierten zur Gültigkeit der Taylor-Rule. Die Diskussion zur problematischen Neu-Keynesianischen Phillips Kurve entfachte etwas später und wurde bereits in Unterkapitel \ref{NeuePhillips} angeführt. Hierzu wurden alsbald auch Alternativen (siehe Unterkapitel \ref{ErweiterungDSGE}) entwickelt. Ebenfalls zur Jahrtausendwende lieferten \textcite{Rotemberg1998, Christiamo1999} empirische Evidenz zu den Auswirkungen von geldpolitischen Schocks.

Trotz aller Unzulänglichkeiten und Erweiterungen wird das Modell von \textcite{Gali2000} in Lehrbüchern noch immer als Ausgangsmodell angeführt \parencite[S. 350]{Romer2019}. Dieses Modell ist mathematisch komplex, aber äußerst elegant und nachvollziehbar. Die Gleichungen bedingen sich gegenseitig und über mehrere Zeitperioden hinweg. Das Modell ist also ein intertemporales Optimierungsmodell und somit \textit{dynamisch}\footnote{Im Spezialfall von fehlender Autoregression, wäre das Modell im Gleichgewicht und Abweichungen einzelner Variablen vom Gleichgewicht würden nur mehr durch die zufällige Schocks vorkommen. Das Ergebnis wäre ident zu Ergebnissen reiner "`Real-Business-Cycle"'-Modellen.}. Durch die Hinzunahme der Fehlerterme umfasst die Annahme zufälliger Schocks, ist also \textit{stochastisch}. Es ist streng mikrofundiert und ausschließlich zukunftsorientiert. Damit entspricht das Modell den Anforderungen der Theorie der rationalen Erwartungen.

Ohne Zweifel sind DSGE-Modelle mit sehr starken Annahmen hinterlegt, das macht sie punktuell recht angreifbar. Man denke nur an die schlechten empirischen Ergebnisse zur "`Neu-keynesianischen Phillips-Kurve"'. Man muss aber im Umkehrschluss festhalten, dass DSGE-Modelle Probleme erfolgreich in den Griff bekommen, die zu Zeiten der traditionellen Keynesianische und Monetaristische Modelle noch nicht einmal also solche identifiziert waren. Zudem sind sie ein Fortschritt gegenüber den "`Real-Business-Cycle"'-Modellen. Faktum ist, dass DSGE-Modelle vor allem in Zentralbanken als Analyseinstrument der Geldpolitik herangezogen werden. Bis heute gibt es außerdem viele Weiterentwicklungen des Standardmodells, auf die wichtigsten gehen wir im nächsten Kapitel ein.




\section{Erweiterungen}
\label{ErweiterungDSGE}
Sticky Wages und Sticky Prices: Kapitel 6 im Gali-Buch!
Unemployment (Neue Phillips Kurve?): Kapitel 7 im Gali-Buch! + Romer-Buch S. 327ff + S. 338. Ursprünglich: Roberts 1995!
"`HANK"'!: Kapitel 9 im Gali-Buch!

\subsection{Arbeitslosigkeit}

\subsection{State-Dependent Pricing}
Die Tatsache, dass jene Unternehmen, die ihre Preise in einer bestimmten Periode anpassen, in DSGE-Modellen per Zufallsprozess ausgewählt werden, simuliert zwar sehr elegant die empirisch zu beobachtende Tatsache, dass es nominale Rigiditäten bei der Preisanpassung gibt, ist aber modell-theoretisch natürlich ein Schwachpunkt. Preisanpassungen werden Unternehmen in der Realität nicht durch einen Zufallsprozess diktiert, vielmehr sorgen rationale - zum Beispiel Kosten der Preisanpassungen - aber auch irrationale - zum Beispiel die Trägheit bei der Preisanpassung, solange Gewinne eingefahren werden - Überlegungen dafür, dass die Preise nicht ständig die theoretischen Gleichgewichtspreise repräsentieren.
Romer-Buch, S. 329.
Danzinger Lucas Modell

\subsection{Staggered Price Adjustmend with Inflation Inertia}


Hybride Phillips Kurve \textcite{Gali1999} und \textcite{Gali2005}
Problem mit "`Inflation Inertia"': 
\textcite{Christiano2005}
und Mankiw-Reis Paper
Romer-Buch S. 341 + S. 354




Status Quo:
Berühmte Modelle in der Praxis: 
Smets/Wouters (EZB-Modell)




Abschluss DSGE: Siegeszug in der Makroökonomie. Aber sehr umstritten durch starke Annahmen. Gilt als "`Schönwettermodell"', das versagt Krisenauswirkungen zu erklären.
Erhielt durch die "`Great Recession"' einen Dämpfer, weil in keinster Weise vorhergesagt.




\section{Inflation}

\subsection{Kosten der Inflation, Inertia}
???

Seite 411 (Snowdon/Vane)


\subsection{Zentralbanken Unabhängigkeit und Inflation-Targeting}
Romer-Buch S.637

\subsection{Divine Coincidence}
Romer-Buch S.602





RBC + \textcite{RomerDavid1990}

Formulierung der Taylor-Rule (1993?) als  Übergangszeitpunkt 1. Generation --> 2. Generation
Zweiter Übergangspunkt: Ab 1990 viel stärker empirisch (bis dahin sehr theoretische Arbeiten der Neu-Keynesianer)

Problem der Arbeitslosigkeit trat in den Vordergrund.



Neue Phillips Kurve

Cost of Inflation (\textcite{Snowdon2005} ab S. 401) 


Wirtschaftspolitisch Dominanz der Geldpolitik
Fiskalpolitik selbst in Krisen umstritten (Paper zur empirischen Bestimmung der Wirksamkeit der Fiskalpolitik (Blanchard))



Interessant ist, das

Elemente aus verschiedenen Schulen:
\begin{itemize}
	\item Keynesianismus: Rigiditäten. Teilweise Fiskalpolitik im Krisenfall
	\item Monetarismus: Zentrale Bedeutung der Geldpolitik und der Zentralbanken, Natürliche Arbeitslosigkeit
	\item Österreichische Schule: Konzept des Gleichgewichtszinssatzes (Wicksell)
	\item Neu-Keynesianismus: "`Nominale Rigiditäten"', "`Monopolistische Konkurrenz"' und "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"'. Älter: NAIRU, Marktversagen
	\item Neue Klassische Makroökonomie: Annahme "`Rationale Erwartungen"', Real-Business-Cycle-Modelle.
	\item Post-Keynes: Mark-up (monopolistische Konkurrenz) kommt eigentlich daher.
\end{itemize}



\section{Fiskalpolitik}
\label{NeueFiskal}


Inhaltlich spielt in der Wirtschaftspolitik fast ausschließlich nur mehr die Geldpolitik eine Rolle. Zur Fiskalpolitik haben die Vertreter der "`neuen Neoklassischen Synthese"' praktisch ausschließlich eine ablehnende Haltung.




Seite 409 (Snowdon/Vane)
\parencite{Woodford2011}
JEEA 2007 von Gali, Lopez-Salido und Valles 


\section{Auf den Schultern von Giganten}
\label{Giganten}

\subsection{Krugman}

\subsection{Blanchard}

\subsection{David Romer \& Mankiv}





Solow (2010) äußerte sich sehr kritisch gegenüber DSGE-Modellen. Er vertrat die Meinung, dass 
DSGE-Modelle keinen Bezug zur Realität hätten und daher ungeeignet für die Politikberatung 
seien.


Blanchard 2016: Do DSGE Models have a Future

Romer, Paul, 2016, The trouble with macroeconomics, in: The American Economist, forthcomin










