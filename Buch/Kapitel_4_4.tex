%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neue Neoklassische Synthese}
\label{Neue Neoklassische Synthese}

Mit diesem Kapitel sind wir in der Gegenwart der Ökonomie angekommen. Man kann zwar durchaus argumentieren, dass die Makroökonomie nach der weltweiten Wirtschaftskrise ab 2008 und der immer noch praktizierten globalen Nullzinspolitik eine erneute "`Revolution"' nötig hätte, aber Stand 2022 ist die State-of-the-Art Mainstream-Ökonomie die \textit{Neue Neoklassische Synthese}. Der Begriff "`Neue Neoklassische Synthese"' ist (noch) nicht wirklich etabliert als Bezeichnung für den aktuellen wirtschaftswissenschaftlichen "`Mainstream"'. Meist spricht man stattdessen von "`Neu-Keynesianismus"' oder auch von "`Neoklassik"'\footnote{In Lehrbüchern wird häufig ohne Unterschied vom "`Neu-Keynesianismus"' gesprochen. Auch "`Neu-Keynesianismus der 2. Generation"', "`Neue Synthese"', "`Neue Keynesianische Synthese"'  kommen vor. Selten werden die Modelle auch als "`Neo-Wicksellianisch"' bezeichnet \parencite[S. 28]{Gali2007}, dies wegen der Ähnlichkeit zur Theorie von Wicksell, der Abweichungen vom natürlichen Gleichgewicht beschreibt}. Beide Begriffe sind aber nicht eindeutig. Um Unklarheiten zu vermeiden wird hier der etwas holprige, aber eindeutige und inhaltlich meiner Meinung nach passende Begriff "`Neue Neoklassische Synthese"' (oder schlicht "`Neue Synthese"') verwendet.

Ungefähr um 1990 versuchten Ökonomen, die nicht vom Streit zwischen Neu-Klassikern und Neu-Keynesianern vorbelastet waren, unvoreingenommen das beste aus beiden Welten zu übernehmen und zu einer "`Neuen Synthese"' zusammen zuführen. Die Abgrenzung zwischen "`Neu-Keynesianismus"' und "`Neuer Synthese"' ist hierbei sowohl inhaltlich als auch zeitlich verlaufend. Vor allem, weil viele Ökonomen, die uns im letzten Kapitel untergekommen sind, auch in diesem Kapitel die "`Hauptdarsteller"' sein werden. Es gibt aber auch durchaus Abspaltungen bei den Vertretern des "`Neu-Keynesianismus"': Paul Krugman und Joseph Stiglitz, zum Beispiel, lehnen viele Ansätze der "`Neuen Synthese"' weitgehend ab. Mankiw, Blanchard und David Romer sind schwieriger einer der beiden Schulen zuzuordnen, sie stehen für den Übergang von "`Neu-Keynesianismus"' zur "`Neuer Synthese"'. Ein zentraler Vertreter der "`Neuen Sythese"' (ohne Vergangenheit im "`Neu-Keynesianismus) ist Jordi Gali. Ein spezieller Vertreter ist John Taylor. Er begründete - gemeinsam mit \textcite{Phelps1968} und \textcite{Fischer1977} - den "`Neu-Keynesianismus"' mit \parencite{Taylor1977}, und auch für die "`Neue Synthese"' lieferte er einen der grundlegenden Beiträge \parencite{Taylor1993}. Es gibt heute in der Ökonomie nicht mehr jene klar abgrenzbaren, konkurrierenden Schulen, die die Wirtschaftsgeschichte des 20. Jahrhunderts geprägt haben: Neoklassiker vs. Keynesianer, Keynesianer vs. Monetaristen, Neu-Klassiker vs. Neu-Keynesianer. Vielmehr ist die gesamte Mainstream-Ökonomie unter einem sehr breiten Dach zusammengefasst. Was aber nicht bedeutet, dass unter diesem Dach alle einer Meinung sind, ganz im Gegenteil.\footnote{Neben diesem Mainstream, gibt es immer noch mehrere heterodoxe Schulen, die im Teil \ref{Heterodox} beschrieben werden.}

Passend dazu verschwammen auch die ideologischen Unterschiede zwischen den verschiedenen ökonomischen Gruppen. Konnte man bis in die 1990er Jahre hinein die ökonomischen Richtungen meist auch einer politischen Richtung zuweisen, ist dies heute nicht mehr möglich. Sozialdemokraten (Kontinental-Europa), Labour-Party (UK) und Demokraten (USA) waren fast ausschließlich dem (Neu-) Keynesianismus zugeneigt. Christ-Demokraten (Kontinental-Europa), Tories (UK) und Republikaner (USA) meist den Neoklassikern,  Monetaristen und Neuen Klassikern. Das Spektrum der Vertreter der Neuen Synthese reicht vom Erzliberalen John Taylor über den bekennenden Republikaner Mankiw bis zu Janet Yellen, die Finanzministerin im Kabinett des demokratischen US-Präsidenten Joe Biden ist. 

Bevor wir uns die rein ökonomischen Aspekte der "`Neuen neoklassischen Synthese"' im Detail ansehen, blicken wir auf das Umfeld. Welchen Herausforderungen waren die Volkswirtschaften Anfang der 1990er Jahre ausgesetzt? Politisch gesehen war natürlich der Zusammenbruch der Sowjetunion und damit des real existierenden Sozialismus dominierend. Die Marktwirtschaft, also die Grundlage fast aller in diesem Buch beschriebenen Ideen, hatte sich durchgesetzt. Der Kommunismus, der ohnehin nie so wie von Marx beschrieben praktiziert wurde, galt endgültig als gescheitert. In den westlichen Marktwirtschaften trat das Problem der Inflation in den Hintergrund. Dafür traten Probleme der Arbeitslosigkeit in den Vordergrund, in Europa stärker ausgeprägt als in den USA. Das Problem der steigenden Staatsschulden wurde zunehmend thematisiert, mit ein Grund warum Fiskalpolitik aus dem Fokus geriet. Wechselkurssysteme waren Anfang der 1990er zwar noch einmal ein Thema, als sich nacheinander mehrere europäische Zentralbanken dem Treiben von Spekulanten ausgesetzt sahen, die versuchten die fixierten Wechselkurse zu manipulieren. Doch mit der Schaffung der Europäischen Wirtschafts- und Währungsunion, die in der Einführung des Euros gipfelte, verlor diese Thematik an Bedeutung. Technologisch Begann mit den frühen 1990er Jahren das Zeitalter der Computer. Rechenmaschinen wurden für Haushalte und Kleinunternehmen leistbar und veränderten damit auch das zentrale Verwaltungssystem. Damit verbunden waren wesentliche Verbesserungen in der Datenverfügbarkeit und der Datenauswertung. Die in diesem Buch nicht gesondert behandelte Ökonometrie, sowie die empirische Wirtschaftswissenschaft erfuhr in dieser Zeit einen enormen Aufschwung. Das ist nicht unwesentlich bei der nun folgenden Betrachtung der Weiterentwicklung der Makroökonomie. Die modernen DSGE-Modelle, die im folgenden erläutert werden, sind nur numerisch und damit mit hohem Rechenaufwand zu lösen. 

Zurück zur Entwicklung der Ökonomie: Es gibt vor allem zwei Punkte, die sich Anfang der 1990er-Jahre entwickelt haben und die Makroökonomie und deren Wirtschaftspolitik seither eindeutig prägen und sehr wohl eine eindeutige Abgrenzung vom Neu-Keynesianismus und der Neuen Klassik ermöglichen:
\begin{enumerate}
	\item Erstens, in der makroökonomischen Theorie: die Zusammenführung der formal-mathematischen Real-Business-Cycle-Gleichgewichtsmodelle mit Elementen der Neu-Keynesianer.
	\item Zweitens, in der Wirtschaftspolitik: Die Dominanz der Bedeutung der Geldpolitik und der Aufstieg der Zentralbanken zum wichtigsten wirtschaftspolitischen Player.
\end{enumerate}





\section{Die Verwissenschaftlichung der Zentralbanken}
\label{Taylor-Rule}

\subsection{Exkurs: Die Evolution der Zentralbanken}
Vom Verwalter des Goldstandards, zum Hüter der Wechselkurse, zum Spieler gegen Spekulanten, zur zentralen Player der Wirtschaftspolitik
HIER WEITER


Evolution: Geldmengenziele, hin zum impliziten und expliziten Inflation-targeting.

Kosten der Inflation, empirische Untersuchungen

Unabhängigkeit der Zentralbanken

Romer-Buch S. 637ff





\subsection{Taylor-Rule: Ein pragmatischer Zugang zur Geldpolitik}
Die Mainstream-Ökonomie kam Anfang der 1990er Jahre weitgehend darin überein, dass Märkte in der Regel nicht vollkommen reibungslos funktionieren. Wäre dies der Fall wäre aktive Wirtschaftspolitik wirkungslos und Konjunkturschwankungen wären rein zufällig, wie im Real-Business-Cycle-Framework dargestellt. Wie \textcite[S. 823]{Akerlof1985} beschrieb, gingen die Neu-Keynesianer also davon aus, dass Wirtschaftspolitik im Allgemeinen und Geldpolitik im Speziellen \textit{nicht} wirkungslos sind.

Die Geldpolitik ist uns ja schon des öfteren in diesem Buch untergekommen. Die Keynesianer legten den Fokus auf Fiskalpolitik. Friedman machte die Geldpolitik, und insbesondere die Gültigkeit der Quantitätstheorie des Geldes, wieder salonfähig. Keynesianer und Monetaristen "`einigten"' sich schließlich darauf, dass ein Policy-Mix aus Fiskal- und Geldpolitik zu optimalen Ergebnissen führt. \textcite{Friedman1960} (vgl. Kapitel \ref{Monetarismus}) plädierte schließlich in der Geldpolitik für ein jährliches "`Mengenziel"'. Die Zentralbanken sollten die Geldmenge mit einer konstanten Rate wachsen lassen. Tatsächlich folgten Zentralbanken diesen Rat vor allem in den 1980er Jahren. Das Resultat dieser Geldmengensteuerung war aber in der Empirie wenig befriedigend. Offensichtlich änderte sich die Umlaufgeschwindigkeit des Geldes mit dem Effekt, dass Inflation und der kurzfristige Zinssatz nicht einem stabilen Pfad folgten, sondern im Gegenteil, stark schwankten. Zentralbanken gingen daher dazu über direkt ihre Zielgröße, nämlich den kurzfristigen Zinssatz, zu steuern. Dazu passen sie ihr Geldangebot so an, dass sich das Marktgleichgewicht bei entsprechender Geldnachfrage genau beim gewünschten, kurzfristigen Zinssatz\footnote{Was ist mit "`kurzfristigem Zinssatz"' gemeint? Hier kommt es oft zu Missverständnissen. Dies ist \textit{nicht} der Leitzins, oder ein anderer Zentralbankenzinssatz. Tatsächlich handelt es sich um den Zinssatz, zu dem sich Finanzinstitutionen "`über Nacht"' gegenseitig liquide Mittel leihen. In Europa entspricht dies also dem "`EONIA"' (bzw. ESTR), in den USA ist dies die "`Federal funds rate"'} einpendelt. Bleibt die Frage, \textit{welchen} kurzfristigen Zinssatz die Zentralbank wählen soll, um den gewünschten, stabilen Wachstumspfad zu erreichen? Und hier kommen die "`Zinssatz-Regeln"', die allerdings meist "`Taylor-Rule"' bezeichnet werden, ins Spiel.

Benannt sind diese nach \textit{John Brian Taylor}. Der Stanford-Professor ist ein sehr interessantes Individuum. Taylor ist eigentlich eher dem erzliberalem Spektrum zuzuordnen. Unter anderem war er Vorsitzender der Mont-Pelerin-Gesellschaft von 2018 bis 2020. Etwas im Widerspruch dazu steht aber, dass er einer der ersten Ökonomen war, der die Existenz und Bedeutung von Nominalen Rigiditäten beschrieb. Er wurde diesbezüglich schon in Kapitel \ref{cha: Neu Keynes} des öfteren zitiert. Er stellte sich damit eben \textit{gegen} die "`Neuen Klassiker"' - die ganz im Sinne des ökonomischen Liberalismus - postulierten, dass Wirtschaftspolitik wirkungslos sei. Anfang der 1990er Jahre schließlich trug er maßgeblich zum theoretischen Verständnis der modernen Zentralbankensteuerung bei. Was sich schließlich als zentraler Baustein in den Modellen der "`Neuen neoklassischen Synthese"' erweisen sollte.

Der bahnbrechende Artikel \textcite{Taylor1993} \textit{begründete} nicht die Zinssteuerung in Zentralbanken - tatsächlich waren viele Zentralbanken schon vor der Veröffentlichung des Artikels zur Zinssteuerung übergegangen - sondern versuchte eine formale Regel zu finden, wie Zentralbanken den kurzfristigen Zins bestimmen sollten. Er schlug tatsächlich eine sehr einfache Formel vor \parencite[S. 202]{Taylor1993}. Normalerweise wird auf die Darstellung von mathematischen Formeln in diesem Buch bewusst verzichtet, aber in diesem Fall hat die Darstellung auch für nicht mathematisch versierte Leser nur Vorteile\footnote{Die im Fließtext angeführte Formel ist tatsächlich jene die \textcite{Taylor1993} verwendete. Verallgemeinert wird sie heute meist (z.B.: \textcite[S. 609]{Romer2019}) so dargestellt: $i_t = r^n + \phi_{\pi}*(\pi_{t} - \pi*) + \phi_y * (ln Y_t - lnY^{n}_{t})$. Dadurch wird der lineare Zusammenhang zwischen Zielvariable und Inflation und der log-lineare Zusammenhang zwiscehn Zeilvariable und BIP-Abweichung deutlich gemacht. Außerdem werden die Konstanten Zielinflation \textit{p} = 2\% und Real-Zinssatz = 2\% durch Variable \textit{r} und $\pi*$ ersetzt, sowie  $\phi_{\pi}$ und $\phi_y$ statt fixer Werte}:

$$ r = p + 0,5y + 0,5 *(p-2) + 2 $$

Dabei ist \textit{r} eben die Zielvariable \textit{kurzfristiger Zinssatz}. \textit{p} bildet die Inflationsrate ab. Und \textit{y} ist die Abweichung des \textit{tatsächlichen realen BIP} vom \textit{potentiellen realen BIP}. Wenn das tatsächliche BIP unter dem potentiellen BIP liegt, so wird der Wert negativ, andernfalls positiv. Zur Erklärung: Taylor geht in seiner Arbeit davon aus, dass das BIP langfristig mit einer konstanten Wachstumsrate von ca. 2\% pro Jahr wächst. Dies ist konform mit allgemeinen Gleichgewichtstheorien und insbesondere auch mit Real Business Cycle-Modellen.


 Gehen wir zum Verständnis der Funktion dieser einfachen Zinsregel drei Beispiele durch:
\begin{enumerate}
	\item Inflation: \textit{p} = 4. Abweichung vom potentiellen BIP: \textit{y} = 4. \\
	Ergibt:	$ 4 + 0,5*4 + 0,5 *(4-2) + 2 = 9$ \\
	Die Zentralbank würde den Zinssatz also auf 9\%, bzw. real 5\% festlegen.
	\item Inflation: \textit{p} = 2. Abweichung vom potentiellen BIP: \textit{y} = 0. \\
	Ergibt:	$ 2 + 0,5*0 + 0,5 *(2-2) + 2 = 4$ \\
	Die Zentralbank würde den Zinssatz also auf 4\%, bzw. real 2\% festlegen.
	\item Inflation: \textit{p} = 1. Abweichung vom potentiellen BIP: \textit{y} = -4. \\
	Ergibt:	$ 1 + 0,5*-4 + 0,5 *(1-2) + 2 = 0,5$ \\
	Die Zentralbank würde den Zinssatz also auf 0,5\%, bzw. real -0,5\% festlegen.
	
\end{enumerate}

Im Modell von Taylor wird eine Zielinflation von 2\%, sowie ein stabiler Real-Zinssatz von 2\% implizit angenommen. 
Wann dann die Inflation tatsächlich 2\% beträgt und es auch zu keinen Abweichungen vom langfristigen Wachstumspfad (Das BIP wächst mit ca. 2\%; \textit{y = 0}) kommt, dann bleibt der Zinssatz bei 4\% und das BIP wächst, vorbehaltlich zukünftiger exogener Schocks, konstant entlang des langfristigen Wachstumspfads. Dieser Gleichgewichtszustand wird im zweiten Beispiel oben dargestellt.
Das erste Beispiel zeigt, was passieren sollte, wenn die Wirtschaft droht zu "`überhitzen"'. Dies ist dann der Fall wenn die Inflation höher ist als 2\% und/oder das potentielle BIP übertroffen wird. Hier sollte der sehr hohe kurzfristige Zinssatz von 9\% angestrebt werden. Das Geldangebot müsste dementsprechend im Verhältnis zur hohen Geldnachfrage - davon kann man aufgrund der Tatsache, dass Inflation wie auch BIP über den Zielwerten liegen ausgehen - recht gering gehalten werden.
Das dritte Beispiel zeigt schließlich, wie die Notenbank stimulierend eingreifen kann. Da möchte sie in der Regel, wenn das tatsächliche BIP hinter seinem Potential zurück geblieben ist und/oder die Inflation unter der angenommenen Zielrate von 2\% liegt. Ein Zinssatz von real unter 0\% sollte für steigenden Nachfrage nach Geld sorgen. Aufgrund des niedrigen angestrebten Nominalzinssatzes von 0,5\% wäre auch das zur Verfügung gestellte Angebot dementsprechend hoch.

Die Ursprungsversion der Taylor-Rule hat einige Ungenauigkeiten, denen sich Taylor auch bewusst war. Erstens geht Taylor davon aus, dass der natürliche, reale Zinssatz - bei dem die Ökonomie auf einem stabilen Wachstumskurs bleibt ohne zu überhitzen, bzw. nicht ausgelastet ist - langfristig konstant und bekannt ist. Beides ist zumindest fraglich. Zweitens, werden die Eingangsvariablen BIP, potentielles BIP und Inflation als bekannt angenommen, was vor allem beim potentiellen BIP alles andere als klar ist. Diese beiden Probleme können nie zur Gänze gelöst werden, aber durch den Fortschritt vor allem bei empirischen Arbeiten, können die Parameter heute zumindest weiter eingeschränkt werden als Anfang der 1990er Jahre. Ein Modell-theoretisches Problem ist drittens, die Berücksichtigung der Inflation: \textcite[S. 211]{Taylor1993} selbst beschrieb bereits, dass Inflation nicht einfach - wie in der von ihm formulierten Formel - als Vergangenheitswert berücksichtigt werden sollte, sondern stattdessen als Kombination aus durchschnittlichen Vergangenheitswerten und erwarteten Inflationsraten. Eine wichtige Erweiterung, die in späteren Zinsregeln Anwendung fand. Später werden wir sehen, dass in DSGE-Modellen alle Parameter sogar rein zukunftsorientiert sind, dementsprechend werden heute häufig ausschließlich die Inflations\textit{erwartungen} als Parameter für Zinsregeln verwendet. Eine bis heute häufig verwendete Umsetzung hierfür kommt von \textcite[S. 150ff]{Gali2000}. Eine vielbeachtete Arbeit zu Zinsregeln ist auch \textcite{Woodford2001}. Darin räumt der Autor modelltheoretische Bedenken, die Ende der 1990er Jahre aufgetreten sind, weitgehend aus. So wurden Zweifel aufgeworfen, ob eine reine Zinsregel dem Ziel ein stabiles Gleichgewicht zu erreichen überhaupt sinnvoll sein kann? So könnte es zum Beispiel sein, dass ein unvorhergesehener Anstieg der Inflationserwartungen dazu führt, dass die Zinsregel einen nominalen Zinssatz vorschlägt, der in einem  Realzinssatz resultiert, der niedriger ist als der Gleichgewichtszinssatz. Dieser Zinssatz würde nicht, wie gewünscht restriktiv wirkend, sondern expansiv. Das Ergebnis wären steigender Output, steigende Inflation und steigende Inflationserwartungen. Das Ergebnis wäre eine immer weiterführende, die Inflation stimulierende, Spirale, also das Gegenteil vom stabilen Gleichgewicht. Eine zweite kritische Frage ist, ob die verwendeten Bestimmungsfaktoren Inflation und BIP-Abweichung überhaupt als Zielfaktoren der Geldpolitik geeignet sind? \textcite{Woodford1999} zeigt für beide Fragen formal, dass Zinsregeln sehr wohl stabilisierend wirken, räumt aber ein, dass vor allem bei der BIP-Abweichung Probleme dahingehend auftauchen, welche zukunftsgerichteten Werte tatsächlich verwendet werden sollen. Michael Woodford ist übrigens jener Ökonom, der das Framework der "`Neuen Neoklassischen Synthese"' lieber "`Neo-Wicksellianische Schule"' nennt. Tatsächlich finden sich zwischen Taylor-Rule-Konzept und der Arbeit von Wicksell (vgl. Kapitel \ref{Austria}) interessante Parallelen, nämlich in der postulierten, stabilisierenden Wirkung des Zinssatzes. Tatsächlich hat schon \textcite{Wicksel1898} die Idee aufgebracht, dass die sich Ökonomie bei einem bestimmten natürlichen Zinssatz im Gleichgewicht befindet. Bei Abweichungen davon könnte man also mittels Erhöhung bzw. Reduktion des Zinssatzes gegensteuern. 

Wie zu Beginn dieses Kapitels bereits angedeutet, erfuhr die \textit{empirische} Wirtschaftsforschung in den 1990er Jahre einen enormen Aufschwung. So wurde auch die Taylor-Rule mittels Datenanalyse auf ihre empirische Robustheit überprüft. \textcite{Taylor1999} war auch hierbei einer der ersten. Er analysierte Zeitreihen für die USA, die zurück bis 1979 reichten. Die Arbeit ist auch deswegen interessant, weil Taylor darin explizit darauf hinweist, dass Zinsregeln nicht im Widerspruch zur Quantitätsgleichung des Geldes stehen (vgl. Kapitel \ref{Monetarismus}), sondern, im Gegenteil, sogar daraus abgeleitet werden können\parencite[S. 322f]{Taylor1999}. Ein zweiter Artikel - der in diesem Kapitel wegen seiner umfassenden theoretischen und empirischen Bedeutung noch öfter zitiert wird - ist jener von \textcite{Gali2000}. Die beiden Artikel kommen zum gleichen zentralen Ergebnis: In den 1960er und 1970er Jahren waren die realen kurzfristigen Zinssätze deutlich geringer als in den 1980er und 1990er Jahren. Zur Erinnerung der erstgenannte Zeitraum war geprägt von hohen Inflationsraten. Nachdem Paul Volcker 1979 Gouverneur der Fed wurde, führte er, zur Inflationsbekämpfung, eine restriktive Zinspolitik ein und folgte damit, retrospektiv betrachtet, einer Taylor-Rule. Tatsächlich waren die 1980er und 1990er Jahre geprägt von niedriger Inflation und recht stabilen Wachstumsraten.  Aus heutiger Sicht sind die Lobgesänge auf die sogenannte Volcker-Greenspan-Ära, auf die sich \textcite{Taylor1999} und \textcite{Gali2015}, in Anlehnung an die damaligen Fed-Vorsitzenden, beziehen, etwas vorsichtiger zu sehen. Folgte doch bereits im März 2000 mit dem Platzen der sogenannten "`Dot-Com"'-Blase eine große Finanzkrise\footnote{Anmerkung: Das Entstehen von Überbewertungen auf Finanzmärkten, wird häufig mit fehlerhafter Geldpolitik im Vorfeld in Verbindung gebracht}. Insgesamt werden die empirischen Ergebnisse - vor allem von Taylor selbst - als Bestätigung dafür gesehen, dass erstens, Geldpolitik dann besonders erfolgreich war, wenn implizit eine solche Zinsregel angewendet wurde und zweitens, die diese eine geeignete Regel-basierte Handlungsanleitung für Notenbanken ist.
Aus europäischer Sicht besonders interessant ist, dass \textcite{Gali1998} internationale Vergleiche angestellt haben. Sie kommen zu dem Schluss, dass neben den USA zumindest auch Japan und Deutschland implizit "`Inflation-Targeting"' betrieben haben, also einer (zukunftsgerichteten) Zinsregel folgten. Interessant ist auch deren Analyse bezüglich Frankreich, Großbritannien und Italien. Alle drei Staaten gehörten bis zu dessen Auflösung im Jahr 1992 dem Europäischen Währungssystem (EWS) an. Durch die wirtschaftliche Dominanz Deutschlands, mussten diese drei Länder Anfang der 1990er Jahre auf eine eigene Geldpolitik praktisch gänzlich verzichten. Tatsächlich sieht man vor allem für Frankreich und Italien, dass die traditionell eher hohen Inflationsraten ab Anfang der 1980er Jahre deutlich zurückgingen. Der Preis den die beiden Länder dafür bezahlen mussten waren aber sehr hohe nominale Zinssätze, die dazu führten, dass die Realzinssätze deutlich höher waren als in Deutschland. Laut Simulationen in \textcite[S. 23f]{Gali1998} lagen diese Zinssätze deutlich \textit{über} jenen, die eine Zinsregel ergeben hätte.

Taylor beteuerte übrigens in der Conclusion seines Artikels: "`Solche Regeln können und sollen nicht [stur] von [Notenbankern] befolgt werden"' \parencite[S. 213]{Taylor1993}. Später änderte Taylor seine Meinung und spricht sich seither dafür aus, dass Notenbanken ihre Geldpolitik stärker an Zinsregeln wie die Taylor-Rule binden sollten. Vertreter von Zentralbanken wollen sich ihre Autonomie hinsichtlich Geldpolitik natürlich nicht von Regeln einschränken lassen \parencite[S. 609]{Romer2019} und es entstand eine Debatte zwischen \textcite{Bernanke2015} und \textcite{Taylor2015} darüber. Taylor argumentierte, dass die Geldpolitik vor der "`Great Recession"' zu wenig restriktiv war und daher die Immobilienblase förderte. Auf jeden Fall zählt die Taylor-Rule - wenn auch in moderner, verbesserter Form - nach wie vor zum Werkzeug der Mainstream-Ökonomie. Nach der "`Great Recession"' reichte eine Orientierung an dieser Zinsregel aber nicht mehr aus um Geldpolitik durchzuführen, da die Zinsuntergrenze von 0\% nicht unterschritten werden kann, dazu aber später mehr.

Zweifelsohne aber veränderte die Taylor-Rule die Geldpolitik. Der Fokus wendete sich vollends von den Geldmengenzielen \parencite{Friedman1960} zu den Zinssatz-Regeln \parencite[S. 36]{Gali2007}. Heute wird zwar - beeinflusst durch zahlreiche theoretische \parencite{Woodford2001} und empirische \parencite{Gali2000, Taylor1999} Forschung - eine verallgemeinerte Form der Taylor-Rule verwendet, aber das grundlegende Prinzip ist das gleiche geblieben, weshalb man nach wie vor das gesamte Konzept als "`Taylor-Rule"' anstatt als "`Zinsregel"' bezeichnet. Außerdem spricht man heute übrigens meist vom "`Inflation-targeting"', also vom Erreichen einer festgelegten Zielinflation. Dies ist äquivalent mit der regelgebundenen Zinssteuerung \parencite{Taylor2006}, weil die Zinsregel als direkte Einflussgröße die Inflation umfasst. Moderne Notenbanken wie die EZB setzen sich eine Zielinflationsrate als Richtlinie dafür, ob ihre Wirtschaftspolitik erfolgreich war. Damit wurde das Inflation-Targeting zu einem zentralen Tool von Notenbanken. Interessant ist, dass die meisten Zentralbanken tatsächlich ein Inflationsziel von 2\% verwenden, diesen Wert hatte Taylor bereits in seinem Journal-Beitrag recht willkürlich gewählt hat \parencite[S. 202]{Taylor1993} 

Zunächst werden wir im nächsten Kapitel sehen, wie in den 1990er Jahre die verschiedenen makroökonomischen Entwicklungen ineinander griffen. Im Rahmen der DSGE-Modelle werden wird die gesamte Ökonomie nämlich auf drei Player eingeschränkt, die mittels Gleichungen dargestellt werden: Haushalte, Unternehmen und Zentralbanken. Und die Handlungen der Zentralbanken wiederum, werden mit der Taylor-Rule modelliert.

\section{DSGE: Die Zweckehe zwischen "`Neuen Klassikern"' und "`Neu-Keynesianern"'}

Dieses Unterkapitel rechtfertigt den Begriff "`Neue Neoklassische \textit{Synthese}"' wie kein anderes. \textit{Die} wesentliche Erweiterung in der Makroökonomie in den 1990er Jahren, war die Entstehung der ersten \textit{Dynamic, Stochastic, General Equilibrium}-Modelle (DSGE-Modelle). Wie wir im Kapitel \ref{Neue Makro} gelesen haben, wurde durch die Arbeiten von \textcite{Kydland1982, Plosser1983} die Methodik der Makroökonomie geradezu revolutioniert. Die dabei entwickelten Real-Business-Cycle-Modelle waren aus mathematisch-modelltheoretischer Sicht den in den 1970er Jahren vorherrschenden Keynesianischen und später auch Monetaristischen Totalmodellen, weit überlegen. Es waren walrasianische Gleichgewichtsmodelle, die damit die damals neu entstandene Mikrofundierung der Makroökonomie umsetzten. Ihre mathematische Eleganz passte perfekt in die Zeit der Formalisierung der Ökonomie, wenngleich die Lösung des Modells, mangels Möglichkeit das Modell analytisch zu lösen, ein paar "`Tricks"' erforderte. Die RBC-Modelle hatten nur einen Haken: Sie scheiterten grandios darin die Empirie gut abzubilden. Eigentlich ein Ausschlusskriterium für ökonomische Modelle. Die Methodik galt aber dennoch als zukunftsträchtig. 

Als Antwort auf diese "`Neu-klassischen"'-Modelle, entwickelten die "`Neu-Keynesianer"' punktuell Ansätze um empirisch beobachtete ökonomische Auffälligkeiten zu erklären. Diese Elemente wurden im letzten Kapitel (\ref{cha: Neu Keynes}) beschrieben. Die "`Verehelichung"' zwischen der RBC-Methodik und Neu-Keynesianischen Lösungsansätzen führte schließlich zu den ersten DSGE-Modellen \parencite[S. 5]{Gali2015}. Konkret wurde das mathematische RBC-Framework um folgende Neu-Keynesianische Elemente erweitert:
\begin{itemize}
	\item "`Nominale Rigiditäten"'
	\item "`Monopolistische Konkurrenz"'
	\item "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"'   
\end{itemize}

Die Kombination der "`neu-keynesianischen"' Ideen in die "`neu-klassischen"' Modelle begründet damit schließlich den Namen "`Neue Neoklassische Synthese"'\footnote{Dies ist aber insofern etwas verwirrend, als die meisten Entwickler von DSGE-Modellen, diese als "`Neu-Keynesianisch"' bezeichnen. Z.B.: \textcite{Gali2015}, \textcite{Romer2019}. \textcite[S. 28]{Gali2007} meint richtigerweise, dass sich der Begriff "`Neu-Keynesianisch"' weitgehend durchgesetzt hat, räumt aber ein, dass dieser Begriff eigentlich unzureichend ist!}.

Das Zusammenführen der beiden Ansätze verlief weniger geradlinig und einfach, als im letzten Absatz dargestellt. So gibt es verschiedene Ansätze \parencite[S. 310]{Romer2019} wie die dynamische Preisanpassung modelliert wird, was wiederum sehr unterschiedliche Auswirkungen der mikroökonomischen Rigiditäten auf die makroökonomischen Ergebnisse verursacht. Die DSGE-Modelle wurden zudem laufend erweitert. Insbesondere die Anfang der 1990er Jahre ebenfalls neuen Erkenntnisse, wie Geldpolitik praktiziert wird (siehe Kapitel \ref{Taylor-Rule}), sowie die Berücksichtigung der "`Neuen Phillipskurve"' (siehe Kapitel \ref{NeuePhillips}) zur Modellierung von Arbeitslosigkeit, stellen wesentliche Weiterentwicklungen dar.   

Konkret werden die drei angeführten Neu-Keynesianische Elemente durch zwei Annahmen eingeführt. Erstens, jedes Unternehmen produziert ein spezielles Produkt, für das es den Preis setzt. Damit ist die Annahme eines monopolistischen Konkurrenzmarktes erfüllt. Zweitens, nominale Preisrigiditäten werden modelliert, indem man annimmt, dass jedes Unternehmen nur zu einem bestimmten Zeitpunkt seine Preise an das Gleichgewicht anpassen kann \parencite[S. 52]{Gali2015}. Im Kapitel \ref{cha: Neu Keynes} wurden drei verschiedene Ansätze erwähnt, wie man dies simulieren kann. Jenen von \textcite{Fischer1977} und \textcite{Taylor1977}, jenen von \textcite{Taylor1979} und schließlich jenen von \textcite{Calvo1983}. Man kann heute sagen, dass sich der Ansatz von \textcite{Calvo1983} durchgesetzt hat. Dieses kann aus mathematischen Gründen am einfachsten in das DSGE-Framework integriert werden. Zur Erinnerung: Welches Unternehmen seinen Preis anpassen darf, wird hier durch eine Poisson-Verteilung, also durch einen Zufallsprozess, bestimmt. Die Annahme der "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"' ist durch die zwei eben genannten Elemente mitumfasst, denn die Preise sind im Ergebnis eben keine walrasianischen Gleichgewichtspreise mehr. Das heißt aber auch, dass sich auch der Geldwert nicht ständig an sein wahres Gleichgewicht anpasst. Diese Abweichung vom Gleichgewicht kann eben für Geldpolitik genutzt werden.

Darauf aufbauend kommen wir nun aber zum DSGE-Grundmodell. Dieses besteht aus drei Gleichungen. Jene für:
\begin{itemize}
	\item Haushalte (Aggregierte Nachfrage)
	\item Unternehmen (Aggregiertes Angebot)
	\item Zentralbank
\end{itemize}

Wie man sieht werden internationaler Handel, aber auch Staatsausgaben und damit Fiskalpolitik, nicht explizit betrachtet\footnote{Mit anderen Worten: Die Ricardianische Äquivalenz ist in DSGE-Modellen eine Standardannahme. Der Nachfrage-Multiplikator ist nicht größer als eins. Fiskalpolitik hat in der Folge keinen positiven Effekt auf das BIP (solange der nominale Zins gesteuert werden kann).}. Der repräsentative Haushalt hat eine unendliche Lebenserwartung und optimiert sein Verhalten unter der Prämisse, dass Konsum einen positiven Nutzen generiert, zu tätigende Arbeit generiert hingegen einen negativen Nutzen.  Außerdem wissen die Haushalte, dass es in der Zukunft zu zufälligen Schocks kommt. Haushalte optimieren also ihren Nutzen, gegeben ihren Erwartungen und dem Wissen um zufällige zukünftige Schocks. Die Optimierungsaufgabe liegt darin das Verhältnis aus Konsum und Arbeit für alle Zeitperioden festzulegen. Die Zukunft (zukünftige Werte), wird dabei - wie immer in der Ökonomie - abgezinst. Der Realzinssatz ist daher \textit{die} entscheidender Stellschraube bei der Optimierung.
Unternehmen haben eine Produktionsfunktion, die von einheitlicher Technologie ausgeht (das heißt nur, dass kein Unternehmen einen Vorteil aus einem technischen Fortschritt generieren kann). Der Produktionsfaktor Arbeit muss zum Nominallohn zugekauft werden. Die Unternehmen produzieren - wie schon erwähnt, Stichwort monopolistische Konkurrenz - jeweils ein einzigartiges Gut. Unternehmen können nur abhängig von einem Zufallsprozess - ebenfalls schon erwähnt bildet dies die nominalen Rigiditäten ab - ihre Preise an Marktgegebenheiten anpassen. Die Optimierungsaufgabe der Unternehmen besteht natürlich in der Gewinnmaximierung. Im Detail verlangt dies eine optimale Preisfestsetzung unter den Nebenbedingungen, dass Preisanpassungen erst wieder in einer zufällig zu bestimmenden Periode in der Zukunft möglich sein werden und in der Zwischenzeit sowohl Änderungen beim Nominalzinssatz, also auch bei Inflationsraten auftreten werden. Aufgrund der Annahme, dass monopolistische Konkurrenzmärkte vorherrschen, führt eine Abweichung des individuellen Preises eines Unternehmens vom Gleichgewichtspreis ja nicht zu einem totalen Rückgang des Gewinns. Unternehmen, die sich monopolistischer Konkurrenz ausgesetzt sehen, schlagen ja ohnehin immer ein "`Mark-up"' auf den Gleichgewichtspreis auf, um ihren Gewinn zu optimieren. Das heißt, Unternehmen setzen ihr Mark-up so, dass sie die Erwartungen ihrer zukünftigen Kostenänderung (durch Inflation) einpreisen und zusätzlich ihre Erwartungen der Dauer bis zur nächsten Preissetzung berücksichtigen. Ich denke es erscheint klar, erstens, dass dies mathematisch herausfordernd zu modellieren ist und Annahmen getroffen werden müssen, zweites, dass es unterschiedliche Ansätze gibt dies zu modellieren, drittens, dass der Prozess als Ganzes Kritik ausgesetzt werden kann, doch dazu später.
Als dritte Gleichung kommt jene der Geldpolitik ins Spiel. Wie wir bereits gesehen haben sind für Haushalte und Unternehmen vor allem die Variablen Nominalzins und Inflation - im Resultat also der Realzins -  von Bedeutung bei ihrer Optimierungsaufgabe. Und an dieser Stelle kommt wieder die Taylor-Regel\parencite{Taylor1993} ins Spiel, die wir bereits im letzten Kapitel \ref{Taylor-Rule} kennengelernt haben. Diese ist zwar nicht mikrofundiert, bestimmt aber zu jedem Zeitpunkt die resultierenden Handlungen der Zentralbanken. Es ist auch möglich, die Geldpolitik anders zu modellieren, aber im Ausgangsmodell hat sich das Konzept der Zinsregel etabliert. 


HIER WEITER:

mit ersten Modellen und Überleitung zu "`Neuen Phillips-Kurve"', die aus dem Calvo1983 abgeleitet wird.

Erstes Neu-keynesianisches DSGE-Modell: Rotemberg und Woodford
Rotemberg, Julio; Woodford, Michael (1993), "Dynamic General Equilibrium Models with Imperfectly Competitive Product Markets", NBER Working Paper No. 4502

Rotemberg, Julio; Woodford, Michael (1995). "Dynamic general equilibrium models with imperfectly competitive product markets". In Cooley, Thomas (ed.). Frontiers of Business Cycle Research. Princeton University Press. ISBN 0-691-04323-X.

Rotemberg, Julio J.; Woodford, Michael (1997). "An optimization-based econometric framework for the evaluation of monetary policy" (PDF). NBER Macroeconomics Annual. 12: 297–346. doi:10.1086/654340. JSTOR 3585236. S2CID 154438345


Yun (1996)


Als Standardmodell etablierte sich schließlich jenes Drei-Gleichungen-Modell, das \textcite{Gali2000} veröffentlichten\parencite[S. 311]{Romer2019}.

Drei Gleichungen:
\begin{itemize}
	\item Aggregiertes Angebot: Neu-Keynesianische Philips-Kurve
	\item Aggregierte Nachfrage: Neu-Keynesianische IS-Kurve
	\item Geldpolitik: Taylor-Rule
\end{itemize}



WElche Aussagen liefern die Modell überhaupt? 





\subsection{Erweiterungen}
Sticky Wages und Sticky Prices: Kapitel 6 im Gali-Buch!
Unemployment (Neue Phillips Kurve?): Kapitel 7 im Gali-Buch! + Romer-Buch S. 327ff + S. 338. Ursprünglich: Roberts 1995!
"`HANK"'!: Kapitel 9 im Gali-Buch!


\subsection{Erweiterungen}


\subsection{Neue Phillips Kurve}
\label{NeuePhillips}

In Gali and Gertler (1999, 2007, S. 32ff) and Gali, Gertler and Lopez-Salido (2005),
Rotemberg 1982 (laut Gali-Buch, S. 80)


Jordi Gali usw nach 2008




Richard Clarida and Jordi Gali and Mark Gertler, 2000. "Monetary Policy Rules and Macroeconomic Stability: Evidence and Some Theory," The Quarterly Journal of Economics, Oxford University Press, vol. 115(1), pages 147-180.

Mark Gertler and Jordi Gali and Richard Clarida, 1999. "The Science of Monetary Policy: A New Keynesian Perspective," Journal of Economic Literature, American Economic Association, vol. 37(4), pages 1661-1707, December.



Inhaltlich spielt in der Wirtschaftspolitik fast ausschließlich nur mehr die Geldpolitik eine Rolle. Zur Fiskalpolitik haben die Vertreter der "`neuen Neoklassischen Synthese"' praktisch ausschließlich eine ablehnende Haltung.






















RBC + \textcite{RomerDavid1990}

Formulierung der Taylor-Rule (1993?) als  Übergangszeitpunkt 1. Generation --> 2. Generation
Zweiter Übergangspunkt: Ab 1990 viel stärker empirisch (bis dahin sehr theoretische Arbeiten der Neu-Keynesianer)

Problem der Arbeitslosigkeit trat in den Vordergrund.



Neue Phillips Kurve

Cost of Inflation (\textcite{Snowdon2005} ab S. 401) 


Wirtschaftspolitisch Dominanz der Geldpolitik
Fiskalpolitik selbst in Krisen umstritten (Paper zur empirischen Bestimmung der Wirksamkeit der Fiskalpolitik (Blanchard))



Interessant ist, das

Elemente aus verschiedenen Schulen:
\begin{itemize}
	\item Keynesianismus: Rigiditäten. Teilweise Fiskalpolitik im Krisenfall
	\item Monetarismus: Zentrale Bedeutung der Geldpolitik und der Zentralbanken, Natürliche Arbeitslosigkeit
	\item Österreichische Schule: Konzept des Gleichgewichtszinssatzes (Wicksell)
	\item Neu-Keynesianismus: "`Nominale Rigiditäten"', "`Monopolistische Konkurrenz"' und "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"'. Älter: NAIRU, Marktversagen
	\item Neue Klassische Makroökonomie: Annahme "`Rationale Erwartungen"', Real-Business-Cycle-Modelle.
	\item Post-Keynes: Mark-up (monopolistische Konkurrenz) kommt eigentlich daher.
\end{itemize}





\section{Auf den Schultern von Giganten}
\label{Giganten}

\subsection{Krugman}

\subsection{Blanchard}

\subsection{David Romer \& Mankiv}





Solow (2010) äußerte sich sehr kritisch gegenüber DSGE-Modellen. Er vertrat die Meinung, dass 
DSGE-Modelle keinen Bezug zur Realität hätten und daher ungeeignet für die Politikberatung 
seien.


Blanchard 2016: Do DSGE Models have a Future

Romer, Paul, 2016, The trouble with macroeconomics, in: The American Economist, forthcomin










