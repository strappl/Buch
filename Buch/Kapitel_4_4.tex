%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neue Neoklassische Synthese}
\label{Neue Neoklassische Synthese}

Mit diesem Kapitel sind wir in der Gegenwart der Ökonomie angekommen. Man kann zwar durchaus argumentieren, dass die Makroökonomie nach der weltweiten Wirtschaftskrise ab 2008 und der immer noch praktizierten globalen Nullzinspolitik eine erneute "`Revolution"' nötig hätte, aber Stand 2022 ist die State-of-the-Art Mainstream-Ökonomie die \textit{Neue Neoklassische Synthese}. Der Begriff "`Neue Neoklassische Synthese"' ist (noch) nicht wirklich etabliert als Bezeichnung für den aktuellen wirtschaftswissenschaftlichen "`Mainstream"'. Meist spricht man stattdessen von "`Neu-Keynesianismus"' oder auch von "`Neoklassik"'\footnote{In Lehrbüchern wird häufig ohne Unterschied vom "`Neu-Keynesianismus"' gesprochen. Auch "`Neu-Keynesianismus der 2. Generation"', "`Neue Synthese"', "`Neue Keynesianische Synthese"'  kommen vor. Selten werden die Modelle auch als "`Neo-Wicksellianisch"' bezeichnet \parencite[S. 28]{Gali2007}, dies wegen der Ähnlichkeit zur Theorie von Wicksell, der Abweichungen vom natürlichen Gleichgewicht beschreibt}. Beide Begriffe sind aber nicht eindeutig. Um Unklarheiten zu vermeiden wird hier der etwas holprige, aber eindeutige und inhaltlich meiner Meinung nach passende Begriff "`Neue Neoklassische Synthese"' (oder schlicht "`Neue Synthese"') verwendet.

Ungefähr um 1990 versuchten Ökonomen, die nicht vom Streit zwischen Neu-Klassikern und Neu-Keynesianern vorbelastet waren, unvoreingenommen das beste aus beiden Welten zu übernehmen und zu einer "`Neuen Synthese"' zusammen zuführen. Die Abgrenzung zwischen "`Neu-Keynesianismus"' und "`Neuer Synthese"' ist hierbei sowohl inhaltlich als auch zeitlich verlaufend. Vor allem, weil viele Ökonomen, die uns im letzten Kapitel untergekommen sind, auch in diesem Kapitel die "`Hauptdarsteller"' sein werden. Es gibt aber auch durchaus Abspaltungen bei den Vertretern des "`Neu-Keynesianismus"': Paul Krugman und Joseph Stiglitz, zum Beispiel, lehnen viele Ansätze der "`Neuen Synthese"' weitgehend ab. Mankiw, Blanchard und David Romer sind schwieriger einer der beiden Schulen zuzuordnen, sie stehen für den Übergang von "`Neu-Keynesianismus"' zur "`Neuer Synthese"'. Ein zentraler Vertreter der "`Neuen Sythese"' (ohne Vergangenheit im "`Neu-Keynesianismus) ist Jordi Gali. Ein spezieller Vertreter ist John Taylor. Er begründete - gemeinsam mit \textcite{Phelps1968} und \textcite{Fischer1977} - den "`Neu-Keynesianismus"' mit \parencite{Taylor1977}, und auch für die "`Neue Synthese"' lieferte er einen der grundlegenden Beiträge \parencite{Taylor1993}. Es gibt heute in der Ökonomie nicht mehr jene klar abgrenzbaren, konkurrierenden Schulen, die die Wirtschaftsgeschichte des 20. Jahrhunderts geprägt haben: Neoklassiker vs. Keynesianer, Keynesianer vs. Monetaristen, Neu-Klassiker vs. Neu-Keynesianer. Vielmehr ist die gesamte Mainstream-Ökonomie unter einem sehr breiten Dach zusammengefasst. Was aber nicht bedeutet, dass unter diesem Dach alle einer Meinung sind, ganz im Gegenteil.\footnote{Neben diesem Mainstream, gibt es immer noch mehrere heterodoxe Schulen, die im Teil \ref{Heterodox} beschrieben werden.}

Passend dazu verschwammen auch die ideologischen Unterschiede zwischen den verschiedenen ökonomischen Gruppen. Konnte man bis in die 1990er Jahre hinein die ökonomischen Richtungen meist auch einer politischen Richtung zuweisen, ist dies heute nicht mehr möglich. Sozialdemokraten (Kontinental-Europa), Labour-Party (UK) und Demokraten (USA) waren fast ausschließlich dem (Neu-) Keynesianismus zugeneigt. Christ-Demokraten (Kontinental-Europa), Tories (UK) und Republikaner (USA) meist den Neoklassikern,  Monetaristen und Neuen Klassikern. Das Spektrum der Vertreter der Neuen Synthese reicht vom Erzliberalen John Taylor über den bekennenden Republikaner Mankiw bis zu Janet Yellen, die Finanzministerin im Kabinett des demokratischen US-Präsidenten Joe Biden ist. 

Bevor wir uns die rein ökonomischen Aspekte der "`Neuen neoklassischen Synthese"' im Detail ansehen, blicken wir auf das Umfeld. Welchen Herausforderungen waren die Volkswirtschaften Anfang der 1990er Jahre ausgesetzt? Politisch gesehen war natürlich der Zusammenbruch der Sowjetunion und damit des real existierenden Sozialismus dominierend. Die Marktwirtschaft, also die Grundlage fast aller in diesem Buch beschriebenen Ideen, hatte sich durchgesetzt. Der Kommunismus, der ohnehin nie so wie von Marx beschrieben praktiziert wurde, galt endgültig als gescheitert. In den westlichen Marktwirtschaften trat das Problem der Inflation in den Hintergrund. Dafür traten Probleme der Arbeitslosigkeit in den Vordergrund, in Europa stärker ausgeprägt als in den USA. Das Problem der steigenden Staatsschulden wurde zunehmend thematisiert, mit ein Grund warum Fiskalpolitik aus dem Fokus geriet. Wechselkurssysteme waren Anfang der 1990er zwar noch einmal ein Thema, als sich nacheinander mehrere europäische Zentralbanken dem Treiben von Spekulanten ausgesetzt sahen, die versuchten die fixierten Wechselkurse zu manipulieren. Doch mit der Schaffung der Europäischen Wirtschafts- und Währungsunion, die in der Einführung des Euros gipfelte, verlor diese Thematik an Bedeutung. Technologisch Begann mit den frühen 1990er Jahren das Zeitalter der Computer. Rechenmaschinen wurden für Haushalte und Kleinunternehmen leistbar und veränderten damit auch das zentrale Verwaltungssystem. Damit verbunden waren wesentliche Verbesserungen in der Datenverfügbarkeit und der Datenauswertung. Die in diesem Buch nicht gesondert behandelte Ökonometrie, sowie die empirische Wirtschaftswissenschaft erfuhr in dieser Zeit einen enormen Aufschwung. Das ist nicht unwesentlich bei der nun folgenden Betrachtung der Weiterentwicklung der Makroökonomie. Die modernen DSGE-Modelle, die im folgenden erläutert werden, sind nur numerisch und damit mit hohem Rechenaufwand zu lösen. 

Zurück zur Entwicklung der Ökonomie: Es gibt vor allem zwei Punkte, die sich Anfang der 1990er-Jahre entwickelt haben und die Makroökonomie und deren Wirtschaftspolitik seither eindeutig prägen und sehr wohl eine eindeutige Abgrenzung vom Neu-Keynesianismus und der Neuen Klassik ermöglichen:
\begin{enumerate}
	\item Erstens, in der makroökonomischen Theorie: die Zusammenführung der formal-mathematischen Real-Business-Cycle-Gleichgewichtsmodelle mit Elementen der Neu-Keynesianer.
	\item Zweitens, in der Wirtschaftspolitik: Die Dominanz der Bedeutung der Geldpolitik und der Aufstieg der Zentralbanken zum wichtigsten wirtschaftspolitischen Player.
\end{enumerate}



\section*{Exkurs: Die Verwissenschaftlichung der Zentralbanken}

Texte über Funktion, Entstehung und Evolution von Zentralbanken könnten ganze Bibliotheken füllen. Hier soll nur ein ganz kurzer Abriss über Zentralbanken im allgemeinen gemacht werden. Der amerikanische Entertainer Will Rogers soll bereits 1920 gesagt haben: "`There have been three great inventions since the beginning of time: fire, the wheel, and central banking."' Das ist zwar deutlich übertrieben, aber tatsächlich scheint die Arbeit von Zentralbanken\footnote{Gemeint ist hier die Durchführung der Geldpolitik.} für überraschend viele Menschen ein Mysterium. Dass es ganz grundlegende Unterschiede gibt, ob eine Regierung oder aber eine Zentralbank Geld ausgibt, verstehen viele nicht. Umgekehrt glauben viele, dass Geld ausschließlich von der Zentralbank geschaffen wird. Und um ehrlich zu sein, ist das Geldsystem wesentlich komplizierter als es auf den ersten Blick wirkt. Auch dieses Buch muss diesbezüglich an der Oberfläche bleiben\footnote{Möglicherweise bleibt der Beitrag sogar zu stark an der Oberfläche, aber die Alternative wäre, dass der Fokus auf das eigentliche Thema verloren ginge.}. Wir betrachten hier die historische Entwicklung, die Zentralbanken vor allem im 20. Jahrhundert durchliefen. Zentralbanken werden häufig die "`Hüterinnen der Währung"' genannt. Geld ist evolutionär so entstanden, dass sich ein bestimmter "`Kreis an Personen"' auf bestimmte Gegenstände als Tauschmittel geeinigt hat. Das hat den Vorteil, dass zum Beispiel ein Fischer, der zur Abwechslung einmal Kartoffel erwerben wollte, nicht solange suchen musste, bis er einen Ackerbauern gefunden hat, der selbst Fische erwerben wollte. Stattdessen konnte der Fischer seine Ware an jeden Fisch-Liebhaber gegen Geld eintauschen und mit dem Geld Kartoffel erwerben. Der Nachteil des Ganzen: Der "`Kreis der Personen"' muss sich auf ein Gut einigen, das alle als Tauschmittel akzeptieren. Das Gut muss also einen "`allgemeinen Wert"' haben. Niemand kann genau sagen warum, aber bestimmte Edelmetalle, vor allem Gold, hat sich als zentrales Tauschmittel früh etabliert. Zur Einordnung: Wir reden hier von vorchristlichen Zeiten, also lange vor der Entstehung der Ökonomie als Wissenschaft. Die ständige Mitnahme seiner Geldreserven in Form von purem Gold erwies sich aber rasch als unpraktisch. Erstens, Gold ist schwer, zweitens es nutzt sich ab, verliert also an Wert und drittens, in der Praxis ist es recht schwer den \textit{reinen} Goldgehalt, zum Beispiel einer Münze, festzustellen. Die Heureka-Geschichte des Archimedes diesbezüglich ist ja recht gut bekannt. Daher ging man dazu über seine Edelmetalle einer zentralen Institution - genannt Bank - zu übergeben, die dafür wiederum ein Dokument (Wechsel) ausstellte. Die Idee ist, dass dieses - an sich wertlose Dokument - einem potentiellen Käufer signalisieren soll: Ich habe Gold bei der Institution Bank liegen, wenn du mir deine Güter gibst, so gebe ich dir das Dokument und du kannst dir mein Gold von der Bank holen. Im recht kleinen Personenkreis funktioniert das recht gut. Allerdings müssten alle Personen ihre Goldreserven bei der gleichen Bank einlagern, damit das System reibungslos funktioniert. Ist \textit{die eine} Bank räumlich weit weg, ist ein Dokument von dieser Bank umständlich gegen Gold einzutauschen. Es entstanden also lokal verschiedenste Banken, die verschiedenste Dokumente ausstellten. Um dennoch ein einheitliches und damit reibungsloses Geldsystem zur Verfügung zu stellen, müssten diese Banken wiederum eine zentrale Institution gründen, die ein einheitliches Dokument ausstellt. Meist trat spätestens hierbei die Politik in das Spiel ein. Die herrschende Macht - weitgehend egal übrigens ob König, Parlament oder Diktator - gründete diese "`Bank der Banken"' und damit das was wir heute "`Zentralbank"' nennen.

Das eben beschriebene System wäre ein "`reiner Goldstandard"'\footnote{Eigentlich müsste es "`Edelmetallstandard"' heißen, denn als hinterlegtes Gut fungierten zunächst häufig auch andere Edelmetalle, tatsächlich Silber häufiger als Gold. Namensgebend ist aber jenes System, dass sich 1870, ausgehende von Großbritannien, weltweit durchgesetzt hat. Im "`reinen Goldstandard"' im engeren Sinn wird das vorhandene Gold tatsächlich zu Münzen geprägt und als Zahlungsmittel verwendet. Ökonomisch gesehen ist es aber von untergeordneter Bedeutung ob das Gold tatsächlich im Umlauf ist, oder ob Geldscheine stattdessen verwendet werden. Wichtig ist, dass stets die \textit{gesamte} Geldmenge durch Gold repräsentiert ist.}. Die Bezeichnung von Währungen erinnert teilweise noch heute an dieses System: Zum Beispiel "`Pfund Sterling"'. Pfund ist eine Gewichtseinheit (Masseneinheit), Sterling eine Metalllegierung aus Silber und Kupfer. Auf den Geldscheinen steht ein Satz, der uns heute seltsam anmutet und auch nicht mehr wörtlich genommen werden darf: "`I promise to pay the bearer on demand the sum of 10 pounds"'. Also: "`Dem Inhaber der Banknote werden 10 Pfund bezahlt"'!? Der Inhaber der Banknote \textit{hat} ja schon den 10-Pfund-Schein. Gemeint ist aber eben, dass man für diesen Schein 10 Pfund Sterling erhält. Was aber eben auch nicht mehr stimmt. Die Englische Zentralbank "`Bank of England"' hat keine Eintauschverpflichtung gegen Silber. Auf den Pfund-Sterling-Banknoten ist außerdem die Queen abgebildet, was suggeriert, dass das Herrscherhaus das Geldsystem kontrolliert. Auch das ist überholt: Wir werden später sehen, dass die Unabhängigkeit der Zentralbanken bis heute ein zentrales Thema in der Geldpolitik ist. Die einzige Aufgabe einer Zentralbank besteht im "`reinen Goldstandard"' darin ein Edelmetall einzulagern und für eine gewisse Menge dieses Edelmetalls genau einen Geldschein auszugeben. Die Menge an Geldscheinen kann nur dann steigen, wenn auch die Menge an Edelmetall im selben Ausmaß steigt. Aktive Geldpolitik ist in diesem Fall nicht möglich. Diese Form des Geldsystems galt lange Zeit als das einzig stabile und denkbare System. Herrscher wichen meist nur deshalb davon ab, um sich selbst einen Vorteil zu verschaffen, indem sie zum Beispiel zusätzliches - nicht durch Gold gedecktes - Geld druckten um ihre Schulden zu bezahlen. Man nennt das "`Seignorage"' und es führt fast immer zu (Hyper-)Inflation, Vertrauensverlust in die Währung und schließlich eine Wirtschaftskrise. Papiergeld war daher lange Zeit sehr unbeliebt und musste der Bevölkerung aufgezwungen werden. Auch nach dem Zweiten Weltkrieg gab es aus diesem Grund in vielen Währungen noch Münzen mit Edelmetallgehalt - nicht ausschließlich zu Sammelzwecken, sondern es wurde damit tatsächlich bezahlt. Die klassischen Ökonomen hielten den "`reinen Goldstandard"' als einzig denkbares, stabiles Währungssystem und Geld als reines Tauschmittel. Heute gilt der Goldstandard als veraltetes System, wenngleich zum Beispiel in der "`Österreichischen Schule"' (vgl. Kapitel \ref{Austria}) nach wie vor immer wieder die Vorzüge des Goldstandards propagiert werden.  

Mit der ersten Globalisierungswelle, Ende des 19. Jahrhunderts, erhielten die Zentralbanken eine neue Aufgabe, nämlich das Aufrechterhalten stabiler Wechselkurse. Mit dem "`klassischen Goldstandard"' wurde dieses System etabliert. Es folgt dem "`Price-Specie-Flow"'-Mechanismus (vgl. Kapitel \ref{Klassik}): Werden Güter aus Land A exportiert, wird gleichzeitig Kapitel - und damit indirekt Gold - importiert. Die Warenmenge fällt durch den Export, die Geldmenge steigt. Im Importland B geschieht aber genau das Gegenteil hier gibt es nach dem Import mehr Güter aber weniger Gold. In diesem einfachen Modell sorgen die Marktkräfte dafür, dass ein Außenhandelsgleichgewicht entsteht. Durch unterschiedliche Zinssätze in verschiedenen Staaten konnte dieses Gleichgewicht gestört werden. Hier kommen die Zentralbanken ins Spiel: Diese müssen international abgestimmt ihre Zinssätze so anpassen, dass das Gleichgewicht beibehalten wird. Als führende Zentralbank gab hierbei die "`Bank of England"' den Ton an. Die politischen Umwälzungen führten schließlich dazu, dass der Goldstandard mit Beginn des Ersten Weltkriegs aufgegeben wurde. Schon zuvor wurde übrigens aus dem "`reinen Goldstandard"' ein "`Proportionalsystem"'. Hierbei geht man davon aus, dass niemals gleichzeitig das ganze Geld gegen Gold eingetauscht wird und man druckt in weiterer Folge für eine bestimmte Goldmenge eine größere Geldmenge. Ein vorab festgelegtes Verhältnis darf dabei aber niemals überschritten werden. Nach dem Ersten Weltkrieg wurde der Goldstandard wieder eingeführt, allerdings wenig erfolgreich. Deutschland und Österreich litten ab Anfang der 1920er-Jahre an großen wirtschaftlichen Problemen, die in Hyperinflation mündeten. Der britische Pfund war überbewertet und die Bank of England hatte Mühe ihre Verpflichtungen aufrechtzuerhalten. In den USA wiederum war das Wirtschaftswachstum hoch und dauernde Exportüberschüsse wurden verzeichnet. Mit der Weltwirtschaftskrise ab 1929 erfuhr der Goldstandard seinen Tiefpunkt. \textcite{Friedman1963} wurde später dafür berühmt, gezeigt zu haben wie die Federal Reserve während der "`Great Depression"' zu lange auf den Goldstandard gesetzt hatte und damit die Schwere der Krise verstärkt hatte. Noch während des Zweiten Weltkriegs wurde 1944 im gleichnamigen "`Bretton Woods"' die Grundlage für eine neues, einheitliches Währungssystem geschaffen. Diesmal ist der US-Dollar die Leitwährung und praktisch alle Industriestaaten binden ihre Währung an jene der Amerikaner. Diesmal sind es die Inflationstendenzen des US-Dollar bei gleichzeitiger guter wirtschaftlicher Entwicklungen der europäischen Staaten, die dazu führen, dass das System in den 1970er Jahren aufgegeben werden muss.

Damit begann die Zeit der Zentralbanken und der Aufstieg der Bedeutung der Geldpolitik. Hier spielten viele Faktoren zusammen: Bei den Keynesianern steht die Fiskalpolitik im Vordergrund, Geldpolitik wird zwar als wichtig anerkannt, aber eher als Ergänzung im Zusammenspiel mit der Fiskalpolitik. Bereits Mitte der 1960er-Jahre veröffentlichten \textcite{Mundell1963} und \textcite{Fleming1962} ihr "`Impossible Trinity"'-Modell (vgl. Kapitel \ref{Supply-Side-Economics}). Darin beschreiben Sie, dass ein Staat grundsätzlich flexible Wechselkurse, freien Kapitalverkehr und aktive Geldpolitik betreiben möchte. Es sind aber stets nur zwei der drei Ziele miteinander vereinbar. Da der freie Kapitalverkehr in westlichen Demokratien unverhandelbar ist, muss eine Entscheidung zwischen den beiden anderen Zielen getroffen werden. Oder mit anderen Worten: Bis in die 1970er-Jahre waren die Zentralbanken an die Aufgabe gebunden fixe Wechselkurse aufrecht zu erhalten. Erst danach konnten andere wirtschaftspolitische Ziele ins Auge gefasst werden. Die 1970er-Jahre waren zudem die hohe Zeit der Monetaristen. \textcite{Friedman1968, Friedman1976b} wollte die Zentralbanken zwar zugunsten einer fixen Wachstumsrate des Geldes "`abschaffen"', aber er stellte auch die Bedeutung der Geldpolitik eindrucksvoll in den Vordergrund. Ähnliches gilt für die "`Neue Klassische Makroökonomie"', die grundsätzlich jegliche wirtschaftspolitischen Eingriffe als nutzlos erachtete. Dennoch lieferten die Arbeiten von zum Beispiel \textcite{Kydland1977, Barro1976} wichtige Beiträge, vor allem zur Organisation von Zentralbanken als \textit{unabhängige} Institutionen. Die Theorien der modernen Politischen Ökonomie, die ab den späten 1980er-Jahren an Bedeutung erlangten (vgl. Kapitel \ref{Pol_Econ}), untermauerten dies. Die "`Neue Neoklassische Synthese"' schließlich machte die Zentralbanken - wie erwähnt - zum zentralen Player der Wirtschaftspolitik. Zunächst in der Theorie: Wenn es die neu-keynesianischen Elemente "`Nominalen Rigiditäten"' und "`Monopolistische Konkurrenz"' in der Praxis gibt, dann ist Geldpolitik in der kurzen Frist eben doch nicht wirkungslos. Fiskalpolitik hingegen spielt in den DSGE-Modellen der "`Neuen Neoklassische Synthese"' hingegen keine Rolle.

In der Praxis war die Entwicklung etwas zeitverzögert, aber heute gibt es auch hier soetwas wie einen globalen Konsens der Geldpolitik. In den USA begann der Umbruch in der Geldpolitik 1979 mit der Bestellung von Paul Volcker zum Leiter der Fed. Er führte einen schmerzhaften aber erfolgreichen Disinflations-Kurs und schaffte es, durch harte restriktive Geldpolitik, die hohe US-Inflation in den Griff zu bekommen. In der Greenspan-Ära von 1987-2006 wurde die enorme Macht eines Notenbankchefs erstmals einer breiten Bevölkerung bewusst. Seine Reden und Aussagen wurden vor allem an den Finanzmärkten peinlichst genau analysiert. In der Ära Bernanke 2006-2014 und auch Yellen (2014-2018) trat eine deutliche Verwissenschaftlichung der Zentralbank hervor. In Kontinentaleuropa prägte stets die äußerst auf Stabilität pochende Deutsche Bundesbank die Geldpolitik. Vor allem im Europäischem Währungssystem (EWS) und später in der Phase vor der Euro-Einführung 1999, in der die Teilnahme-Länder die Stabilitätskriterien nach Maastricht einhalten mussten. Die europäischen Zentralbanken hatten vor allem in den frühen 1990er Jahren damit zu kämpfen, die festgelegten Wechselkurse aufrechtzuerhalten. Spekulanten identifizierten immer wieder Überbewertungen verschiedener Währungen gegenüber der harten Deutschen Mark. Berühmt wurde der Investor George Soros, der 1992 auf die Überbewertung des Britischen Pfunds setzte und bei dessen Abwertung schließlich eine hohe Summe gewann. In dieser Zeit waren die europäischen Zentralbanken also sowas wie "`Spieler"' auf den internationalen Devisenmärkten. Mit der Einführung des Euros und der damit verbundenen Schaffung der Europäischen Zentralbank (EZB), entstand der heute zweitwichtigste geldpolitische Akteur nach der US-Notenbank Fed. Die EZB ist dahingehend interessant, weil sie als unabhängiges Institut gegründet wurde und zudem als Primärziel einzig die Geldwertstabilität - also explizit festgelegtes Inflation-Targeting - festgeschrieben hat (siehe weiter unten in diesem Kapitel \ref{Inflation}). Die Zentralbanken haben sich also auch in der zweiten Hälfte des 20. Jahrhunderts kräftig gewandelt: Bis in die 1970er-Jahre mussten sie im Rahmen des "`Bretton-Woods-Systems"' die Währungskurse stabil halten. Eine Aufgabe, die im wesentlichen von der Politik vorgegeben wurde. Bald danach begann der Versuch der Geldmengensteuerung im Sinne von Friedman. Bis schließlich das Anstreben einer Zielinflation in den Vordergrund rückte.



\section{DSGE: Die Zweckehe zwischen "`Neuen Klassikern"' und "`Neu-Keynesianern"'}

Dieses Kapitel rechtfertigt den Begriff "`Neue Neoklassische \textit{Synthese}"' wie kein anderes. \textit{Die} wesentliche Erweiterung in der Makroökonomie in den 1990er Jahren, war die Entwicklung der ersten \textit{Dynamic, Stochastic, General Equilibrium}-Modelle (DSGE-Modelle). Wie wir im Kapitel \ref{Neue Makro} gelesen haben, wurde durch die Arbeiten von \textcite{Kydland1982, Plosser1983} die Methodik der Makroökonomie geradezu revolutioniert. Die dabei entwickelten Real-Business-Cycle-Modelle waren aus mathematisch-modelltheoretischer Sicht den in den 1970er Jahren vorherrschenden Keynesianischen und später auch Monetaristischen Totalmodellen, weit überlegen. RBC-Modelle sind walrasianische Gleichgewichtsmodelle, welche die damals neu entstandene Mikrofundierung der Makroökonomie umsetzten. Ihre mathematische Eleganz passte perfekt in die Zeit der Formalisierung der Ökonomie. Wenngleich die Lösung dieser Modelle, mangels Möglichkeit das Modell analytisch zu berechnen, ein paar "`Tricks"' erforderte. Die RBC-Modelle hatten nur einen Haken: Sie scheiterten grandios darin die Empirie gut abzubilden \parencite[S. 309]{Romer2019}. Eigentlich ein Ausschlusskriterium für ökonomische Modelle. Die Methodik galt aber dennoch als zukunftsträchtig. 

Als Antwort auf diese "`Neu-klassischen"'-Modelle, entwickelten die "`Neu-Keynesianer"' punktuell Ansätze um empirisch beobachtete ökonomische Auffälligkeiten zu erklären. Diese Elemente wurden im letzten Kapitel (\ref{cha: Neu Keynes}) beschrieben. Die "`Verehelichung"' zwischen der RBC-Methodik und Neu-Keynesianischen Lösungsansätzen führte schließlich zu den ersten DSGE-Modellen \parencite[S. 5]{Gali2015}. Konkret wurde das mathematische RBC-Framework um folgende Neu-Keynesianische Elemente erweitert:
\begin{itemize}
	\item "`Nominale Rigiditäten"'
	\item "`Monopolistische Konkurrenz"'
	\item "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"'   
\end{itemize}

Die Kombination der "`Neu-Keynesianischen"' Ideen mit dem "`Neu-Klassischen"' Modellansatz begründet damit den Namen "`Neue Neoklassische Synthese"'\footnote{Dies ist aber insofern etwas verwirrend, als die meisten Entwickler von DSGE-Modellen, diese als "`Neu-Keynesianisch"' bezeichnen. Z.B.: \textcite{Gali2015}, \textcite{Romer2019}. \textcite[S. 28]{Gali2007} meint richtigerweise, dass sich der Begriff "`Neu-Keynesianisch"' weitgehend durchgesetzt hat, räumt aber ein, dass dieser Begriff eigentlich unzureichend ist!}.

Das Zusammenführen der beiden Ansätze verlief weniger geradlinig und einfach, als im letzten Absatz dargestellt. So gibt es verschiedene Ansätze \parencite[S. 310]{Romer2019} wie die dynamische Preisanpassung modelliert wird, was wiederum sehr unterschiedliche Auswirkungen der mikroökonomischen Rigiditäten auf die makroökonomischen Ergebnisse verursacht. Die DSGE-Modelle wurden zudem laufend erweitert. Insbesondere die Anfang der 1990er Jahre ebenfalls neuen Erkenntnisse, wie Geldpolitik praktiziert wird (siehe Unterkapitel \ref{Taylor}), sowie die Berücksichtigung der "`Neuen Phillipskurve"' (siehe Unterkapitel \ref{NeuePhillips}) zur Modellierung von Arbeitslosigkeit, waren wesentliche Weiterentwicklungen des Grundmodells und sind heute wichtige Bausteine der DSGE-Modelle.

Konkret werden die drei soeben angeführten Neu-Keynesianischen Elemente durch zwei Annahmen eingeführt. Erstens, jedes Unternehmen produziert ein spezielles Produkt, für das es den Preis \textit{setzt}. Damit ist die Annahme eines monopolistischen Konkurrenzmarktes erfüllt. Zweitens, nominale Preisrigiditäten werden modelliert, indem man annimmt, dass jedes Unternehmen nur zu einem bestimmten Zeitpunkt seine Preise an das Gleichgewicht anpassen kann \parencite[S. 52]{Gali2015}. Im Kapitel \ref{cha: Neu Keynes} wurden drei verschiedene Ansätze erwähnt, wie man dies simulieren kann. Jenen von \textcite{Fischer1977} und \textcite{Taylor1977}, jenen von \textcite{Taylor1979} und schließlich jenen von \textcite{Calvo1983}. Man kann heute sagen, dass sich der Ansatz von \textcite{Calvo1983} durchgesetzt hat. Dieser kann aus mathematischen Gründen am einfachsten in das DSGE-Framework integriert werden. Zur Erinnerung: Welches Unternehmen seinen Preis anpassen darf, wird hier durch eine Poisson-Verteilung, also durch einen Zufallsprozess, bestimmt. Die Annahme der "`Nicht-Neutralität der Geldpolitik in der kurzen Frist"' ist durch die zwei eben genannten Elemente mitumfasst, denn die Preise sind im Ergebnis eben keine walrasianischen Gleichgewichtspreise mehr. Das heißt aber auch, dass sich auch der Geldwert nicht ständig an sein wahres Gleichgewicht anpasst. Diese Abweichung vom Gleichgewicht kann eben für Geldpolitik genutzt werden.

Die drei folgenden Unterkapitel stellen die "`Bausteine"' der "`Dynamisch Stochastischen General Equilibrium"' (DSGE) Modelle dar. Diese sind ursprünglich alleinstehend entwickelt worden. 

\subsection{Die Neu-Keynesianische IS-Kurve}
\label{NeueIS}

Das berühmte IS-LM-Modell hat bis heute in den einführenden VWL-Büchern überlebt. Vor allem als Instrument der kurzfristigen Gleichgewichtsanalyse. Natürlich gilt das Modell heute nicht mehr als zeitgemäß: Inflation spielt darin keine Rolle, des unterscheidet nicht zwischen realen und nominalen Zinsen. Von Mikrofundierung war zu seiner Entstehung im Jahr 1937 noch lange nicht die Rede. Mitte der 1990er Jahre stellten sich Ökonomen schließlich die Frage, wie man die aggregierte Nachfrage zukunftsorientiert darstellen könnte. Das Optimierungsverhalten der Haushalte ist schon aus dem Kapitel \ref{Arrow-Debreu} bekannt. Im Ramsey-Cass-Koopmans-Modell liefert Konsum positiven und Arbeit negativen Nutzen. Konsum kann mittels Realzinssatz von einer Periode in die nächste übertragen werden. \textcite{Kerr1996, McCallum1999} leiteten daraus eine zukunftsorientierte \textit{Neu Keynesianische IS-Kurve} ab. Dazu stellen sie fest, dass man unter der Prämisse rationaler Erwartungen von \textit{einem} repräsentativen Haushalt ausgeht. Im Gleichgewicht gilt nach wie vor, dass Konsum gleich Gesamtoutput (BIP) gilt. Die Ramsey-Cass-Koopmans-Konsumtheorie wird damit so umgestellt, dass man das aktuelle BIP durch das erwartete, zukünftige BIP erklärt. Zusätzlich wird jener Anteil am Einkommen, den man in spätere Perioden verschiebt und dafür Zinsen generiert, vom aktuellen BIP abgezogen. Je höher der Realzinssatz, desto höher wird der Anteil sein, den man erst in zukünftigen Perioden konsumiert. Das ist auch der entscheidende Punkt der Neu-Keynesianischen IS-Kurve: Es gibt einen negativen Zusammenhang zwischen BIP und Realzins und die Erklärung des Gesamtoutputs (BIP) ist zukunftsorientiert. Vom Ergebnis her unterscheiden sich die Neu-Keynesianische IS-Kurve und die traditionellen IS-Kurve auf den ersten Blick nur wenig: In beiden Fällen gibt es den negativen Zusammenhang zwischen BIP und Realzins. Von der Idee her \parencite[S. 241]{Romer2019} unterscheiden sich beide deutlich: Bei der Neu-Keynesianischen IS-Kurve reagiert der \textit{Konsum} negativ auf den Realzins, während die Investitionen in der Herleitung gar nicht vorkommen. Im traditionellen Modell ist es genau umgekehrt, \textit{IS} steht hierbei ja auch für \textit{I}nvestition und \textit{S}paren. 
Ein kleines Gedankenexperiment aus \textcite[S. 243]{Romer2019} soll vorweg andeuten, warum sich diese Neu-Keynesianische IS-Kurve so gut eignet, Nominale Rigiditäten (vgl. Kapitel \ref{Nominale Rigiditäten}) in ein Gleichgewichtsmodell zu implementieren. Erinnern sie sich an das traditionelle IS-LM-Modell. Die IS-Kurve hat eine negative Steigung, die LM-Kurve eine positive Steigung. (Zur Vorstellung: jeweils durchgehende 45-Grad-Linien eignen sich gut). Die LM-Kurve bildet die Geldpolitik ab, auf den Achsen sind der Zinssatz (y-Achse), bzw. der Output (x-Achse) abgebildet. Jetzt das Gedankenexperiment: Angenommen die Geldmenge wird schlagartig (in t=0) ausgeweitet und im kommenden Jahr (t=1) wieder auf das Ursprungsniveau zurückgefahren. Die neue IS-Kurve hängt vom \textit{zukünftigen (t=1)} Output ab. Dieser bleibt ja gleich, da die Geldmenge in Zukunft (t=1) wieder zurückgefahren wird. Allerdings hängt die IS-Kurve auch vom aktuellen (t=0) Zinssatz ab. Der fällt vorübergehend durch die vorübergehende Geldmengenausweitung. Im Resultat wirkt sich dieser Geldmengenschock kurzfristig, nämlich in (t=0) positiv auf den Output aus, mittelfristig (bereits in t=1) ist das ursprüngliche Gleichgewicht aber wieder hergestellt. Die Neu-Keynesianische IS-Kurve hat also Eigenschaften, mit denen man die Auswirkungen von Nominalen Rigiditäten darstellen kann. Konkret nämlich, dass Geldpolitik\footnote{Ebenso ist Fiskalpolitik kurzfristig wirksam: Das gleiche Gedankenexperiment liefert für temporär wirksame Staatsausgaben die selben Ergebnisse.} kurzfristig wirksam ist. Damit liegt mit der Neu-Keynesianischen IS-Kurve ein Modell vor, dass den Forderungen der Neuen Klassiker, siehe Lucas-Kritik(vgl.: Kapitel \ref{Neue Makro}), entspricht und gleichzeitig das Neu-Keynesianische Element der Nominalen Rigiditäten berücksichtigt.

\subsection{Die erneute Auferstehung der Phillips Kurve}
\label{NeuePhillips}
George Akerlof nannte die Phillips-Kurve in seiner Nobelpreis-Rede im Jahr 2001 "`probably the single most important macroeconomic relationship"' \parencite{Nobelpreis-Komitee2001}. Dabei ist die Phillips-Kurve seit jeher umstritten und ständigem Wandel unterworfen. Die ursprüngliche Phillips-Kurve postulierte einen \textit{langfristigen} und stabilen Zusammenhang zwischen Inflation und Arbeitslosigkeit. In Kapitel \ref{cha: Neu Keynes} führten wir die "`erwartungsgestützte Phillips-Kurve"' ein, basierend auf adaptiven Erwartungen gilt der postulierte Zusammenhang nur mehr in der kurzen Frist. Arbeitslosigkeit hängt also von der \textit{Inflation der Vorperiode} ab. In Kapitel \ref{Neue Makro} war eine Folge der Lucas-Kritik, dass der Zusammenhang zwischen Arbeitslosigkeit und Inflation von den \textit{aktuellen Inflationserwartungen} abhängt. Da alle Akteure rationale Erwartungen haben, existiert ein entsprechender Zusammenhang nicht, außer die Wirtschaftspolitik tätigt unvorhersehbare Maßnahmen. In den 1990er Jahren schließlich verwarf man die radikalen Ideen der "`Neu-Klassiker"' diesbezüglich weitgehend und es wurde eine "`Neu Keynesianische Phillips Kurve"' postuliert. Diese ist mikrofundiert, basiert auf rationalen Erwartungen ist aber rein zukunftsorientiert. Das heißt die akutelle Arbeitslosigkeit hängt darin von den \textit{aktuellen Erwartungen der zukünftigen Inflation} ab \parencite[S. 980]{Roberts1995}. Mittlerweile spricht man übrigens meist vom Zusammenhang zwischen Inflation und Output (also BIP), durch den empirisch gut abgesicherten Zusammenhang zwischen BIP und Arbeitslosigkeit ist dies allerdings nur eine simple Erweiterung.

Unter der Annahme rationaler Erwartungen, bricht der postulierte Zusammenhang eigentlich zusammen, durch nominale Rigiditäten und damit verbundenen Preisanpassungsprozessen, muss man die Phillips-Kurve aber wieder in Betracht ziehen. Im Kapitel \ref{cha: Neu Keynes} haben wir die Arbeit von \textcite{Calvo1983} bereits dazu herangezogen nominale Rigiditäten zu modellieren. Und aus dieser Arbeit wurde auch tatsächlich die "`Neu-Keynesianische Phillips Kurve"' direkt formal-mathematisch abgeleitet. Dies ist einigermaßen interessant, da die ursprüngliche Phillips-Kurve ja ausschließlich auf empirischen Beobachtungen basierte.

Historisch gesehen hat \textcite{Rotemberg1982} ein äquivalentes Konzept zur "`Neu-Keynesianische Phillips Kurve"' bereits vor \textcite{Calvo1983} entwickelt. In \textcite{Roberts1995} geht der Autor auf die beiden eben genannten Arbeiten direkt ein und leitete eine Formel ab, die er - erstmals ausdrücklich - "`Neu-Keynesianische Phillips Kurve"' nannte \parencite[S. 979]{Roberts1995}. 

Die genannten Unterschiede erscheinen gering, schließlich lautet die Hauptaussage immer noch, dass es einen kurzfristigen negativen Zusammenhang zwischen BIP (oder Arbeitslosigkeit) und Inflation gibt. Wenn wir die drei modernen Formen aber vergleichen, fällt auf, dass sich die Grundaussage wesentlich ändert: Bei der "`erwartungsgestützte Phillips-Kurve"' geht man davon aus, dass eine (aktuelle) sinkende Inflation mit niedrigeren BIP-Wachstumsraten verbunden ist. Nach den Neu-Klassikern wirkt sich eine Änderung der Inflationsraten auf die Realwirtschaft, also die BIP-Wachstumsraten real gar nicht aus. Bei der "`Neu-Keynesianische Phillips Kurve"' geht man davon aus, dass die sinkende Inflation ja eine reine zukunftsbezogene Erwartung ist, demnach lautet die Grundaussage, dass sich die Ökonomie in t0 in einer Hochkonjunkturphase befindet. Mit anderen Worten: Das Vorzeichen ist falsch, oder zumindest nicht so wie erwartet. Konkret passt dies nicht zusammen mit der empirisch und theoretisch recht gut abgesicherten Annahme der "`Inflationsträgheit"' (Inflation Inertia). Hierbei geht man davon aus, dass einer hohen Inflation nur mittels schmerzhafter, also wachstumshemmender Maßnahmen, beizukommen ist.
Diese Unzulänglichkeit der Neu-Keynesianischen Phillips-Kurve macht das Konzept insgesamt umstritten. \textcite{Gali1999} nahmen Anpassungen vor, indem sie nicht nur die Inflationserwartungen, sondern auch historische Inflationswerte einfließen ließen. Wobei empirische Ergebnisse der beiden Autoren in der Folge ergaben, dass die zukunftsgerichteten Erwartungswerte der Inflation bessere Ergebnisse liefern, also historische Werte. Der Output wurde zudem durch die Lohnquote der funktionalen Einkommensverteilung ersetzt. Diese steigt nämlich empirisch während Rezessionsphasen. Letztlich ein Kunstgriff, der die Theorie nicht wesentlich besser macht. \textcite{Rudd2005, Rudd2006} zeigten empirisch die Unzulänglichkeiten der "`Neu-keynesianischen Phillips-Kurve"' und die Gültigkeit der Inflationsträgheit. Unumstritten ist die faktische Unmöglichkeit den Output-Gap, also die Differenz zwischen BIP und potenziellem BIP, zu messen. Da dieser ein wichtiger Bestandteil der "`Neu-keynesianischen Phillips-Kurve"' ist, sind empirische Untersuchungen schwierig durchzuführen. Die Diskussion zur Gültigkeit der "`Neu-keynesianischen Phillips-Kurve"' ist also schwer zu lösen. Aber es ist doch "`State of the Art"', dass diese Gleichung ein Schwachpunkt der ursprünglichen DSGE-Modelle ist. \textcite{Mankiw2002} und \textcite{Christiano2005} haben auch tatsächlich Modelle entwickelt, die erfolgreich Inflationsträgheit berücksichtigen, diese sind aber schwer in das übliche DSGE-Framework zu integrieren und haben auch andere Nachteile, wie wir im Unterkapitel \ref{ErweiterungDSGE} sehen werden. 

Insgesamt führt dies zu der etwas unbefriedigenden Situation, dass die "`Neu-keynesianische Phillips-Kurve"' empirisch problembehaftet ist, aufgrund ihrer Einfachheit und formaler Eleganz wird sie aber in DSGE-Modellen dennoch häufig verwendet \parencite[S. 341]{Romer2019}. Insbesondere wurde kritisiert, dass Inflation und Output-Gap nicht wie erwartet korrelieren. Demnach könnte man zu hohe Inflation bekämpfen ohne dafür Wachstumseinbußen hinnehmen zu müssen. Das stimmt aber nicht mit der empirischen Beobachtung überein. Dass es in DSGE-Modellen keinen Trade-Off zwischen Inflationszielen und Wachstumszielen gibt, heißt auch, dass eine Zentralbank, bei Anwendung der optimalen Strategie, stets beide Ziele erreichen kann: Keine Inflation und auch keinen Output-Gap. \textcite{Blanchard2005} nannten diese Eigenschaft der "`Neu-Keynesianischen Phillips-Kurve"' den "`Göttlichen Zufall"' ("`Divine Coincidence"'). Die Existenz eines solchen wäre zwar sehr angenehm, aber die Empirie zeigt, dass dieser Zusammenhang eben nicht existiert. In \textcite{Gali1999, Gali2015, Blanchard2005} wird diskutiert, warum dieser "`Göttliche Zufall"' in der Praxis nicht existiert.


\subsection{Taylor-Rule: Ein pragmatischer Zugang zur Geldpolitik}
\label{Taylor}
Die Mainstream-Ökonomie kam Anfang der 1990er Jahre weitgehend darin überein, dass Märkte in der Regel nicht vollkommen reibungslos funktionieren. Wäre dies der Fall wäre aktive Wirtschaftspolitik wirkungslos und Konjunkturschwankungen wären rein zufällig, wie im Real-Business-Cycle-Framework dargestellt. Wie \textcite[S. 823]{Akerlof1985} beschrieb, gingen die Neu-Keynesianer also davon aus, dass Wirtschaftspolitik im Allgemeinen und Geldpolitik im Speziellen \textit{nicht} wirkungslos sind.

Die Geldpolitik ist uns ja schon des öfteren in diesem Buch untergekommen. Die Keynesianer legten den Fokus auf Fiskalpolitik. Friedman machte die Geldpolitik, und insbesondere die Gültigkeit der Quantitätstheorie des Geldes, wieder salonfähig. Keynesianer und Monetaristen "`einigten"' sich schließlich darauf, dass ein Policy-Mix aus Fiskal- und Geldpolitik zu optimalen Ergebnissen führt. \textcite{Friedman1960} (vgl. Kapitel \ref{Monetarismus}) plädierte schließlich in der Geldpolitik für ein jährliches "`Mengenziel"'. Die Zentralbanken sollten die Geldmenge mit einer konstanten Rate wachsen lassen. Tatsächlich folgten Zentralbanken diesen Rat vor allem in den 1980er Jahren. Das Resultat dieser Geldmengensteuerung war aber in der Empirie wenig befriedigend. Offensichtlich änderte sich die Umlaufgeschwindigkeit des Geldes mit dem Effekt, dass Inflation und der kurzfristige Zinssatz nicht einem stabilen Pfad folgten, sondern im Gegenteil, stark schwankten. Zentralbanken gingen daher dazu über direkt ihre Zielgröße, nämlich den kurzfristigen Zinssatz, zu steuern. Dazu passen sie ihr Geldangebot so an, dass sich das Marktgleichgewicht bei entsprechender Geldnachfrage genau beim gewünschten, kurzfristigen Zinssatz\footnote{Was ist mit "`kurzfristigem Zinssatz"' gemeint? Hier kommt es oft zu Missverständnissen. Dies ist \textit{nicht} der Leitzins, oder ein anderer Zentralbankenzinssatz. Tatsächlich handelt es sich um den Zinssatz, zu dem sich Finanzinstitutionen "`über Nacht"' gegenseitig liquide Mittel leihen. In Europa entspricht dies also dem "`EONIA"' (bzw. ESTR), in den USA ist dies die "`Federal funds rate"'} einpendelt. Bleibt die Frage, \textit{welchen} kurzfristigen Zinssatz die Zentralbank wählen soll, um den gewünschten, stabilen Wachstumspfad zu erreichen? Und hier kommen die "`Zinssatz-Regeln"', die allerdings meist "`Taylor-Rule"' bezeichnet werden, ins Spiel.

Benannt sind diese nach \textit{John Brian Taylor}. Der Stanford-Professor ist ein sehr interessantes Individuum. Taylor ist eigentlich eher dem erzliberalem Spektrum zuzuordnen. Unter anderem war er Vorsitzender der Mont-Pelerin-Gesellschaft von 2018 bis 2020. Etwas im Widerspruch dazu steht aber, dass er einer der ersten Ökonomen war, der die Existenz und Bedeutung von Nominalen Rigiditäten beschrieb. Er wurde diesbezüglich schon in Kapitel \ref{cha: Neu Keynes} des öfteren zitiert. Er stellte sich damit eben \textit{gegen} die "`Neuen Klassiker"' - die ganz im Sinne des ökonomischen Liberalismus - postulierten, dass Wirtschaftspolitik wirkungslos sei. Anfang der 1990er Jahre schließlich trug er maßgeblich zum theoretischen Verständnis der modernen Zentralbankensteuerung bei. Was sich schließlich als zentraler Baustein in den Modellen der "`Neuen neoklassischen Synthese"' erweisen sollte.

Der bahnbrechende Artikel \textcite{Taylor1993} \textit{begründete} nicht die Zinssteuerung in Zentralbanken - tatsächlich waren viele Zentralbanken schon vor der Veröffentlichung des Artikels zur Zinssteuerung übergegangen - sondern versuchte eine formale Regel zu finden, wie Zentralbanken den kurzfristigen Zins bestimmen sollten. Er schlug tatsächlich eine sehr einfache Formel vor \parencite[S. 202]{Taylor1993}. Normalerweise wird auf die Darstellung von mathematischen Formeln in diesem Buch bewusst verzichtet, aber in diesem Fall hat die Darstellung auch für nicht mathematisch versierte Leser nur Vorteile\footnote{Die im Fließtext angeführte Formel ist tatsächlich jene die \textcite{Taylor1993} verwendete. Verallgemeinert wird sie heute meist (z.B.: \textcite[S. 609]{Romer2019}) so dargestellt: $i_t = r^n + \phi_{\pi}*(\pi_{t} - \pi*) + \phi_y * (ln Y_t - lnY^{n}_{t})$. Dadurch wird der lineare Zusammenhang zwischen Zielvariable und Inflation und der log-lineare Zusammenhang zwiscehn Zeilvariable und BIP-Abweichung deutlich gemacht. Außerdem werden die Konstanten Zielinflation \textit{p} = 2\% und Real-Zinssatz = 2\% durch Variable \textit{r} und $\pi*$ ersetzt, sowie  $\phi_{\pi}$ und $\phi_y$ statt fixer Werte}:

$$ r = p + 0,5y + 0,5 *(p-2) + 2 $$

Dabei ist \textit{r} eben die Zielvariable \textit{kurzfristiger Zinssatz}. \textit{p} bildet die Inflationsrate ab. Und \textit{y} ist die Abweichung des \textit{tatsächlichen realen BIP} vom \textit{potentiellen realen BIP}. Wenn das tatsächliche BIP unter dem potentiellen BIP liegt, so wird der Wert negativ, andernfalls positiv. Zur Erklärung: Taylor geht in seiner Arbeit davon aus, dass das BIP langfristig mit einer konstanten Wachstumsrate von ca. 2\% pro Jahr wächst. Dies ist konform mit allgemeinen Gleichgewichtstheorien und insbesondere auch mit Real Business Cycle-Modellen.


 Gehen wir zum Verständnis der Funktion dieser einfachen Zinsregel drei Beispiele durch:
\begin{enumerate}
	\item Inflation: \textit{p} = 4. Abweichung vom potentiellen BIP: \textit{y} = 4. \\
	Ergibt:	$ 4 + 0,5*4 + 0,5 *(4-2) + 2 = 9$ \\
	Die Zentralbank würde den Zinssatz also auf 9\%, bzw. real 5\% festlegen.
	\item Inflation: \textit{p} = 2. Abweichung vom potentiellen BIP: \textit{y} = 0. \\
	Ergibt:	$ 2 + 0,5*0 + 0,5 *(2-2) + 2 = 4$ \\
	Die Zentralbank würde den Zinssatz also auf 4\%, bzw. real 2\% festlegen.
	\item Inflation: \textit{p} = 1. Abweichung vom potentiellen BIP: \textit{y} = -4. \\
	Ergibt:	$ 1 + 0,5*-4 + 0,5 *(1-2) + 2 = 0,5$ \\
	Die Zentralbank würde den Zinssatz also auf 0,5\%, bzw. real -0,5\% festlegen.
	
\end{enumerate}

Im Modell von Taylor wird eine Zielinflation von 2\%, sowie ein stabiler Real-Zinssatz von 2\% implizit angenommen. 
Wann dann die Inflation tatsächlich 2\% beträgt und es auch zu keinen Abweichungen vom langfristigen Wachstumspfad (Das BIP wächst mit ca. 2\%; \textit{y = 0}) kommt, dann bleibt der Zinssatz bei 4\% und das BIP wächst, vorbehaltlich zukünftiger exogener Schocks, konstant entlang des langfristigen Wachstumspfads. Dieser Gleichgewichtszustand wird im zweiten Beispiel oben dargestellt.
Das erste Beispiel zeigt, was passieren sollte, wenn die Wirtschaft droht zu "`überhitzen"'. Dies ist dann der Fall wenn die Inflation höher ist als 2\% und/oder das potentielle BIP übertroffen wird. Hier sollte der sehr hohe kurzfristige Zinssatz von 9\% angestrebt werden. Das Geldangebot müsste dementsprechend im Verhältnis zur hohen Geldnachfrage - davon kann man aufgrund der Tatsache, dass Inflation wie auch BIP über den Zielwerten liegen ausgehen - recht gering gehalten werden.
Das dritte Beispiel zeigt schließlich, wie die Notenbank stimulierend eingreifen kann. Da möchte sie in der Regel, wenn das tatsächliche BIP hinter seinem Potential zurück geblieben ist und/oder die Inflation unter der angenommenen Zielrate von 2\% liegt. Ein Zinssatz von real unter 0\% sollte für steigenden Nachfrage nach Geld sorgen. Aufgrund des niedrigen angestrebten Nominalzinssatzes von 0,5\% wäre auch das zur Verfügung gestellte Angebot dementsprechend hoch.

Die Ursprungsversion der Taylor-Rule hat einige Ungenauigkeiten, denen sich Taylor auch bewusst war. Erstens geht Taylor davon aus, dass der natürliche, reale Zinssatz - bei dem die Ökonomie auf einem stabilen Wachstumskurs bleibt ohne zu überhitzen, bzw. nicht ausgelastet ist - langfristig konstant und bekannt ist. Beides ist zumindest fraglich. Zweitens, werden die Eingangsvariablen BIP, potentielles BIP und Inflation als bekannt angenommen, was vor allem beim potentiellen BIP alles andere als klar ist. Diese beiden Probleme können nie zur Gänze gelöst werden, aber durch den Fortschritt vor allem bei empirischen Arbeiten, können die Parameter heute zumindest weiter eingeschränkt werden als Anfang der 1990er Jahre. Ein Modell-theoretisches Problem ist drittens, die Berücksichtigung der Inflation: \textcite[S. 211]{Taylor1993} selbst beschrieb bereits, dass Inflation nicht einfach - wie in der von ihm formulierten Formel - als Vergangenheitswert berücksichtigt werden sollte, sondern stattdessen als Kombination aus durchschnittlichen Vergangenheitswerten und erwarteten Inflationsraten. Eine wichtige Erweiterung, die in späteren Zinsregeln Anwendung fand. Später werden wir sehen, dass in DSGE-Modellen alle Parameter sogar rein zukunftsorientiert sind, dementsprechend werden heute häufig ausschließlich die Inflations\textit{erwartungen} als Parameter für Zinsregeln verwendet. Eine bis heute häufig verwendete Umsetzung hierfür kommt von \textcite[S. 150ff]{Gali2000}. Eine vielbeachtete Arbeit zu Zinsregeln ist auch \textcite{Woodford2001}. Darin räumt der Autor modelltheoretische Bedenken, die Ende der 1990er Jahre aufgetreten sind, weitgehend aus. So wurden Zweifel aufgeworfen, ob eine reine Zinsregel dem Ziel ein stabiles Gleichgewicht zu erreichen überhaupt sinnvoll sein kann? So könnte es zum Beispiel sein, dass ein unvorhergesehener Anstieg der Inflationserwartungen dazu führt, dass die Zinsregel einen nominalen Zinssatz vorschlägt, der in einem  Realzinssatz resultiert, der niedriger ist als der Gleichgewichtszinssatz. Dieser Zinssatz würde nicht, wie gewünscht restriktiv wirkend, sondern expansiv. Das Ergebnis wären steigender Output, steigende Inflation und steigende Inflationserwartungen. Das Ergebnis wäre eine immer weiterführende, die Inflation stimulierende, Spirale, also das Gegenteil vom stabilen Gleichgewicht. Eine zweite kritische Frage ist, ob die verwendeten Bestimmungsfaktoren Inflation und BIP-Abweichung überhaupt als Zielfaktoren der Geldpolitik geeignet sind? \textcite{Woodford1999} zeigt für beide Fragen formal, dass Zinsregeln sehr wohl stabilisierend wirken, räumt aber ein, dass vor allem bei der BIP-Abweichung Probleme dahingehend auftauchen, welche zukunftsgerichteten Werte tatsächlich verwendet werden sollen. Michael Woodford ist übrigens jener Ökonom, der das Framework der "`Neuen Neoklassischen Synthese"' lieber "`Neo-Wicksellianische Schule"' nennt. Tatsächlich finden sich zwischen Taylor-Rule-Konzept und der Arbeit von Wicksell (vgl. Kapitel \ref{Austria}) interessante Parallelen, nämlich in der postulierten, stabilisierenden Wirkung des Zinssatzes. Tatsächlich hat schon \textcite{Wicksel1898} die Idee aufgebracht, dass die sich Ökonomie bei einem bestimmten natürlichen Zinssatz im Gleichgewicht befindet. Bei Abweichungen davon könnte man also mittels Erhöhung bzw. Reduktion des Zinssatzes gegensteuern. 

Wie zu Beginn dieses Kapitels bereits angedeutet, erfuhr die \textit{empirische} Wirtschaftsforschung in den 1990er Jahre einen enormen Aufschwung. So wurde auch die Taylor-Rule mittels Datenanalyse auf ihre empirische Robustheit überprüft. \textcite{Taylor1999} war auch hierbei einer der ersten. Er analysierte Zeitreihen für die USA, die zurück bis 1979 reichten. Die Arbeit ist auch deswegen interessant, weil Taylor darin explizit darauf hinweist, dass Zinsregeln nicht im Widerspruch zur Quantitätsgleichung des Geldes stehen (vgl. Kapitel \ref{Monetarismus}), sondern, im Gegenteil, sogar daraus abgeleitet werden können\parencite[S. 322f]{Taylor1999}. Ein zweiter Artikel - der in diesem Kapitel wegen seiner umfassenden theoretischen und empirischen Bedeutung noch öfter zitiert wird - ist jener von \textcite{Gali2000}. Die beiden Artikel kommen zum gleichen zentralen Ergebnis: In den 1960er und 1970er Jahren waren die realen kurzfristigen Zinssätze deutlich geringer als in den 1980er und 1990er Jahren. Zur Erinnerung der erstgenannte Zeitraum war geprägt von hohen Inflationsraten. Nachdem Paul Volcker 1979 Gouverneur der Fed wurde, führte er, zur Inflationsbekämpfung, eine restriktive Zinspolitik ein und folgte damit, retrospektiv betrachtet, einer Taylor-Rule. Tatsächlich waren die 1980er und 1990er Jahre geprägt von niedriger Inflation und recht stabilen Wachstumsraten.  Aus heutiger Sicht sind die Lobgesänge auf die sogenannte Volcker-Greenspan-Ära, auf die sich \textcite{Taylor1999} und \textcite{Gali2015}, in Anlehnung an die damaligen Fed-Vorsitzenden, beziehen, etwas vorsichtiger zu sehen. Folgte doch bereits im März 2000 mit dem Platzen der sogenannten "`Dot-Com"'-Blase eine große Finanzkrise\footnote{Anmerkung: Das Entstehen von Überbewertungen auf Finanzmärkten, wird häufig mit fehlerhafter Geldpolitik im Vorfeld in Verbindung gebracht}. Insgesamt werden die empirischen Ergebnisse - vor allem von Taylor selbst - als Bestätigung dafür gesehen, dass erstens, Geldpolitik dann besonders erfolgreich war, wenn implizit eine solche Zinsregel angewendet wurde und zweitens, die diese eine geeignete Regel-basierte Handlungsanleitung für Notenbanken ist.
Aus europäischer Sicht besonders interessant ist, dass \textcite{Gali1998} internationale Vergleiche angestellt haben. Sie kommen zu dem Schluss, dass neben den USA zumindest auch Japan und Deutschland implizit "`Inflation-Targeting"' betrieben haben, also einer (zukunftsgerichteten) Zinsregel folgten. Interessant ist auch deren Analyse bezüglich Frankreich, Großbritannien und Italien. Alle drei Staaten gehörten bis zu dessen Auflösung im Jahr 1992 dem Europäischen Währungssystem (EWS) an. Durch die wirtschaftliche Dominanz Deutschlands, mussten diese drei Länder Anfang der 1990er Jahre auf eine eigene Geldpolitik praktisch gänzlich verzichten. Tatsächlich sieht man vor allem für Frankreich und Italien, dass die traditionell eher hohen Inflationsraten ab Anfang der 1980er Jahre deutlich zurückgingen. Der Preis den die beiden Länder dafür bezahlen mussten waren aber sehr hohe nominale Zinssätze, die dazu führten, dass die Realzinssätze deutlich höher waren als in Deutschland. Laut Simulationen in \textcite[S. 23f]{Gali1998} lagen diese Zinssätze deutlich \textit{über} jenen, die eine Zinsregel ergeben hätte.

Taylor beteuerte übrigens in der Conclusion seines Artikels: "`Solche Regeln können und sollen nicht [stur] von [Notenbankern] befolgt werden"' \parencite[S. 213]{Taylor1993}. Später änderte Taylor seine Meinung und spricht sich seither dafür aus, dass Notenbanken ihre Geldpolitik stärker an Zinsregeln wie die Taylor-Rule binden sollten. Vertreter von Zentralbanken wollen sich ihre Autonomie hinsichtlich Geldpolitik natürlich nicht von Regeln einschränken lassen \parencite[S. 609]{Romer2019} und es entstand eine Debatte zwischen \textcite{Bernanke2015} und \textcite{Taylor2015} darüber. Taylor argumentierte, dass die Geldpolitik vor der "`Great Recession"' zu wenig restriktiv war und daher die Immobilienblase förderte. Auf jeden Fall zählt die Taylor-Rule - wenn auch in moderner, verbesserter Form - nach wie vor zum Werkzeug der Mainstream-Ökonomie. Nach der "`Great Recession"' reichte eine Orientierung an dieser Zinsregel aber nicht mehr aus um Geldpolitik durchzuführen, da die Zinsuntergrenze von 0\% nicht unterschritten werden kann, dazu aber später mehr.

Zweifelsohne aber veränderte die Taylor-Rule die Geldpolitik. Der Fokus wendete sich vollends von den Geldmengenzielen \parencite{Friedman1960} zu den Zinssatz-Regeln \parencite[S. 36]{Gali2007}. Heute wird zwar - beeinflusst durch zahlreiche theoretische \parencite{Woodford2001} und empirische \parencite{Gali2000, Taylor1999} Forschung - eine verallgemeinerte Form der Taylor-Rule verwendet, aber das grundlegende Prinzip ist das gleiche geblieben, weshalb man nach wie vor das gesamte Konzept als "`Taylor-Rule"' anstatt als "`Zinsregel"' bezeichnet. Außerdem spricht man heute übrigens meist vom "`Inflation-targeting"', also vom Erreichen einer festgelegten Zielinflation. Dies ist äquivalent mit der regelgebundenen Zinssteuerung \parencite{Taylor2006}, weil die Zinsregel als direkte Einflussgröße die Inflation umfasst. Moderne Notenbanken wie die EZB setzen sich eine Zielinflationsrate als Richtlinie dafür, ob ihre Wirtschaftspolitik erfolgreich war. Damit wurde das Inflation-Targeting zu einem zentralen Tool von Notenbanken. Interessant ist, dass die meisten Zentralbanken tatsächlich ein Inflationsziel von 2\% verwenden, diesen Wert hatte Taylor bereits in seinem Journal-Beitrag recht willkürlich gewählt hat \parencite[S. 202]{Taylor1993}. 

\subsection{\textit{Das} DSGE-Grundmodell}
\label{DSGEGrundmodell}

In diesem Unterkapitel werden wir sehen, wie in den späten 1990er Jahre die verschiedenen nun dargestellten makroökonomischen Entwicklungen ineinander griffen. Im Rahmen der DSGE-Modelle wird die gesamte Ökonomie nämlich auf drei Player eingeschränkt, die mittels Gleichungen dargestellt werden. Jene für:

\begin{itemize}
	\item Haushalte: Die Aggregierte Nachfrage wird modelliert mittels "`Neu-Keynesianischer IS-Kurve"'.
	\item Unternehmen: Das Aggregierte Angebot wird mittels "`Neu-Keynesianischer Phillips-Kurve"' dargestellt.
	\item Zentralbank: Deren Verhalten ist nicht mikrofundiert, aber Regel-gebunden: Abgebildet durch eine "`Taylor-Rule"'.
\end{itemize}

Bevor wir uns ansehen wie diese Komponenten kombiniert werden, betrachten wir kurz, was das Ergebnis, ein DSGE-Modell, nicht umfasst: Wie man sieht werden Staatsausgaben und damit Fiskalpolitik, nicht explizit betrachtet. Vor allem letzteres ist natürlich ein krasser Bruch zu traditionellen Keynesianischen Modellen. Die Staatsausgaben sind im "`repräsentativen Haushalt"' mitumfasst, das heißt, steigende Staatsausgaben führen zu einem höheren BIP. Allerdings gibt es keinen Multiplikatoreffekt (wie in Kapitel \ref{Keynes}). Der Nachfrage-Multiplikator ist gleich eins. Fiskalpolitik hat in der Folge nur einen kurzfristigen und einmaligen positiven Effekt auf das BIP\footnote{Dies gilt zumindest solange der nominale Zins gesteuert werden kann. Zur Diskussion über die Rolle der Fiskalpolitik in der "`Neuen Synthese"' siehe Kapitel \ref{NeueFiskal}} \parencite{Woodford2011}. Anders ausgedrückt: Die Ricardianische Äquivalenz ist in DSGE-Modellen eine Standardannahme. 

Zurück zum Aufbau der DSGE-Modelle: Was können wir aus den Unterkapiteln \ref{NeueIS} bis \ref{Taylor} und den Grundlagen der "`Real-Business-Cycle"'-Modellen ableiten:

Der repräsentative Haushalt hat eine unendliche Lebenserwartung und optimiert sein Verhalten unter der Prämisse, dass Konsum einen positiven Nutzen generiert, zu tätigende Arbeit generiert hingegen einen negativen Nutzen. Außerdem wissen die Haushalte, dass es in der Zukunft zu zufälligen Schocks kommt. Diese sind ja integraler Bestandteil der Real-Business-Cycle-Methodik. Haushalte optimieren also ihren Nutzen, gegeben ihren Erwartungen und dem Wissen um zufällige zukünftige Schocks. Die Optimierungsaufgabe liegt darin, das Verhältnis aus Konsum und Arbeit für alle Zeitperioden festzulegen. Die Zukunft (zukünftige Werte), wird dabei - wie immer in der Ökonomie - abgezinst. Der Realzinssatz ist daher \textit{die} entscheidende Stellschraube bei der Optimierung. Ein steigender Realzinssatz sorgt dafür, dass Konsum aufgeschoben wird, mit dem Ergebnis, dass der aktuelle Konsum - und damit der Gesamtoutput (BIP) - in der aktuellen Periode sinkt. Dies wir repräsentiert durch die "`Neu-Keynesiansische IS-Kurve"'.

Unternehmen haben eine Produktionsfunktion, die von einheitlicher Technologie ausgeht (das heißt nur, dass kein Unternehmen einen Vorteil aus einem technischen Fortschritt generieren kann). Der Produktionsfaktor Arbeit muss zum Nominallohn zugekauft werden. Unternehmen können nur abhängig von einem Zufallsprozess ihre Preise an Marktgegebenheiten anpassen. Die Unternehmen produzieren jeweils ein einzigartiges Gut. Die Optimierungsaufgabe der Unternehmen besteht natürlich in der Gewinnmaximierung. Im Detail verlangt dies eine optimale Preisfestsetzung unter den Nebenbedingungen, dass Preisanpassungen erst wieder in einer zufällig zu bestimmenden Periode in der Zukunft möglich sein werden und in der Zwischenzeit sowohl Änderungen beim Nominalzinssatz, also auch bei Inflationsraten auftreten werden. Aufgrund der Annahme, dass monopolistische Konkurrenzmärkte vorherrschen, führt eine Abweichung des individuellen Preises eines Unternehmens vom Gleichgewichtspreis ja nicht zu einem totalen Rückgang des Gewinns. Unternehmen, die sich monopolistischer Konkurrenz ausgesetzt sehen, schlagen ja ohnehin immer ein "`Mark-up"' auf den Gleichgewichtspreis auf, um ihren Gewinn zu optimieren. Das heißt, Unternehmen setzen ihr Mark-up so, dass sie die Erwartungen ihrer zukünftigen Kostenänderung (durch Inflation) einpreisen und zusätzlich ihre Erwartungen der Dauer bis zur nächsten Preissetzung berücksichtigen. Dieser Preissetzungsmechanismus ist abgebildet durch die "`Neu-keynesianische Phillips-Kurve"'. Die Probleme, nämlich erstens, dass dies mathematisch herausfordernd zu modellieren ist und Annahmen getroffen werden müssen, und zweites, dass es unterschiedliche Ansätze gibt dies zu modellieren sind zwar bekannt, werden aber zumindest im Standardmodell vernachlässigt.

Als dritte Gleichung kommt jene der Geldpolitik ins Spiel. Wie wir bereits gesehen haben, sind für Haushalte und Unternehmen vor allem die Variablen Zinssatz und Inflation von Bedeutung bei ihrer Optimierungsaufgabe. Und an dieser Stelle kommt wieder die Taylor-Regel\parencite{Taylor1993} ins Spiel. Diese ist zwar nicht mikrofundiert, bestimmt aber zu jedem Zeitpunkt die resultierenden Handlungen der Zentralbanken. Es ist auch möglich, die Geldpolitik anders zu modellieren, aber im Ausgangsmodell hat sich das Konzept der Zinsregel etabliert. 

Aus den drei Gleichungen bilden wir jetzt ein DSGE-Modell:
\begin{itemize}
	\item Die Neu-Keynesianische IS-Kurve erklärt die Entwicklung des Outputs $Y$ durch den Real-Zinssatz $r_t$. Für diesen Real-Zinssatz haben wir eine Zinsregel in Form einer Taylor-Rule, welche den Term $r_t$ in der IS-Gleichung ersetzt.
	\item Die Formel für den Real-Zinssatz wiederum hängt ab vom erwarteten Output $E_t[y_{t+1}]$ und der erwarteten Inflationsrate $E_t[\pi_{t+1}]$ ab.
	\item Sowohl in der IS-Gleichung, als auch in der Real-Zinssatz-Gleichung ist außerdem ein Fehlerterm $u_t^{IS}$ bzw. $u_t^{MP}$, der zufällige Nachfrageschocks, bzw. Geldmengen-Schocks darstellt.
	\item Die Neu-Keynesianische Phillips-Kurve erklärt die Inflation $\pi_t$ durch erwartete Inflation $E_t[\pi_{t+1}]$ und aktuellem Output $y_t$. der abhängig vom Grad der nominalen Rigidität $\kappa$ ist.
	\item Dieser aktuelle Output kann wiederum durch die IS-Gleichung ersetzt werden, die wiederum die Gleichung des Real-Zinssatzes enthält.
	\item Die Neu-Keynesianische Phillips-Kurve enthält zusätzlich einen Fehlerterm für Inflationsschocks $u_t^{\pi}$.
\end{itemize}

Wie funktionieren solche Modelle nun? Zunächst muss festgehalten werden, dass es sich um mehrperiodige Gleichgewichtsmodelle handelt, das heißt, nach exogenen Schocks nehmen deren Auswirkungen mit fortlaufender Zeit ab und verschwinden schließlich ganz, womit das System wieder im Gleichgewicht ist. Die drei Stellschrauben sind die Fehlerterme. An diesen Stellen können die zufälligen Schocks simuliert werden: Der Fehlerterm der Zinsregel $u_t^{MP}$ simuliert hierbei einen geldpolitischen Schock. Durch das Gleichungssystem werden die verschiedenen Variablen durch diesen Schock unterschiedlich beeinflusst. Durch die intertemporäre Ausgestaltung des Systems werden diese Abweichungen vom Gleichgewicht über mehrere Perioden wirksam. Bei einem restriktiven, geldpolitischen Schock würde zum Beispiel die Inflation sinken, in weiterer Folge auch die Beschäftigung, sowie damit verbunden, die aggregierten Reallöhne. Der nominale Zinssatz würde steigen. Ebenso der Real-Zinssatz, welcher aufgrund des Rückgangs der Inflation sogar noch stärker steigen würde. Schließlich würde die Gesamtoutput-Entwicklung zurückgehen, also das BIP-Wachstum fallen und der Output-Gap steigen (im negativen Bereich), da das potenzielle BIP unverändert bleibt \parencite[S. 68f]{Gali2015}. Änderungen im Fehlerterm der Neu-Keynesianischen IS-Kurve $u_t^{IS}$ würden einen Nachfrage-Schock (IS-Schock) verursachen. Der Fehlerterm der Neu-Keynesianische Phillips-Kurve $u_t^{\pi}$ simuliert einen Inflationsschock\footnote{Auch ein Technologieschock kann über die Phillips-Kurve simuliert werden.}. Das vollständig definierte Modell gibt somit als Antwort auf auszuwählende Schocks für die verschiedenen Variablen zu verschiedenen Zeitpunkten Abweichungen vom Gleichgewicht an. 

Julio Rotemberg und Michael Woodford waren die ersten Ökonomen, die DSGE-ähnliche Modelle entwickelten. In \textcite{Rotemberg1993} implementierten die beiden das Konzept der Monopolistischen Konkurrenz in ein dynamisches Gleichgewichtsmodell. Als erstes vollständiges DSGE-Modell wird häufig \textcite{Yun1996} genannt, zum Beispiel in \textcite[S. 80]{Gali2015}. Tatsächlich veröffentlichte, der sonst recht unbekannt gebliebene Ökonom, schon Mitte der 1990er Jahre ein DSGE-Modell, das vom Aufbau her den heutigen Modellen schon sehr ähnlich ist. Monopolistische Konkurrenz, sowie Nominale Rigiditäten, die mittels den von \textcite{Calvo1983} vorgeschlagenen Prozess simuliert werden, sind bereits enthalten. Auch die Darstellung der Ergebnisse ist bis heute ähnlich geblieben. \textcite{Rotemberg1997} veröffentlichten noch eine Reihe weiterer DSGE-Paper. Interessant ist, dass \textcite{Woodford1996} bereits im Jahr 1996 mittels DSGE-Modell analysierte, dass sich in einer Währungsunion expansive Fiskalpolitik eines Mitgliedstaats auf die Preisstabilität der anderen Mitgliedsstaaten auswirkt. Er ging hierbei davon aus, dass die Ricardianische Äquivalenz nicht gültig ist. Das Ausgangsbeispiel war hierbei die Europäische Währungsunion und der Sinn und Zweck der Maastricht-Kriterien, die den potentielle Mitgliedsstaaten enge fiskalpolitische Grenzen auferlegte\footnote{So durfte ein jährliches Budgetdefizit von 3\% des BIP nicht überschritten werden und der Verschuldungsgrad musste unter 60\% des BIP liegen.}. Der Durchbruch der DSGE-Modell erfolgte schließlich mit der Jahrtausendwende. Als \textit{das} Standardmodell etablierte sich schließlich jenes Drei-Gleichungen-Modell, das \textcite{Gali2000} veröffentlichten. In dieser Zeit wurde das Modell auch schon ausgiebig auf seine empirische Robustheit getestet und diskutiert \parencite{Gali1999}. \textcite{Gali1998, Taylor1999} zum Beispiel publizierten zur Gültigkeit der Taylor-Rule. Die Diskussion zur problematischen Neu-Keynesianischen Phillips Kurve entfachte etwas später und wurde bereits in Unterkapitel \ref{NeuePhillips} angeführt. Hierzu wurden alsbald auch Alternativen (siehe Unterkapitel \ref{ErweiterungDSGE}) entwickelt. Ebenfalls zur Jahrtausendwende lieferten \textcite{Rotemberg1998, Christiano1999} empirische Evidenz zu den Auswirkungen von geldpolitischen Schocks.

Trotz aller Unzulänglichkeiten und Erweiterungen wird das Modell von \textcite{Gali2000} in Lehrbüchern noch immer als Ausgangsmodell angeführt \parencite[S. 350]{Romer2019}. Dieses Modell ist mathematisch komplex, aber äußerst elegant und nachvollziehbar. Die Gleichungen bedingen sich gegenseitig und über mehrere Zeitperioden hinweg. Das Modell ist also ein intertemporales Optimierungsmodell und somit \textit{dynamisch}\footnote{Im Spezialfall von fehlender Autoregression, wäre das Modell im Gleichgewicht und Abweichungen einzelner Variablen vom Gleichgewicht würden nur mehr durch die zufällige Schocks vorkommen. Das Ergebnis wäre ident zu Ergebnissen reiner "`Real-Business-Cycle"'-Modellen.}. Durch die Hinzunahme der Fehlerterme umfasst es die Annahme zufälliger Schocks, ist also \textit{stochastisch}. Es ist streng mikrofundiert und ausschließlich zukunftsorientiert. Damit entspricht das Modell den Anforderungen der Theorie der rationalen Erwartungen.

Ohne Zweifel sind DSGE-Modelle mit sehr starken Annahmen hinterlegt, das macht sie punktuell recht angreifbar. Man denke nur an die schlechten empirischen Ergebnisse zur "`Neu-keynesianischen Phillips-Kurve"'. Man muss aber im Umkehrschluss festhalten, dass DSGE-Modelle Probleme erfolgreich in den Griff bekommen, die zu Zeiten der traditionellen Keynesianische und Monetaristische Modelle noch nicht einmal also solche identifiziert waren. Zudem sind sie ein Fortschritt gegenüber den "`Real-Business-Cycle"'-Modellen. Faktum ist, dass DSGE-Modelle vor allem in Zentralbanken als Analyseinstrument der Geldpolitik herangezogen werden. Bis heute gibt es außerdem viele Weiterentwicklungen des Standardmodells, auf die wichtigsten gehen wir im nächsten Kapitel ein.

Zum Abschluss betrachten wir noch den Konjunkturzyklus-Mechanismus in diesem Standard-DSGE-Modell. Unternehmen setzen also ihre Preise auf Grundlage ihrer Grenzkosten. Kommt es zu einem kurzfristigen geldpolitischen Schock, reagiert auch hier - wie im Neu-Keynesianischen Modell - die Inflation kurzfristig kaum. Geht aufgrund einer Geldmengenausweitung der nominale Zinssatz zurück, so fällt in diesem Fall auch der Realzinssatz. In der Folge steigen Konsumausgaben und auch die Einkommen und in weiterer Folge dann auch die Inflation (durch steigende Grenzkosten, die wiederum durch die höhere Beschäftigung steigen). Die Zinssätze werden dann wieder angehoben bis die Zielinflation wieder unterschritten wird. 

\subsection{Erweiterungen der DSGE-Modelle}
\label{ErweiterungDSGE}

Das dargestellte Ausgangsmodell bildet nur die Grundlage für wahrlich unzählige Erweiterungen dieses Ansatzes. Nach der "`Great Recession"' nach 2007 schien es zwar so als wäre die Zeit der DSGE-Modelle vorbei, dies dürfte jedoch ein Trugschluss gewesen sein. 
Stattdessen wurde das Rahmenwerk in verschiedene Richtungen erweitert, bzw. wurde versucht bekannte Kritikpunkte durch neue Ansätze zu heilen. Obwohl die Kritik an DSGE-Modellen nicht abreißt, scheint die "`Mainstream-Ökonomie"' an diesen Gleichgewichtsmodellen nach wie vor festzuhalten. Verschiedene Erweiterungen sorgen aber dafür, dass zum einen mehr makroökonomische Variablen - wie zum Beispiel die Arbeitslosigkeit - im Modell berücksichtigt werden, und zum anderen bekannte Variablen anders modelliert werden. Einige wichtige Erweiterungen des Ausgangsmodells sind in der Folge dargestellt.

\subsubsection{Das Problem der Inflationsträgheit}
Wir haben bereits im Kapitel \ref{NeuePhillips} angedeutet, dass der von der "`Neu-Keynesianischen Phillips-Kurve"' implizierte  Zusammenhang zwischen sinkender zukünftiger Inflation und steigendem Wirtschaftswachstum problematisch ist. Wir werden uns der unklaren Problematik der Inflation in Kapitel \ref{Inflation} noch widmen. Vorweggenommen sei jedoch, dass es weitgehend anerkannt ist, dass hohe Inflation negative Auswirkungen auf das Wirtschaftswachstum hat und hohe Inflation nur reduziert werden kann, wenn man Maßnahmen setzt, die Wirtschaftswachstum bremsen. Dafür wird häufig der Begriff Inflationsträgheit ("`Inflation Inertia"') verwendet. Die Geldentwertung tendiert dazu hoch zu bleiben. Möchte man sie reduzieren, muss man Wachstumseinbußen akzeptieren. Es gab in der Folge einige Ansätze diesen Widerspruch der "`Neu-Keynesianischen Phillips-Kurve"' in modernen DSGE-Modelle zu beheben. \textcite{Gali1999} und \textcite{Gali2005} erweiterten die Funktion einfach um einen "`Vergangenheits"'-Term und nannten das Resultat "`Hybride Neu-Keynesianischen Phillips-Kurve"' \parencite[S. 34]{Gali2007}. Theoretisch fundierte Modelle lieferten aber erst \textcite{Mankiw2002} und \textcite{Christiano2005}. 
Letztgenannte argumentieren, dass Unternehmen ihre Preise zwar tatsächlich nicht ständig an das optimale Gleichgewicht anpassen, aber auch nicht gänzlich unverändert lassen. Unternehmen indexieren in diesem Modell ihre Preise. Das heißt, sie passen ihre Preise jede Periode an die vergangene Inflation an, aber nur von Zeit zu Zeit an das wahre Gleichgewicht. Nach dieser Theorie macht sich ein Unternehmen nur von Zeit zu Zeit - im Modell simuliert durch einen Poisson-Zufallsprozess, wie in \textcite{Calvo1983} vorgeschlagen - die Arbeit seine Preise an das individuelle Optimum (Gleichgewichtspreis plus Mark-up) anzupassen. Dadurch entstehen gesamtwirtschaftlich die Nominalen Rigiditäten. Die Preise bleiben aber nicht gänzlich unverändert, eine Anpassung an das aktuelle Preisniveau wird nämlich jede Periode vorgenommen. Das heißt, die vergangene Inflationsrate sorgt für die laufenden Preisanpassungen. Diese Berücksichtigung der \textit{vergangenen} Inflationsrate führt zur Inflationsträgheit. Einen ähnlichen Ansatz schlagen \textcite{Mankiw2002} vor. Auch hier bleiben die Preise zwischen den zufällig zugeordneten Anpassungsterminen nicht unverändert. Anstatt die Preise aber ständig an die Inflation anzupassen, entwickeln sich diese laufend entlang eines Pfades, der wiederum nur an den zufällig zugeordneten Terminen angepasst werden kann. Die Autoren simulieren damit "`Informationsträgheit"'. Die Unternehmen passen zwar häufig die Preise an, machen sich aber nur in zufälligen Abständen die Arbeit das Optimum zu bestimmen und den Preis daran anzupassen.

Durch beide Ansätze werden alle gewünschten Eigenschaften abgedeckt. Die "`Neu-Keynesianischen Phillips-Kurve"' umfasst somit, inhaltlich begründet, die vergangene Inflation. Tatsächlich findet man den Ansatz in modernen DSGE-Modellen häufig. \textcite[S. 344f]{Romer2019} zeigt aber Gründe, warum auch dieser Ansatz nicht unumstritten ist. Zum einen fällt auf, dass die Kosten der Inflationsbekämpfung - die man ja mit diesem Ansatz abbilden will - unrealistisch gering sind, wenn man den Ansatz in einem DSGE-Modell verwendet und für die übrigen Variablen übliche Werte wählt. Zum anderen verlässt man mit dem Ansatz den sonst so hervorgehobenen Pfad der Mikrofundierung: Eine Indexierung der Preise, also eine quartalsweise Anpassung aller Preise an die veröffentlichten Inflationsrate, kann in der Empirie nicht beobachtet werden, ebensowenig vordefinierte Preispfade. Was bleibt sind Ansätze, die das gewünschte Verhalten zeigen und die empirisch beobachtbare Inflationsträgheit abbilden. Der Preis dafür sind Modelle, deren Story doch recht konstruiert wirkt.  

\subsubsection{State-Dependent Pricing}
Die Tatsache, dass jene Unternehmen, die ihre Preise in einer \textit{bestimmten Periode} anpassen, in DSGE-Modellen per Zufallsprozess ausgewählt werden, simuliert zwar sehr elegant die empirisch zu beobachtende Tatsache, dass es nominale Rigiditäten bei der Preisanpassung gibt, ist aber Modell-theoretisch natürlich ein Schwachpunkt. Preisanpassungen werden Unternehmen in der Realität nicht durch einen Zufallsprozess diktiert, vielmehr sorgen rationale - zum Beispiel Kosten der Preisanpassungen - aber auch irrationale - zum Beispiel die Trägheit bei der Preisanpassung - Überlegungen dafür, dass die Preise nicht ständig die theoretischen Gleichgewichtspreise repräsentieren. Die "`Neu-Keynesianische Phillips Kurve"', in der der Zufallsprozess nach der Idee von \textcite{Calvo1983} zur Anpassung der Preise führt, wurde dafür häufig kritisiert. \textcite{Caplin1987} hatten schon früh eine Alternative zu Calvo's Idee der rein zufälligen Preisanpassungen. Die beiden argumentieren richtigerweise, dass in Zeiten ausgeprägter geldpolitischer Schocks oder Nachfrage-Schocks, eine höhere Anzahl an Unternehmen ihre Preise anpassen wird. Bei hoher Inflation oder allgemein hoher Nachfrage werden \textit{mehr} Unternehmen realisieren, dass ihre Preise nicht mehr dem aktuellen Gleichgewichtspreis inklusive gewünschten Mark-up entsprichen. \textcite{Danziger1999} und später \textcite{Golosov2007} argumentierten in eine ähnliche Richtung. Im \textit{Danziger-Golosov-Lucas}-Modell realisieren jene Unternehmen, deren Preise am weitesten vom aktuellen Gleichgewichtspreis inklusive gewünschten Mark-up abweichen, dass sie Anpassungen derselben vornehmen sollten. Es werden als \textit{andere} (als rein zufällig ausgewählte) Unternehmen ihre Preise anpassen. Der \textcite{Caplin1987}-Ansatz, vor allem aber der \textcite{Golosov2007}-Ansatz führen dazu, dass Nominale Rigiditäten weniger stark ausgeprägt sind und in der Folge monetäre Schocks geringere Auswirkungen haben.

\subsubsection{Arbeitslosigkeit}
Wie in Kapitel \ref{cha: Neu Keynes} prominent dargestellt, revolutionierten die Neu-Keynesianer vor allem den ökonomischen Blick auf die Arbeitslosigkeit. Durch Rigiditäten lässt sich auch unfreiwillige Unterbeschäftigung erklären. Ein Hauptangriffspunkt auf die "`Real Business Cycle"'-Modelle war, dass Unterbeschäftigung nur auf freiwillige Arbeitslosigkeit zurückzuführen ist. Paradoxerweise tritt diese Schwäche des RBC-Modells im DSGE-Grundmodell aber auch auf \parencite[S. 1]{Blanchard2010}. Die Neu-Keynesianischen Elemente zur Arbeitslosigkeit wurden allerdings in einem alternativen Ansatz von \textcite{Erceg2000} berücksichtigt, eine vielzitierte Studie, in welcher der Arbeitsmarkt als monopolistischer Konkurrenzmarkt dargestellt wird. In Kapitel \ref{RR_AM} wurden verschiedene Modelle vorgestellt, warum diese Annahme die empirischen Beobachtungen gut abdeckt. Modell-technisch sorgt sie jedenfalls dafür, dass Arbeitnehmer so dargestellt werden, dass sie die Löhne etwas über dem Gleichgewichtsniveau festsetzen können. Das Resultat ist eine Arbeitslosenquote, die über die freiwillige Arbeitslosigkeit hinausgeht. Als zweites Element wurde wieder ein zufälliger Anpassungsprozess, wie in \textcite{Calvo1983} vorgeschlagen, implementiert. Das heißt, das DSGE-Modell von \textcite{Erceg2000} erweitert das Ausgangsmodell darum, dass neben den Preisen auch die Löhne mittels monopolistischem Konkurrenzmarkt abgebildet werden \textit{und}, dass auch die Löhne nicht ständig an das aktuelle Gleichgewicht angepasst werden, sondern nur zu bestimmten Zeitpunkten. Diesen Anpassungsprozess haben wir im Detail schon bei der "`Neu-Keynesianischen Phillips-Kurve"' im Zusammenhang mit der Preisanpassung kennen gelernt. Neben den Preisen sind in diesem Modell also auch Löhne rigide und werden nur zu zufälligen Zeitpunkten angepasst\footnote{Im Englischen spricht man dann von "`sticky and staggered prices and wages"'.}. Zunächst klingt diese Erweiterung unspektakulär, tatsächlich hat sie aber enorme Auswirkungen auf daraus abgeleitete wirtschaftspolitische Empfehlungen. Bisher musste die Zentralbank den Zinssatz "`einfach"' so setzen, dass die Inflation möglichst konstant bleibt, dann wird das BIP auf den natürlichen Wachstumspfad zurückkehren, der Output-Gap also sinken. Es gibt dementsprechend eine Pareto-optimale Geldpolitik, nämlich ein Form eines möglichst engmaschigen Inflation-Targeting. Durch das \textcite{Erceg2000}-Modell kommt jetzt aber eine dritte Zielgröße hinzu: Neben der Preis-Inflation und dem Output-Gap muss nun auch die Lohnentwicklung betrachtet werden \parencite[S. 282]{Erceg2000}. Die Zentralbank sieht sich also einem Trade-Off ausgesetzte, ob sie die Inflation oder die Löhne möglichst stabil halten möchte. Die Zentralbank kann im Hinblick auf \textit{zwei} rigide Kennzahlen nicht mehr die eine Pareto-optimale Lösung finden. Im Gegenteil, das bisher als optimale Strategie angesehene "`Inflation-Targeting"', führt dazu, dass die rigiden Löhne stark vom Gleichgewicht abweichen, was steigende Arbeitslosigkeit und eine höheren Output-Gap verursachen würde \parencite[S. 283]{Erceg2000}. Stattdessen sollten Zentralbanken ihre Geldpolitik auf Regeln basieren, die neben der Inflation auch Output-Gap und Lohnentwicklung berücksichtigen.
Interessant am \textcite{Erceg2000}-Ansatz ist auch, dass damit der negative Effekt von rigiden Löhnen auf den Konjunkturzyklus wieder in den Vordergrund rückt. Rigide Löhne waren schließlich schon \textit{die} wesentlichen Verstärker der zyklischen Konjunkturschwankungen im Keynesianismus (vgl.: Kapitel \ref{Keynes}). Zwischenzeitlich war das Konzept der Lohnrigiditäten aus dem Fokus der Ökonomie ziemlich verschwunden.

Die Modellerweiterungen von \textcite{Erceg2000} wurden recht rasch weitgehend übernommen und sind heute in den meisten DSGE-Modellen, die in der Praxis verwendet werden, eine Standardannahme. \textcite[S. 13]{Christiano2005}, zum Beispiel, verweisen explizit darauf, dass ihre Haushalts-Funktion die Lohnsetzung wie in \textcite{Erceg2000} vorgeschlagen, berücksichtigt.

Genau genommen ist das in \textcite{Erceg2000} dargestellte Konzept noch gar kein Modell der Arbeitslosigkeit, sondern eines, das über die Schwankungen der Nominallöhne zu Schwankungen bei der Beschäftigung führt. Das würde grundsätzlich bedeuten, dass die Angestellten das Ausmaß ihrer geleisteten Stunden anpassen, je nachdem ob der Reallohn durch die Nominallohn-Schwankungen steigt oder fällt. Erst \textcite{Gali2011a} \textit{reinterpretierte}, wie er es selbst in \textcite[S. 200]{Gali2015} bezeichnet, diese Beschäftigungsschwankungen als Arbeitslosigkeit. Dazu führte er die Annahme ein, dass ein Arbeitnehmer entweder eine fixe Anzahl von Arbeitsstunden leistet, oder gar keine, also arbeitslos ist. Nur dann führen Nominallohn-Schwankungen nämlich auch zu Schwankungen der Arbeitslosenrate und nicht nur zu Schwankungen der geleisteten Arbeitsstunden. \textcite{Gali2011a, Gali2011b} schuf in der \textit{Haushaltsfunktion} mit der Aufnahme der Arbeitslosigkeit einen direkten Zusammenhang zwischen Löhnen und Arbeitslosigkeit. \textcite[S. 441]{Gali2011a} nannte dies "`Neu-Keynesianische \textit{Lohn-}Phillips-Kurve"'\footnote{Wichtig ist hier der Unterschied zwischen Preis-Inflation und Lohn-Inflation. \textcite[S. 444]{Gali2011a} behauptet, dass die ursprüngliche Phillips-Kurve einen negativen Zusammenhang zwischen Arbeitslosigkeit und \textit{Lohn}-Inflation - also steigende Löhne - unterstellte. Erst später wurde dies erweitert auf einen negativen Zusammenhang zwischen Arbeitslosigkeit und \textit{Preis}-Inlation - also allgemein steigendes Preisniveau. Der hier - leider - von Gali verwendete Begriff "`Neu-Keynesianische Lohn-Phillips-Kurve"' ist daher auf zwei Arten verwirrend. Erstens, darf er nicht verwechselt werden mit der "`Neu-Keynesianische Phillips-Kurve, die wir in Kapitel \ref{NeuePhillips} als Funktion für das aggregierte Angebot kennen gelernt haben. Diese beiden Konzepte sind zwar von der Methodik her ähnlich, sind aber Konzepte für gegensätzliche  Marktseiten. Die "`Neu-Keynesianische Lohn-Phillips-Kurve"' ist ein Modell für die aggregierte Nachfrage, also die Haushalte, während die "`Neu-Keynesianische Phillips-Kurve"' ein Modell für das aggregierte Angebot ist, also für die Unternehmen. Zweitens, ist der Zusammenhang zwischen steigenden Löhnen und höherer Arbeitslosigkeit im Neu-Keynesianismus sehr intensiv behandelt worden (Kapitel \ref{RR_AM}), wurde dort aber nie als "`Neu-Keynesianische Lohn-Phillips-Kurve"' bezeichnet.}. In der Conclusio seines Journal-Beitrages meint \textcite{Gali2011a}, dass damit gezeigt wurde, dass ein Phillips-Kurven-artiger Zusammenhang zwischen Lohninflation und Arbeitslosigkeit besteht. Dieser lässt sich zudem direkt aus dem bestehenden \textcite{Erceg2000}-Modell ableiten. Außerdem lieferte Gali auch empirische Ergebnisse, die seine Theorie untermauern sollten. Der Ansatz wurde auch in das berühmte Smets-Wouters DSGE-Framework der Europäischen Zentralbank (siehe unten) implementiert \parencite{GaliSmetsWouters2012} hat sich aber bisher nicht vollständig durchgesetzt. 

Alternativ wurde Arbeitslosigkeit in DSGE-Modellen berücksichtigt, indem man vom Grundmodel (vgl. Kapitel \ref{DSGEGrundmodell}) ausging - also vollkommen flexible Löhne annahm - aber dafür das Modell der Friktionalen Arbeitslosigkeit (vgl. Kapitel \ref{Suchtheorie}) implementierte \parencite{Pissarides2000}. So ein Modell erstellten \textcite{Blanchard2010}, wobei die Friktionale Arbeitslosigkeit im Zusammenspiel mit den nominalen Rigiditäten bei der Preissetzung analysiert wird. Ihre zentrale Erkenntnis ist, dass Inflation von der "`Labour Market tightness"' abhängt. Dieser Begriff hat im Deutschen noch kein passendes Äquivalent erhalten und wir häufig mit "`Anspannungen auf dem Arbeitsmarkt"' übersetzt. Gemeint ist ein Mangel an Arbeitskräften\footnote{Beide Übersetzungen sind höchst unscharf. Zum ersten Begriff: Anspannungen am Arbeitsmarkt werden meist als zu \textit{hohe} Arbeitslosigkeit verstanden. Zum zweiten Begriff: Als Maß für den "`Mangel an Arbeitskräften"' wird oft das Verhältnis aus Arbeitslosen und Offenen Stellen herangezogen. Dieses ist fast immer größer als eins. Demzufolge gäbe es keinen Mangel an Arbeitskräften, sondern einen Überschuss. Passender wäre demnach vielleicht der Begriff "`Mangel an geeigneten Arbeitskräften"'.}. Ähnlich wie zuvor \textcite{Gali2011a, Gali2011b} führt dies zu einer direkten Verbindung zwischen Arbeitslosigkeit und Inflation, bei \textcite[S. 15]{Blanchard2010} aber eben über das Konzept der Friktionalen Arbeitlosigkeit. Wenn Unternehmen keine qualifizierten Arbeitnehmer finden, müssen sie ihre Suchkosten und Personalkosten erhöhen, dies erhöht die Grenzkosten und steigert damit die Inflation. Das wesentliche Ergebnis ist, ebenfalls ähnlich wie in \textcite{Gali2011a, Gali2011b}, dass eine Zentralbank beide Zielgrößen - Inflation und Arbeitslosigkeit - bei der Auswahl ihrer Strategie beachten sollte. Da es einen Trade-Off zwischen den Kennzahlen gibt, gibt es keine Strategie, die beide Ziele ins Optimum führt, damit würde auch das Problem mit dem "`Göttlichen Zufall"', der in Kapitel \ref{RR_AM} beschrieben wurde, gelöst \parencite[S. 16]{Blanchard2010}. Ebenfalls Friktionale Arbeitslosigkeit berücksichtigen \textcite{Christiano2016} in deren Modell, allerdings verzichten sie gänzlich darauf anzunehmen, dass Löhne nur regelmäßig - mittels Indexierung - oder bestimmt durch einen Zufallsprozess - also wie in \textcite{Calvo1983} vorgeschlagen - angepasst werden. Stattdessen gehen die drei Autoren von einem spieltheoretischen Verhandlungsprozess (nach \textcite{Hall2008}, vgl. Kapitel \ref{Spieltheorie}) zwischen Arbeitnehmer und Arbeitgeber aus, der es erlaubt, dass Abweichungen vom Gleichgewichtspreis entstehen. In diesem Ansatz spielt die Höhe des Arbeitslosengeldes eine bedeutende Rolle bei der Auswirkung der Arbeitslosigkeit. Eine Tatsache, die seither auch für wirtschaftspolitische Argumente genutzt wird. 

Insgesamt scheint auch dieser Zugang eine wertvolle Erweiterung des DSGE-Rahmenwerks zu sein. In angewendeten Modellen hat sich bisher aber weitgehend nur der Ansatz von \textcite{Erceg2000} durchgesetzt. Dieser ist aber - wie erwähnt - eigentlich noch kein Modell, dass die Arbeitslosigkeit im engeren Sinn berücksichtigt. Zu Arbeitslosigkeit in DSGE-Modellen gibt es zwar, neben den hier dargestellten Arbeiten \textcite{Gali2011a, Gali2011b} und \textcite{Blanchard2010} eine ganze Reihe von Arbeiten, allesamt schafften es aber bislang aber nicht als Teil des DSGE-Grundmodells anerkannt zu werden. Die Entwicklung der DSGE-Modelle in dieser Hinsicht ist auch insofern interessant, dass manche Zentralbanken - zum Beispiel die EZB - nach wie vor explizites "`Inflation Targeting"' betreiben, während andere Zentralbanken - zum Beispiel die Federal Reserve - bei ihren Zinsentscheidungen auch Arbeitslosenzahlen mitberücksichtigen. Das wirkt wie ein Spiegelbild der dahinter liegenden DSGE-Modelle. In den USA dürften die Erweiterungen, die in diesem Kapitel vorgestellt wurden, eher Eingang gefunden haben in die Geldpolitik als zum Beispiel in Europa.

\subsubsection{HANK}
Eine wesentliche Erweiterung des DSGE-Frameworks liefern \textcite{Kaplan2018}. Mit ihrem "`Heterogeneous Agent New Keynesian"'-Model (HANK) berücksichtigen die drei Autoren, dass die Standardannahme eines "`repräsentativen Haushalts"' zu kurz greift. Unter dieser Annahme sehen alle Haushalte gleich aus. In HANK geht man von dieser Annahme ab und geht stattdessen - wie der Name schon sagt - von "`\textbf{H}eterogenen \textbf{A}genten"', also unterschiedlich zusammengesetzten Haushalten aus. Dies ist in zweifacher Hinsicht eine interessante Weiterentwicklung des Rahmenwerks, weil nun die unterschiedliche Vermögensverteilung der Haushalte innerhalb von DSGE-Modellen berücksichtigt werden kann \parencite[S. 167]{Kaplan2018b}. Erstens, war und ist das Thema der Einkommens- und Vermögensverteilung nach der "`Great Recession"' wieder in den Vordergrund wirtschaftspolitischer Diskussionen getreten (vgl. Kapitel \ref{Ungleichheit}). Zweitens, gilt das DSGE-Rahmenwerk zwar diesbezüglich als Fortschritt gegenüber den "`Real-Business-Cycle"'-Modellen, aber insgesamt immer noch relativ liberal im ökonomischen Sinn \parencite[S. 309]{Romer2019}. Dies ändert sich mit HANK schlagartig. Die Ergebnisse erinnern wieder stärker an den Keynesianismus als an liberale Schulen. Wie passiert dies im Modell nun? Wie erwähnt wird vom "`repräsentativen Haushalts"' abgegangen, stattdessen wird berücksichtigt, dass unterschiedliche Haushalte unterschiedliche Vermögensverhältnisse aufweisen. Ein gewisser Teil der Haushalte verfügt in HANK über wenig Ersparnisse und einen großen Teil seines Einkommens müssen diese Haushalte stets konsumieren (hohe Marginale Konsumneigung). Diese Haushalte werden "`Von der Hand in den Mund lebende"'-Haushalte ("`hand to mouth"') genannt. Zusätzliches Einkommen würden diese Haushalte in hohem Ausmaß konsumieren und weiterhin kaum Ersparnisse anlegen. Zusätzliches Einkommen hätte demnach große Auswirkungen auf die aggregierte Nachfrage. Ein Sinken des Zinssatzes hätte hingegen kaum Auswirkungen auf das Konsumverhalten dieser Haushalte, da ja keine Ersparnisse vorhanden sind, die wegen der nun geringeren Verzinsung aufgelöst werden. Beide Annahmen sind genau gegensätzlich zu dem, was in DSGE-Standardmodellen angenommen wird. Dort spielen Zinssatzänderungen eine große Rolle, Einkommensänderungen - aufgrund der Annahme der permanenten Einkommenshypothese - hingegen kaum. Das heißt aber auch, dass die Annahme der Gültigkeit der "`Ricardianischen Äquivalenz"' nicht aufrecht zu erhalten ist. Dementsprechend ist der Unterschied zwischen Standard-DSGE-Modellen und HANK-Modellen vor allem im Hinblick auf Fiskalpolitik enorm. Diese Erkenntnis bringt auch neue Argumente in die Debatte, ob Fiskalpolitik generell zumindest während Wirtschaftskrisen sinnvoll ist. In der "`Great Recession"' wurde dies kontrovers diskutiert (vgl.: Kapitel \ref{NeueFiskal}). Der Crowding-Out-Effekt - also der Anstieg des Zinssatzes als Folge der staatlichen Ausgaben und der damit verbundene Rückgang des privaten Konsums - fällt beim HANK-Modell wesentlich geringer aus, als in Standard-DSGS-Modellen. Die zentrale Aussage des Modells ist also, dass, erstens, Fiskalpolitik - neben der weiterhin bedeutenden Geldpolitik - sehr wohl einen positiven Einfluss auf den Konjunkturzyklus hat und zweitens, die Berücksichtigung der Vermögensverteilung darauf einen entscheidenden Einfluss hat!

Die Idee die Nachfrageseite nicht als repräsentativen Haushalt, sondern heterogen abzubilden, gab es übrigens schon in den frühen 1990er Jahren \parencite{Imrohoroglu1989, Huggett1993, Aiyagari1994}, setzte sich aber nicht durch. Dementsprechend nennen \textcite[S. 699]{Kaplan2018} die Haushaltsseite in ihrem Modell auch "`Aiyagari-Huggett-Imrohoroglu incomplete market model"'. \textcite{Oh2012} und \textcite{McKay2016} erkannten schon früh nach der "`Great Recession"', dass eine Abkehr vom repräsentativen Haushalt die Entwicklungen vor und während der Krise besser erfassen könnten. \textcite{Kaplan2018} führte deren Überlegungen in ein vollständiges DSGE-Modell über. Ob sich dieses als neues DSGE-Framework durchsetzen wird, ist derzeit noch offen. Stand 2022 lässt eine Google-Suche aber erwarten, dass das Interesse daran auch außerhalb der rein akademischen Welt recht hoch zu sein scheint.

Natürlich nur von symbolischer Bedeutung aber gerade deswegen auch irgendwie erwähnenswert: Der HANK-Erstautor Greg Kaplan ist Professor an der University of Chicago. Also in  jener Stadt, in der \textit{die} großen \textit{liberalen} Ökonomen tätig waren: Milton Friedman, Friedrich Hayek\footnote{Wenn auch nicht am Economics Department und insgesamt in Chicago nicht so einflussreich wie etwa in London} und Robert Lucas\footnote{Erinnern Sie sich an dessen Aussage zur Bedeutung von Verteilungsfragen in der Ökonomie? (vgl.: Kapitel \ref{Neue Makro})}. Ausgerechnet hier wird das sonst recht liberale DSGE-Framework doch deutlich an den ursprünglichen Keynesianismus herangeführt, indem die Bedeutung der Fiskalpolitik darin wieder in den Vordergrund rückt \parencite[S. 173]{Kaplan2018b}.

\subsubsection{Sonstige Erweiterungen}
Ein wichtiges Forschungsgebiet sind DSGE-Modelle für offene Volkswirtschaften, also die Implementierung des Außenhandels. Hier gab es schon vor der Entwicklung der DSGE-Modelle wie wir sie heute kennen, Bemühungen Gleichgewichtsmodelle mit nominalen Rigiditäten und eben Außenhandel zu entwickeln. Berühmt sind in diesem Bereich vor allem \textcite{Obstfeld1995, Obstfeld2002} und die Weiterentwicklungen von \textcite{Corsetti2001}. In diesem Zusammenhang sind natürlich vor allem die Auswirkungen geldpolitischer Maßnahmen auf die Wechselkurse, sowie damit verbundene Wohlfahrtseffekte interessant.

Bei Betrachtung der DSGE-Modellgleichungen fällt auf, dass Investitionen und Staatliche Ausgaben nicht vorkommen. Investitionen werden implizit durch den Profitmaximierungsprozess der Unternehmen berücksichtigt. Staatliche Ausgaben spielen im DSGE-Grundmodell tatsächlich keine Rolle. Allerdings werden sie in den meisten praktisch eingesetzten Modellen als exogene Variable aufgenommen \parencite[S. 361]{Romer2015}.

Die globale Wirtschaftskrise "`Great Recession"', die 2007/2008 als Finanzmarktkrise begonnen hatte, schien das Interessen an DSGE-Modellen schlagartig zu verringern. Aber auch der Versuch der Implementierung von Kreditmarkt-Verwerfungen wurde durch die Krise angestoßen. Vor der Krise gab es dazu wenige Beiträge, zum Beispiel \textcite{Gertler2015}. Nach der Great Recession berücksichtigten zum Beispiel \textcite{Curdia2016} einen Finanzsektor, der Kosten für die Intermediation zwischen Sparern und Kreditnehmern (Investitionen) verrechnet. \textcite{Gertler2015} entwickelten ein DSGE-Modell, das einen großen Schattenbanken-Sektor beinhaltet, der langfristige Assets durch die Aufnahme kurzfristiger Schulden finanziert. Bei der rollierenden Refinanzierung kann es dazu kommen, dass es individuell gesehen nutzenmaximierend ist, keine neuen kurzfristigen Kredite aufzunehmen, sondern stattdessen die Assets zu verkaufen. So kommt es zu Notverkäufen ("`Fire Sales"') und damit zu Ausfällen im Bankensystem, die wiederum eine Finanzkrise auslösen.


\subsection{Status Quo und Quo vadis, DSGE?}

DSGE-Modelle traten um die Jahrtausendwende einen Siegeszug an. Sie sind noch immer \textit{die} State-of-the-Art-Modelle der Makroökonomie und auch in der Praxis heute das zentrale Analyseinstrument. Wobei natürlich vor allem Zentralbanken auf DSGE-Modelle zurückgreifen, \textcite[S. 132]{Christiano2018} zählt nur einige Institutionen, die ihre makroökonomischen Vorhersagen auf dieses Modell stützen, zum Beispiel: Der Internationale Währungsfond, die Bank of Israel, die Tschechische Nationalbank, die Schwedische Reichsbank, die Bank of Canada, die Schweizer Nationalbank und selbstverständlich auch die Federal Reserve Bank und die Europäische Zentralbank (EZB). In letzterer wurde der einigermaßen bekannt gewordene Ansatz von \textcite{SmetsWouters2003, SmetsWouters2007} entwickelt, der erstmals Bayesianische Schätzmethoden verwendet. Ein Modell, das auch Arbeitslosigkeit berücksichtigt wurde von \textcite{GaliSmetsWouters2012} ebenfalls entwickelt. Die EZB hat aber dennoch als einziges Primärziel die Geldwertstabilität festgeschrieben. Davon abweichende Ziele darf sie nur verfolgen, insofern dies mit dem Inflationsziel von 2\% vereinbar ist. Sie betreibt damit ein explizites Inflation Targeting. Wie wir gesehen haben, schlagen neuere wissenschaftliche Studien vor, dass Geldpolitik auch die Beschäftigung als Ziel berücksichtigen sollte. Dies macht die US-Notenbank Fed. Zwar verfolgt auch sie ein (implizites) 2\%-Inflation Targeting, allerdings werden bei geldpolitischen Entscheidungen auch immer Beschäftigungsziele herangezogen. 

DSGE-Modelle können als \textit{das} Element der "`Neuen Neoklassischen Synthese" schlechthin bezeichnet werden. Tatsächlich finden sich darin Ansätze aus verschiedenen Schulen:
\begin{itemize}
	\item Keynesianismus: Obwohl die DSGE-Modelle meist nur "`Neu-Keynesianische"'-Modelle genannt werden, sind aus dem ursprünglichen Keynesianismus nur die Rigiditäten übrig geblieben. Erst neuere Ansätze \parencite{Kaplan2018} berücksichtigen auch wieder die Bedeutung der Fiskalpolitik.
	\item Monetarismus: Geldpolitik ist in DSGE-Modelle sind in der kurzen Frist der zentrale Treiber, langfristig hingegen neutral. Im Kern entspricht dies den Aussagen von \textcite{Friedman1968}, wenn auch methodisch ganz anders umgesetzt. Vor allem auch die "`Permanente Einkommenshypothese"' ist eine wichtige Annahmen in Standard-DSGE-Modellen hinsichtlich des "`repräsentativen Haushalts"'.
	\item Österreichische Schule: Das Konzept des Gleichgewichtszinssatzes wurde bemerkenswerterweise schon von \textcite{Wicksel1898} herangezogen. Dieses ist dem DSGE-Ansatz so ähnlich, dass Michael Woodford - einer der ersten und führenden Entwickler von DSGE-Modellen - darauf besteht die Schule als "`Neo Wicksellianisch"' zu bezeichnen. Auch die Überinvestitionstheorie zur Erklärung der "`Great Depression"' von Friedrich Hayek, mit ihrem Fokus auf zu niedrige Zinssätze, geht in diese Richtung.
	\item Neue Klassische Makroökonomie: Zusammen mit dem "`Neu-Keynesianismus"' die wichtigste Grundlage: Die Annahme "`Rationaler Erwartungen"' im Sinne der \textcite{Lucas1976}-Kritik inklusive der Mikrofundierung der Modelle ist ein Muss-Kriterium in jedem DSGE-Modell. Die Ricardianische Äquivalenz nach \textcite{Barro1974} wird implizit durch die Methodik der DSGE-Modelle vorausgesetzt, erst in modernen HANK-Modellen wird dieses umstrittene Konzept innerhalb des DSGE-Rahmenwerks aufgeweicht. Und nicht zuletzt zu nennen ist natürlich die methodische Grundlage eines jedes DSGE-Ansatzes: die Real-Business-Cycle-Modelle von \textcite{Kydland1982}. Ebenfalls zu erwähnen die Tatsache, dass Geldpolitik Regel-basiert erfolgen soll. Diese Grundidee geht auf \textcite{Kydland1977} zurück.	
	\item Neu-Keynesianismus: Wie schon erwähnt, ebenfalls zentral und auch namensgebend. Konkrete Konzepte zu "`Nominalen Rigiditäten"' (zum Beispiel \textcite{Akerlof1985, Mankiw1985b}) und deren Berücksichtigung \parencite{Calvo1983} und zur "`Monopolistischen Konkurrenz"' \parencite{Blanchard1987}, später auch zur Berücksichtigung von Arbeitslosigkeit in Form von Friktionen am Arbeitsmarkt \parencite{MortensenPissarides1994} kommen direkt aus dem "`Neu-Keynesianismus"'.
	\item Post-Keynes: Wenn man so möchte, könnte man bei der Berücksichtigung der "`Monopolistischen Konkurrenz"' noch weiter zurückgehen. Schließlich stammt das Grundlegende Konzept des Mark-ups eigentlich von den frühen Post-Keynesianern \parencite{Robinson1933}.
\end{itemize}

Es ist beeindruckend wie elegant DSGE-Modelle diese Komponenten allesamt vereint. Das DSGE-Rahmenwerk und dessen Vertreter ist aber auch schon seit jeher heftiger Kritik ausgesetzt. Die konkreten Angriffspunkte werden im nächsten Kapitel \ref{SchwächenDSGE} behandelt. Allgemein erlitten DSGE-Modelle einen Dämpfer durch die "`Great Recession"', weil es deren Ausbruch in keinster Weise vorhersagen konnte. Als "`Schönwettermodell"' wurden diese bezeichnet, demnach gäbe es keine Probleme eine gleichbleibenden Wachstumspfad annäherungsweise Vorherzusagen, große Abweichungen, oder systematische Verwerfungen erkennen DSGE-Modelle hingegen nicht. Das Problem mit DSGE-Modellen ist zweischneidig: Auf der einen Seite ist das Ausgangs-Modell \parencite{Gali2000} zweifelsohne solide mikrofundiert und theoretisch einwandfrei. Die Ergebnisse liegen aber nahe an den Ergebnissen, die "`Real Business Cycle"'-Modelle bringen und sind damit recht weit weg von empirisch zu beobachtenden Ergebnissen \parencite[S. 311]{Romer2019}. Versucht man, andererseits, entsprechende Anpassungen vorzunehmen, bekommt man zwar empirisch besser passende Ergebnisse, aber die Modelle sind dann konstruiert und somit nicht länger wirklich mikrofundiert \parencite[S. 364]{Romer2019}. 
Nach der "`Great Recession"' äußerten sich einige Wirtschafts-Nobelpreisträger kritisch zu DSGE-Modellen: \textcite{Solow2010} vertrat die Meinung, dass DSGE-Modelle keinen Bezug zur Realität hätten und daher ungeeignet für die Politikberatung seien. \textcite{Stiglitz2017} kritisierte, dass wichtige, mikroökonomische Gesichtspunkte, in  DSGE-Modellen unberücksichtigt bleiben. Vor allem Probleme asymmetrischer Information und Finanzmarkt-Verwerfungen, aber auch das Konzept des "`repräsentativen Agenten"', der ja später im HANK-Framework tatsächlich ersetzt wurde, seien problematisch. \textcite[S. 133]{Christiano2018} widersprachen Stiglitz' Kritik übrigens vehement.

Eine offensichtliche Schwäche der Standard-DSGE-Modelle stellt die "`Neu-Keynesianische Phillips-Kurve"' dar. Ihr Vorteil liegt darin, dass sie formal hergeleitet und mikrofundiert ist, außerdem - entsprechend den Annahme der Rationalen Erwartungen - ist sie rein zukunftsorientiert. Aber hier liegt auch genau das Problem. Es ist empirisch recht unumstritten, dass Inflationsreduktion zu Wachstumseinbußen führt, die sogenannte Inflationsträgheit. Das heißt, aktuelle Inflation beeinflusst sehr wohl die zukünftige Inflation. In der Neu-Keynesianische Phillips-Kurve gibt es diesen Zusammenhang nicht.  Ein ähnlich gelagertes Problem ergibt sich für die "`Neu-Keynesianische IS-Kurve"'. In Standard-DSGE-Modellen geht man davon aus, dass Haushalte ihren Konsum vom Lebenseinkommen abhängig machen. Das heißt die Summe des aktuellen Vermögens plus aller zukünftigen Einkommen, wird über den gesamten Lebenszeitraum verteilt konsumiert. Dies entspricht den Annahmen der "`Permanenten Einkommenshypothese"' von \textcite{Friedman1957}. Wichtig dabei für DSGE-Modelle: Der aktuelle Konsum hängt damit primär vom Zinssatz ab und \textit{nicht} vom aktuellen Einkommen. Wenn man in einem Jahr zusätzliches Einkommen erzielt, führt dies nur dazu, dass das permanente Einkommen um einen kleinen Teil (zusätzliches Einkommen/Anzahl der Restlebensjahre) erhöht und hat dementsprechend nur eine kleine Auswirkung auf den aktuellen Konsum. Dies passt gut zusammen mit der Annahme, dass Fiskalpolitik wirkungslos ist. Denn selbst wenn ein Haushalt durch Fiskalpolitik kurzfristig ein höheres Einkommen erzielt, wirkt sich dies ja kaum auf seinen aktuellen Konsum aus. Da durch Fiskalpolitik der Zinssatz steigt, würde der Konsum - über diesen Wirkungskanal - fallen. Insgesamt kommt es zum vollständigen Crowding out. Empirisch ist diese vollkommene Optimierung des Konsums über den Lebenszeitraum ebenfalls sehr umstritten \parencite[S. 354]{Romer2019} und \parencite[S. 262]{Gali2015}. Eine ausgeprägte Literatur zur Kritik an dieser reinen Zukunftorientierung ("`Forward Guidance"') wurde in Folge der "`Great Recession"', während der globalen Niedrigzinsphase erstellt \parencite{DelNegro2015}.

Ein sehr allgemeines Problem ist die Frage nach dem "`natürlichen Zinssatz"' und dem "`langfristigen Wachstumspfad"'. In DSGE-Modelle spielt der Output-Gap eine große Rolle. Also die Abweichung des aktuellen BIPs vom potentiellen BIP, also jenem BIP, das ohne jegliche Schocks aber auch ohne jeglichen wirtschaftspolitischen Eingriff, realisiert wäre. Ebenso spielt die Abweichung vom natürlichen Zinssatz eine Rolle. Stimulierende Wirkung erzielt eine Zinssenkung nur wenn der Realzinssatz unter dem natürlichen Zinssatz liegt. Das Problem ist nun aber, dass es keine Möglichkeit gibt die natürlichen Levels dieser Kennzahlen zu bestimmen \parencite[S. 263]{Gali2015}.
Möglicherweise hat sich die Wachstumsdynamik in den letzten Jahrzehnten verringert. Dann würden potentielles BIP und natürlicher Zinssatz systematisch überschätzt werden und DSGE-Modelle schon aufgrund falscher Input-Daten keine vernünftigen wirtschaftspolitischen Maßnahmen liefern können.

Klar ist, dass jedes Modell immer nur einen kleinen Teil der Realität abbilden kann und somit immer daran scheitern muss, ausschließlich richtige Vorhersagen zu treffen. Unzweifelhaft sind DSGE-Modelle ein Fortschritt gegenüber den "`Real-Business-Cycle"'-Modellen, aber auch gegenüber punktuellen "`Neu-Keynesianischen"'-Lösungen für ein spezifisches Problem. Vor allem neuere Entwicklungen wie das HANK-Framework geben Hoffnung, dass noch mehr aktuell beobachtete ökonomische Entwicklungen in DSGE-Modellen berücksichtigt werden können. Eine exakte Wissenschaft kann die Ökonomie aber - definitionsgemäß - nie werden. Das heißt, es wird immer wissenschaftliche, aber auch ideologische Kritik geübt werden. Damit verbunden ist, dass Modelle den Wirtschaftspolitikern und Ökonomen Entscheidungen zwar erleichtern, aber nie gänzlich abnehmen können. Oder, wie es Stanley \textcite{Fischer2017} ausdrückte, der wiederum Paul Samuelson zitierte: "`Ich hätte lieber Bob Solow als ein ökonometrisches Modell, aber ich hätte lieber Bob Solow mit einem ökonometrischen Modell, als ohne."'


\section{Der Fokus auf Inflation und Arbeitslosigkeit}
\label{Inflation}
Die 1990er-Jahre, also jene Zeit in der die "`Neue Neoklassische Synthese"' zur vorherrschenden Schule in der Makroökonomie wurde, waren geprägt von der soeben dargestellten Idee, dass mittels Geldpolitik die Inflation stabil gehalten werden soll. Daneben spielte aber - vor allem in Europa - auch das Thema Arbeitslosigkeit eine große Rolle. In diesem Unterkapitel wird zunächst die Frage behandelt, warum niedrige und stabile Inflation überhaupt so erstrebenswert ist? Danach betrachten wir das Thema Arbeitslosigkeit.

Historisch geprägt ist die Abneigung gegen Inflation auf durchlebte Hyperinflationen zurückzuführen. Dabei versuchten Staaten sich durch Seignorage - also das Drucken von Geld - zu entschulden. Bekannte Beispiele sind die Hyperinflationen in Deutschland und Österreich Anfang der 1920er Jahre. In lateinamerikanischen und afrikanischen Staaten ist dies auch in den letzten Jahren vereinzelt immer wieder vorgekommen. Diese extreme Form der Geldentwertung ist selbstverständlich eine Katastrophe für die betroffenen Staaten, da das Tauschmittel Geld vollständig wertlos wird und damit die tägliche Geschäftstätigkeit erschwert wird.

Heute herrscht in den Wirtschaftswissenschaften weitgehend Einigung darin, dass hohe Inflation - grob gesagt also alles was über 3\% ist -  schädlich und mit Kosten verbunden ist. Dies war nicht immer so. Die Keynesianer betrachteten - vor allem im Hinblick auf die traditionelle Phillips-Kurve - eine moderaten Inflation als durchaus akzeptabel. Wie schon beschrieben, zeigten \textcite{Phelps1968} und \textcite{Friedman1968}, dass der Phillips-Kurven-Zusammenhang langfristig nicht aufrecht zu erhalten ist. Aber selbst dann - und dies ist auch eine Grundaussage der  (Neuen) Klassik -  dürfte Inflation keine Rolle spielen. Das Preisniveau hat dann keinerlei Auswirkungen auf reale Werte. Demnach sollte Inflation weder eine positive aber auch keine negative Auswirkung haben. Woher kommt dann dieser so eindeutige Konsens hinsichtlich Inflationsabneigung? Rein theoretische Arbeiten liefern wenig überzeugende Argumente \parencite[S. 589]{Romer2018}. Eine - einigermaßen bekannte - Theorie ist jene der "`Schuhsohlen-Kosten"'. Bei hoher Inflation verliert Bargeld rascher an Wert, daher müssen alle Individuen häufiger ihre Finanzanlagen in Bargeld umwandeln um nicht reale Verluste durch die Inflation zu haben. Sie müssen also öfter zur Bank laufen und daher ihre Schuhsohlen stärker abnutzen, so die namensgebende Geschichte. Die aus Kapitel \ref{cha: Neu Keynes} bereits bekannten "`Menu Costs"' sind ein zweites Beispiel für unmittelbare Inflationskosten: Ständige Anpassung der Preislisten kostet Geld. Der dritte Grund ist die "`Kalte Progression"' (oder ähnliche Steuereffekte): Einkommenssteuern werden auf Basis der Nominal-Einkommen und häufig progressiv berechnet. Steigen die Nominaleinkommen so steigt auch die Steuerlast. Wenn die Inflation die realen Nominallohnzuwächse aber auffrisst, bleibt aufgrund der gestiegenen Steuerzahlungen ein Reallohnverlust. Wenn auch alle drei Gründe ohne Zweifel empirisch auftreten, so ist die klare Ablehnung, die Ökonomen aber auch die breite Bevölkerung gegenüber Inflation zeigen, damit sicherlich nicht vollständig zu erklären. Zu vergessen ist auch nicht, dass - zumindest aus Sicht der Wirtschafts\textit{wissenschafter} - Inflation auch seine guten Seiten hat. Erstens ist Inflation die oft einzige Möglichkeit Reallöhne zu senken\footnote{Auch wenn wir mittlerweile wissen, dass dies nicht systematisch - wie von der traditionellen Phillips-Kurve postuliert - genutzt werden kann um Arbeitslosigkeit zu senken.}. Zweitens verschafft höhere Inflation der Geldpolitik einen gewissen Puffer. Zentralbanken sind bei ihrer Zinssetzung an die nominale Zinsuntergrenze gebunden. Ein nominaler Zinssatz von unter 0\% ist langfristig nicht durchzusetzen. Beträgt die Inflation 0\% so kann die Zentralbank zur Stimulierung der Wirtschaft den Realzinssatz nur auf 0\% senken. Es droht eine Deflation, die es auf jeden Fall zu verhindern gilt. Bei einer Inflation von 4\% hingegen, könnte die Zentralbank den Nominalzinssatz zwar ebenfalls nur auf 0\% senken, dies würde aber in einem Realzinssatz von -4\% resultieren. Man könnte auch sagen die Zentralbanken haben bei höherer Inflation "`mehr Pfeile im Köcher"' im Wirtschaftskrisen zu bekämpfen.

Warum aber wird dann niedrige Inflation propagiert? Vor allem empirische Gründe werden hierfür gebracht. \textcite{Fischer1993, Taylor1999, Akerlof1996} und \textcite{Lucas1994} argumentieren vor allem mit empirischen Beobachtungen, dass Wirtschaftswachstum mit hohen Inflationsraten negativ korreliert, beziehungsweise zu Wohlfahrtsverlusten führt. \textcite{Shiller1997} kam durch eine Umfrage-Studie zu dem Schluss, dass die Bevölkerung - aus Gründen, die nicht vollständig nachvollziehbar sind und sich vor allem von ökonomischen Gründen unterscheiden - eine starke Aversion gegen Inflation habe. So haben Menschen oft eine hohe Präferenz für die Haltung liquider Mittel, was rational nicht zu erklären ist. Außerdem scheint Geldillusion - also das implizite Glauben an einen stabilen Geldwert - weit verbreitet zu sein. \textcite{Ramey1995} verfeinerten die Analysen, indem sie feststellten, dass sich hohe \textit{Schwankungen} ökonomischer Kennzahlen negativ auf das Wachstum auswirken. Ähnlich argumentierten bereits \textcite{Ball1990}, dass hohe Inflation meist höhere Schwankungen vorweise. Bei niedriger Inflation herrsche ein Konsens vor, dass das für alle vorteilhaft sei. Daher gäbe es kaum Bestrebungen die niedrige Inflationsrate zu verändern, was diese gleichzeitig stabil werden lässt. Dies verringert Unsicherheit bezüglich Inflation und das wiederum ist vorteilhaft für langfristige Geschäftsbeziehungen und Investitionen. \textcite{Romer1998} zeigten außerdem, dass hohe Inflation auch negative Verteilungseffekte hat, insbesondere werde die Einkommensverteilung dadurch gespreizt.

Insgesamt führt dies dazu, dass in den Wirtschaftswissenschaften der überwiegende Konsens herrscht, dass niedrige und stabile Inflation anzustreben ist \parencite[S. 411]{Snowdon2005}. Dagegen ist nichts einzuwenden, interessant ist aber, dass man nicht genau weiß warum niedrige und stabile Inflation so vorteilhaft ist \parencite[S. 591]{Romer2018}. Fix ist aber, dass die allermeisten Zentralbanken dazu übergegangen sind, sich eine Zielinflation aufzulegen, deren Einhaltung eine primäre Aufgabe ist.

Ab Mitte der 1990er-Jahre haben sich in Zentralbanken zwei wesentliche Faktoren einer langfristigen und erfolgreichen Geldpolitik herauskristallisiert. Erstens, die Unabhängigkeit und Transparenz und zweitens, das Anstreben einer Zielinflation. Eine ganze Reihe von Artikel wurde dazu Mitte der 1990er-Jahre publiziert \parencite{Alesina1993, Mishkin2000, Svensson2000, Bernanke1997}. Bei der tatsächlichen Implementierung von Inflationszielen war laut \textcite{Bernanke1999} übrigens die Zentralbank von Neuseeland im Jahr 1990 Vorreiter. Die Geldmengenziele der Deutschen Bundesbank ab 1975 waren einem Inflation Targeting aber auch schon sehr ähnlich. Offen bleibt welchen Zielwert das Inflation Targeting nun anstreben soll. Es gibt hier keine theoretische Herleitung eines optimalen Wertes. Es gibt durchaus Ökonomen, die einen Inflationsrate von 0\% - also tatsächliche Preisstabilität - als Ziel vorschlagen. Breiter Konsens, unter anderem in \textcite{Bernanke1999b} und \textcite{Akerlof1996} dargestellt - herrscht heute aber darüber, dass ein Zielwert zwischen 1\% und 3\% sinnvoll ist. Die Abweichung vom 0\%-Zielwert wird hierbei vor allem mit den beiden oben bereits angeführten Gründen zu erklären: Einer drohenden Deflation sollte jederzeit mit Geldpolitik entgegen getreten werden können, außerdem werden dadurch reale Lohnrigiditäten verhindert.

Das zweite Thema in diesem Kapitel ist die Arbeitslosigkeit. Nur auf den ersten Blick ist dieses Thema völlig unabhängig von der gerade betrachteten Inflation. Die Phillips-Kurve, als negativer Zusammenhang zwischen Arbeitslosigkeit und Inflation, ist uns ja nun schon öfter untergekommen. In den 1990er Jahren trat schließlich praktisch der umgekehrte Blick auf das Thema in den Vordergrund: Wirtschaftswissenschaftler versuchten zu analysieren welche Arbeitslosenrate als Gleichgewichtsrate anzusehen ist. Das Konzept der NAIRU (Non-Accelerating Inflation Rate of Unemployment) - womit die Arbeitslosenrate, bei der die Inflation nicht steigt, beschrieben wird - gibt es schon seit den bahnbrechenden Arbeiten von \textcite{Friedman1968} und \textcite{Phelps1967}. In \textcite[S. 8]{Friedman1968} repräsentiert die "`Natürliche Arbeitslosigkeit"' jene Arbeitslosenrate, bei der die Ökonomie - unter Berücksichtigung aller Einflussfaktoren, wie zum Beispiel unvollkommene Märkte oder zufällige Schwankungen bei Angebot und Nachfrage - im Gleichgewicht ist. Das Konzept der NAIRU ist jenem der "`Natürlichen Arbeitslosenrate"' sehr ähnlich. Im Detail ist damit aber die Arbeitslosenrate gemeint, bei der die Inflation identisch der Zielinflation ist, bzw. die realisierten Löhne den Gleichgewichtslöhnen entsprechen. Der Begriff NAIRU wurde dabei erstmals von \textcite{Tobin1980} verwendet, zuvor als NIRU (Non-Inflationary Rate of Unemployment) von \textcite{Modigliani1975}. Nun zurück zu den 1990er Jahren, warum trat die Arbeitslosigkeit gerade zu dieser Zeit in den Fokus der Ökonomen? Nun zum einen haben wir bereits gesagt, dass die empirische Forschung seit damals extrem an Bedeutung gewonnen hat. Und zum anderen stellte man eben empirisch fest, dass die Arbeitslosigkeit, vor allem in Westeuropa, seit Anfang der 1980er Jahre stark zugenommen hat, ohne dass Rezessionen dafür verantwortlich gemacht werden könnten. Die natürliche Arbeitslosenrate\footnote{Die Bezeichnungen "`Natürliche Arbeitslosigkeit"' und "`NAIRU"' werden hier als austauschbar angesehen.} muss also entweder gestiegen sein, oder das Konzept ist falsch. Bei Vertretern der "`Neuen neoklassischen Synthese"' etablierten sich rasch eine Theorien, zur Erklärung der variablen natürlichen Arbeitslosenrate. Vor allem die Ausführung von \textcite{Blanchard1986} zur Theorie der "`Hysterese"' erhielt starke Aufmerksamkeit. Als "`Hysterese"' bezeichnet man in der Physik ein Phänomen, das man mit Wirkungsverzögerung übersetzen könnte. Dabei ist ein bestimmter Wirkungszusammenhang beim beobachten einer Variable noch immer vorhanden, obwohl der eigentliche verursachende Einfluss schon weggefallen ist. Auf Ökonomie umgelegt würde das bedeuten, dass die Arbeitslosenquote selbst dann hoch bleibt - konkret höher als die natürliche Arbeitslosenrate - wenn eine Rezession schon überwunden ist. Trotzdem das Wirtschaftswachstum, die Inflation usw., nach einer Rezession wieder auf dem Vorkrisenniveau sind, bleibt die Arbeitslosigkeit hoch. Oder mit anderen Worten: Die natürliche Arbeitslosenrate ist gestiegen. \textcite{Blanchard1986} erklären damit vor allem die Steigerungen der Arbeitslosenraten in Westeuropa. Bis 1970 waren diese in Deutschland, Großbritannien und Frankreich extrem niedrig mit maximal 2\%. In den USA waren knapp 5\% normal. Die Ölkrisen und die Stagflation der 1970er Jahre ließen die Zahl der Beschäftigungslosen sowohl in Amerika als auch in Europa in die Höhe schnellen. Während in den USA die Arbeitslosigkeit in den 1980er Jahren wieder abnahm, blieb diese in Europa ungewöhnlich hoch \parencite[S. 18]{Blanchard1986}. \textcite{Blanchard1986} liefern selbst verschiedene Erklärungsansätze für die "`Hysterese"'. Insgesamt löste die empirische Beobachtung steigender natürlicher Arbeitslosenraten einen enormen Forschungsaufwand aus. Dieser wurde sogar noch verstärkt, als in der zweiten Hälfte der 1990er Jahre die (natürlichen) Arbeitslosenraten sowohl in den USA als auch in Europa wieder deutlich zurückgingen. \textcite{Staiger2001} machten vor allem demographische Änderungen für die Variabilität der NAIRU verantwortlich. Unter anderem \textcite{Krueger1999} und \textcite{Autor2001} fanden mit mikroökonomischen Arbeiten teilweise Erklärungen dafür. \textcite{Ball2002} geben eine Theorie wieder, die sowohl den Anstieg der NAIRU in den 1970er Jahren, als auch deren Rückgang in den späten 1990er Jahren erklären kann. In der erstgenannten Periode fielen die Produktivitäts-Zuwachsraten nachdem diese zuvor zwei jahrzehntelangen stetig gestiegen sind. Da die Real-Löhne in der neoklassischen Theorie mit den Produktivitäts-Zuwachsraten Schritt halten, wäre es angemessen gewesen, wenn die Löhne in den 1970er Jahren ebenfalls weniger stark gewachsen wären. Die Arbeitnehmer waren aber an die höheren Lohnwachstumsraten gewohnt und setzten diese - trotz gesunkener Produktivitäts-Zuwachsraten - auch weiterhin durch. Als Folge stieg die natürliche Arbeitslosigkeit. In der zweiten Hälfte der 1990er Jahre beobachtete man schließlich das umgekehrte: Die Produktivität stieg in der "`New Economy-Phase"' stärker als zuvor. Die Zuwächse bei den Real-Löhnen fielen aber niedriger aus als jene bei der Produktivität, was die natürliche Arbeitslosenrate senkte.

Das Konzept der NAIRU wurde in der Neuen neoklassischen Synthese durch die Theorie der Hysterese erweitert. Insgesamt bleibt das Thema Arbeitslosigkeit und die NAIRU im Speziellen Gegenstand intensiver Forschung. Wobei in den 1990er Jahre vor allem - hier nicht dargestellte - mikro-ökonometrische Verfahren entwickelt und exzessiv angewendet wurden. In der Makroökonomie ist das Konzept der NAIRU nach wie vor ein wesentlicher Bestandteil der Mainstream Ökonomie.






