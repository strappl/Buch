%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neu-Keynesianismus} \label{cha: Neu Keynes}

Der "`Neu-Keynesianismus"' weniger eindeutig von anderen Schulen abzugrenzen als etwa der Keynesianismus oder der Monetarismus. Dies gilt sowohl in inhaltlicher Sicht, also auch in zeitlicher Sicht. Inhaltlich lässt sich der Neu-Keynesianismus am ehesten negativ abgrenzen. Einige Ökonomen erkannten, dass die Theorien der Keynesianer nicht mehr zeitgemäß waren. Sie akzeptierten aber auch nicht die starren Annahmen der "`Neuen Klassiker"'. Diese Ökonomen könnte man "`Neu-Keynesianer"' nennen. Die "`Neu-Keynesianer"' sind dementsprechend als keine geschlossene Gruppierung von Ökonomen entstanden, sondern behandelten eher zerstreut einzelne Fragen der Ökonomie. Dazu gehörten zum Beispiel Fragen der Inflation (Phillips-Kurve), der Arbeitslosigkeit (Suchproblem, Natürliche Arbeitslosigkeit) und des Marktversagens (Informationsasymmetrien, Natürliche Monopole). 
Auch was die Personen betrifft ist die Abgrenzung schwierig. So ist der Erzliberale \textsc{John Taylor} - Präsidenten der Mont Pelerin Society von 2018 - 2020 - inhaltlich zweifelsohne als ein früher Vertreter des Neu-Keynesianismus anzusehen. Ebenso aber steuerte auch Joseph Stiglitz - scharfer Kritiker von liberaler Wirtschaftspolitik - wichtige Beiträge zum "`Neu-Keynesianismus"' bei.

Eine interessante Einordnung des "`Neu-Keynesianismus"' nimmt \textcite[S. 21]{RomerDavid1993} vor. Er beschränkt sich hierbei auf zwei Fragen um die verschiedenen Schulen voneinander abzugrenzen. Erstens, \textit{Sind die (meisten) Märkte im wesentlichen perfekte Konkurrenzmärkte, auf denen es zu einer Markträumung im Sinne des Walrasianischen Gleichgewichts kommt?} und zweitens, \textit{Hat die "`klassische Dichotomie"', also das Zusammenhang zwischen Realwirtschaft und Finanzwirtschaft im Sinne der Quantitätsgleichung des Geldes, Gültigkeit?}.

Die "`Neu-Klassiker"' würden beide Fragen mit Ja beantworten. Die "`Neu-Keynesianer"' hingegen mit Nein. Keynesianer würden die erste Frage bejahen, die zweite allerdings verneinen. Während die Monetaristen ebenso die erste Frage bejahen würden, bei der zweiten aber wohl unstimmig wären.

Insgesamt umfasst der "`Neu-Keynesianismus"' vor allem folgende Eigenschaften, die ihn gegen andere Schulen abgrenzen. Die Neu-Keynesianer akzeptieren die Vorteile der Mikrofundierung der Makroökonomie, die sie als Fortschritt gegenüber den rein makroökonomische fundierten Modellannahmen der Keynesianer und Monetaristen sehen. Ähnlich ist es im Hinblick auf Rationale Erwartungen. Auch diese akzeptieren die Neu-Keynesianer weitgehend. In diesen beiden Punkten grenzen sie sich vom Keynesianismus und Monetarismus ab. Die Neu-Keynesianer gehen aber auch von der Existenz von Rigiditäten auf verschiedenen Märkten aus. Was dazu führt - wie wir später sehen - ,dass aktiver Wirtschaftspolitik eine stabilisierende Wirkung zukommen kann. Dies steht in Verbindung mit einer weiteren Annahme der "`Neu-Keynesianer"', nämlich, dass die meisten Märkte imperfekte Märkte sind auf denen sich die Preise wie untere monopolistischer Konkurrenz bilden. Diese beiden Punkte sind wiederum die Abgrenzung gegenüber der "`Neuen klassischen Makroökonomie"'. 

Die schwierigere Abgrenzung erfolgt aber in zeitlicher Hinsicht. Schließlich wird die heutige Mainstream-Ökonomie häufig als "`Neu-Keynesianismus"' bezeichnet. In dieser Logik müsste man den "`Neu-Keynesianismus"' zumindest in zwei Generationen teilen. Zweifelsohne beginnt der "`Neu-Keynesianismus"' nämlich als Antwort auf die "`Neue Klassische Makroökonomie"' ab den frühen 1980er Jahren zu existieren. In Wahrheit sogar schon deutlich früher, nämlich mit der Kritik an der Phillips-Kurve ab Mitte der 1960er Jahre. Dieser "`frühe"' Neu-Keynesianismus wird in diesem Kapitel beschrieben und dauerte bis etwa Anfang der 1990er Jahren. Die Hauptproponenten sind hier \textit{Edmund Phelps, Peter Diamond, Joseph Stiglitz, George Akerlof, Michael Spence, John Taylor und Olivier Blanchard}. Diese Schule war der Gegenpol zur aufstrebenden "`Neuen Klassischen Makroökonomie"' um Lucas, Sargeant und Barro. Die Vertreter dieses frühen Neu-Keynesianismus liefern mit ihren Arbeiten vor allem "`Aufweichungen"' der zu starren Annahmen der "`Neuen Klassiker"'. Sie lehnen in diesem Sinn die Arbeiten der "`Neuen Klassiker"' ab, akzeptieren aber auch, dass der Keynesianismus veraltet ist. Von der Zuordnung der Personen her entwickelte sich der "`Neu-Keynesianismus"' eher aus den Salzwasser-Universitäten (vgl. die entsprechende Einteilung in Kapitel \ref{Neue Makro}), die die Neuen Klassiker ja strikt - und nicht nur auf inhaltlicher Ebene - ablehnten. Es wurden aber nicht alle Keynesianer zu "`Neu-Keynesianern"': James Tobin zum Beispiel bestand darauf ein "`Alt-Keynesianer"' zu sein \parencite[S. 45ff]{Tobin1993}. Edmund Phelps drückte dies so aus: "`I [had] warm personal relations with Jim [James] Tobin and Bob [Robert] Solow as well as with Bob [Robert] Lucas and Tom [Thomas] Sargent – relations that have survived our differences. But I belonged to neither school." \parencite{Phelps2006}.

Ab Anfang der 1990er Jahre kam es zunehmend zu einer Verschmelzung von "`Neu-Keynesianismus"' und "`Neuer Klassischer Makroökonomie"'. Diese wird in diesem Buch im nächsten Kapitel als "`Neue Neoklassische Synthese"' beschrieben. Da es eher eine Verdrängung der "`starren"' Neuen Klassischen Makroökonomie durch junge Vertreter des "`Neu-Keynesianismus"' ist, wird sie aber häufig auch einfach "`Neu-Keynesianismus"' genannt\footnote{Man könnte sie auch Zweite Generation des "`Neu-Keynesianismus"' nennen}. Die Hauptvertreter sind hier \textsc{John Taylor}\footnote{der aber auch zur ersten Generation der Neu-Keynesianer gezählt werden muss} \textsc{David Romer, Greg Mankiw, Paul Krugman und Jordi Gal\i}. Der Unterschied zwischen der ersten Generation der Neu-Keynesianer und der zweiten Generation ("`Neue Neoklassische Synthese"') ist, dass die Letztgenannte vor allem die Methoden der "`Neuen Klassiker"', insbesondere "`Dynamische Stochastische General Equilibrium"'-Modelle aus der "`Real Business Cycle"'-Theorie, übernommen hat und um ursprünglich keynesianische Elemente, nämlich "`Monopolistische Konkurrenz"', "`Rigide Löhne und Preise"' und "`Nicht-Neutralität der Geldpolitik"' (und Fiskalpolitik) in der kurzen Frist, übernommen hat. Mehr dazu aber im nächsten Kapitel

Allgemein aber täuscht der Name "`Neu-Keynesianismus"' auf jeden Fall: Er ist nicht etwa eine Weiterentwicklung des Keynesianismus. Schon die hier beschriebene "`Erste Generation der Neu-Keynesianer"' akzeptierte inhaltlich und methodologisch die Fortschritte durch die "`Neuen Klassiker"', bestand aber auf der Bedeutung von Fiskal- und vor allem Geldpolitik, sowie der Existenz von Marktversagen. 

\section{Mikroökonomische Vorläufer: Marktversagen} \label{cha: Marktversagen}

\subsection{Informationsasymmetrie}
Akerlof: Market for Lemons
Stiglitz: Adverse Selektion auf Versicherungsmärkten (Screening)
Moral Hazard - Signalling (Spence)
Eventuell: Shiller Finanzmarktinstabilitäten

\subsection{Natürliche Monopole}
Natürliche Monopole oder Baumol's angreifbare Märkte


HIER WEITER
 
 
 

\section{Phelps: Mikrofoundation der Makroökonomie}
\label{micmac}

Man findet wohl kaum einen Namen, der den Übergang von "`Keynesianismus"' zu "`Neu-Keynesianismus"' besser repräsentiert als \textsc{Edmund Phelps}. Ökonomisch geprägt wurde er in einem eindeutig keynesianischen Umfeld: Er verfasste bei James Tobin seine Dissertation und arbeitete Mitte der 1960er Jahre mit Robert Solow, Paul Samuelson und Franco Modigliani zusammen. Also alles eindeutig keynesianische Ökonomen, die wir aus Kapitel \ref{Synthese} kennen. Laut seines autobiografischen Artikels \textcite[S. 93]{Heertje1995} war diese Zeit, inklusive Gastprofessur am Massachusetts Institute of Technologie (MIT), die prägendste seiner Karriere. Er selbst war innerhalb weniger Jahre ein international anerkannter Ökonom. Schon 1961 veröffentlichte er sein erstes bedeutendes Werk: \textit{The Golden Rule of Accumulation} \parencite{Phelps1961}. Ein bemerkenswerter Artikel, den der gerade mal 28-jährige Phelps im American Economic Review veröffentlichte. Gerade einmal sieben Seiten lang, beginnt dieser - so wie im Englischen normalerweise Märchen  - mit "`Once upon a time"'. In weiterer Folge wechseln sich mathematische Formeln mit Dialogen zwischen dem König und dem Volk der Solovians ab \parencite[S. 640]{Phelps1961}. So witzig und amüsant die Geschichte des Artikels, so bahnbrechend ist auch deren Inhalt. Diese Arbeit kann als direkter Anschluss an die Wachstumstheorie Solow's gesehen werden und im Zentrum steht folgende hypothetische Überlegung: Wenn die gesamte aktuelle Wirtschaftsleistung für die Investition (Investition = Sparen!) in neue Produktionsgüter verwendet wird, dann wird nichts für den aktuellen Konsum ausgegeben. Wird hingegen die gesamte aktuelle Wirtschaftsleistung für Konsum verwendet, werden im Umkehrschluss keinerlei neuen Investitionen getätigt. Beide Extrembetrachtungen führen also zu keinem sinnvollen Gleichgewicht. Das heißt aber auch, dass dazwischen irgendein optimales Verhältnis zwischen Sparen/Investieren auf der einen Seite und Konsumieren auf der anderen Seite bestehen muss. Dieses erreicht man eben durch \textit{The Golden Rule of Accumulation}. Diese wird erreicht - solange man einige vereinfachenden Annahmen zulässt - wenn die Wachstumsrate des BIPs dem Zinssatz entspricht. Bereits Phelps nannte diese natürliche Wachstumsrate "`nachhaltig"' \parencite[S. 638]{Phelps1961}. Weiters zeigt Phelps formal, dass diese Wachstumsrate erzielt wird, wenn die Summe der Investitionen der Summe der Profite entspricht, also alle Profite investiert werden. Umgekehrt werden im Optimum alle Löhne konsumiert. Zusammengefasst: Wenn alle Löhne konsumiert werden und alle Profite investiert werden, befindet sich die Ökonomie auf einem nachhaltigen Wachstumspfad. Die Wachstumsrate entspricht dann dem Zinssatz. Insgesamt erinnert das Ergebnis an die Arbeiten von Wicksell und Hayek. Die formale Herleitung durch Phelps war aber zu diesem Zeitpunkt - im Jahre 1961 - eine bahnbrechende Erweiterung des Solow-Wachstumsmodells.

Das bisher in diesem Unterkapitel dargestellte, entspricht noch vollständig dem keynesianischem Denken aus Kapitel \ref{Synthese}. Im Jahr 1966 wechselte Phelps von Yale an die University of Pennsylvania (Penn). Mit dem Umzug konzentrierte er sich auf neue Themen, nämlich auf die theoretische Fundierung der Phillipskurve. Seine Arbeiten dazu sollten später die ersten Zweifel am dominierenden, keynesianschen Framework begründen. Im Nachhinein kann man getrost sagen, dass damit die Grundlagen für den "`Neu-Keynesianismus"' geschaffen wurden.

Wie in Kapitel \ref{sec: Phillips} dargestellt, war der vermeintliche, negative Zusammenhang zwischen Inflation und Arbeitslosigkeit zwar nicht Bestandteil der ursprünglichen keynesianischen Theorie. Aber in weiterer Folge vor allem in der keynesianischen Wirtschaftspolitik ein fixer Bestandteil. Unabhängig voneinander waren Milton Friedman und eben Edmund Phelps bereits ab Mitte der 1960er Jahre die ersten Ökonomen, die den Zusammenhang zwischen Inflation und Arbeitslosigkeit in Frage stellten. Wohlgemerkt zu einer Zeit, in der der Zusammenhang empirisch noch recht gut beobachtet werden konnte. Das in den 1970er Jahren diese Korrelation weitgehend verschwand gab den Kritikern Friedman und Phelps natürlich gehörig Auftrieb. Phelps hatte seine Kritik dabei - im Gegensatz zu Friedman - mathematisch-formal unterlegt. 

Der Artikel mit dem unscheinbaren Titel "`Money-Wage Dynamics and Labor-Market Equilibrium"' \parencite{Phelps1968} stellte die bis dahin unbestrittene Phillipskurve nicht nur infrage, sondern legte die Grundlage für eine ganz neue Sicht auf die Wirtschaftswissenschaften. Interessant ist, dass gleich mehrere Punkte, die natürlich ineinandergriffen, in diesem Artikel revolutionäre waren:
\begin{enumerate}
\item Die Mikrofundierung der Makroökonomie
\item Die formale Einführung der Erwartungen (als adaptive Erwartungen) als Notwendigkeit bei Unvollständiger Information
\item Die formale Einführung der Natürlichen Arbeitslosigkeit und "`Effizienzlöhne"'
\end{enumerate}
Bemerkenswert ist insbesondere, dass alle drei genannten Punkte bis heute fixer Bestandteil der Mainstream-Modelle sind. Die heutigen DSGE-Modelle sind mikrofundiert, beinhalten das Konzept der Erwartungen (wenn auch der rationalen statt der adaptiven) und akzeptieren einen gewissen Prozentsatz an Arbeitslosigkeit als Gleichgewichtszustand. Natürlich wurden alle drei Konzepte seit 1968 wesentlich erweitert, aber im Gegensatz zu den Arbeiten anderer großen Ökonomen, fällt auf, dass Phelps' Arbeiten bis heute, 50 Jahre später, kaum an Gültigkeit verloren. Keynes' Multiplikator ist heute höchst umstritten, Friedman's Geldmengensteuerung betreibt keine Zentralbank der Welt mehr und selbst die späteren Arbeiten von Robert Lucas wurden größtenteils von der Realität überholt. Phelps' bahnbrechende Erkenntnisse sind hingegen bis heute die Grundlage ökonomischer Modelle und kann daher als Geburtsstunde des "`Neu-Keynesianismus"' gesehen werden.

In seiner Nobelpreis-Biographie schreibt Phelps, dass es seit seiner College-Zeit das Gefühl hatte die wichtigste aktuelle Herausforderung der Wirtschaftswissenschaften sei die Integration der Mikroökonomie in die Makroökonomie \parencite{Phelps2006}. Heute nennen wir dies die Mikrofundierung der Makroökonomie.
Der inhaltliche Ausgangspunkt des oben genannten Artikels \parencite{Phelps1968} ist die Phillipskurve. Phelps beschreibt sie als Naivität der Keynesianer. Wobei er Keynes selbst ausdrücklich in Schutz nimmt: Keynes' Nachfragesteuerung wäre niemals soweit gegangen einen dauerhaft stabilen Zusammenhang zwischen Inflation und Arbeitslosigkeit anzunehmen \parencite{Phelps2006}. Phelps stellt stattdessen einen Zusammenhang zwischen der \textit{erwarteteten} Inflation und Arbeitslosigkeit her. Dieser Zusammenhang sei aber nur in der kurzen Frist stabil. Angenommen die erwartete Inflation läge bei 4\%. Arbeitgeber und Arbeitnehmer würden bei ihren Vertragsverhandlungen diese Inflationserwartung einfließen lassen und die Lohnhöhe entsprechend festlegen. Will die Zentralbank nun die Arbeitslosigkeit senken, kann sie Maßnahmen setzen, die die Inflation auf zum Beispiel 6\% erhöhen. Solange die erwartete Inflation unter der tatsächlichen Inflation liegt, wird die Arbeitslosigkeit sinken und sich somit wie von der Phillipskurve postuliert verhalten. Es ist aber klar, dass die Diskrepanz zwischen tatsächlicher und erwarteter Inflation nur kurzfristig aufrechterhalten werden kann, bevor sich die Erwartung dem tatsächlichen Wert anpasst. Die keynesianische, langfristige Phillipskurve wurde durch die neu-keynesianische, kurzfristige erwartungsgestützte Phillipskurve ersetzt. Als solche findet sie bis heute Eingang in die makroökonomischen Lehrbücher. Nebenbei etablierte Phelps dabei das Konzept der adaptiven Erwartungen, das aber später vom neuklassischen Konzept der rationalen Erwartungen abgelöst werden sollte.
Die zentrale Aussage in \textcite{Phelps1968} lautet, dass durch Geldpolitik die Arbeitslosigkeit nicht dauerhaft beeinflusst werden kann, sehr wohl aber unter Umständen in der kurzen Frist. Geldpolitik funktioniere außerdem über Inflations\textit{erwartungen} und diese passen sich recht schnell an die aktuelle Inflation an. Eine niedrige Inflation wird daher auch nicht langfristig zu höherer Arbeitslosigkeit führen\parencite{Phelps1967}. Daraus könnte man ableiten, dass die zentrale Aufgabe der Zentralbanken die Inflationssteuerung ist. Heute orientieren sich fast alle führenden Zentralbanken tatsächlich primär an den Inflationszielen, dies aber direkt auf Phelps' frühe Arbeiten zurückzuführen ginge aber zu weit, folgten doch noch weitere Arbeiten dazu von anderen Neu-Keynesianern und Neuen Klassikern.
Sehr wohl direkte Folge aus \textcite{Phelps1968} ist hingegen die Idee der "`natürlichen Arbeitslosenrate"', später häufig als NAIRU\footnote{non-accelerating inflation rate of unemployment} bezeichnet. Während die Klassiker davon ausgingen, dass es im Gleichgewicht keine Arbeitslosigkeit gäbe und die Neuen Klassiker meinten im Gleichgewicht gäbe es ausschließlich freiwillige Arbeitslosigkeit, verfolgten die Keynesianer den Ansatz Arbeitslosigkeit sei stets mit nachfrageorientierter Wirtschaftspolitik zu minimieren. Die Neu-Keynesianer gehen davon aus, dass es im Gleichgewicht ein gewisses Maß an unfreiwilliger Arbeitslosigkeit gäbe. Diese "`natürliche Arbeitslosenrate"' wird häufig Milton Friedman zugeschrieben, der einen sehr ähnlichen Ansatz ebenfalls 1968 veröffentlichte \parencite{Friedman1968}. Tatsächlich hatten Friedman und Phelps unterschiedliche Wege gewählt, die sie zu den gleichen Schlussfolgerungen führten. Der Begriff "`natürliche Arbeitslosenrate"' ist wohl Friedman zuzuschreiben, größeren Einfluss in der akademischen Welt hatte aber der Ansatz von Phelps \textcite[S. 9f]{Nobelpreis-Komitee2006}. Die theoretische Bearbeitung der Arbeitslosigkeit wurde später zu einem eigenen Forschungsgebiet innerhalb des "`Neu-Keynesianismus"' und ist in Kapitel \ref{Suchtheorie} dargestellt.
Die zentralen Aussagen der damals neuen Theorie wurden in einer Konferenz aufgearbeitet und schließlich gesammelt als Buch veröffentlicht \textcite{Phelps1970}. Dieses war in weiterer Folge einflussreich und erreichte unter Ökonomen einen hohen Bekanntheitsgrad unter dem Titel "`The Phelps volume"'.

Die Mikrofundierung der Makroökonomie wird ebenfalls häufig als wesentliche Neuerung der "`Neuen Klassischen Makroökonomie"' gesehen. Es ist aus methodischer Sicht \textit{der} große Bruch mit den Theorien der Keynesianer und auch Monetaristen. Tatsächlich ist nicht von der Hand zu weisen, dass die Neuen Klassiker diesen Ansatz als Standard in ökonomischen Modellen etablierten (vgl. Kapitel \ref{Neue Makro}). Aber auch hier gilt, dass die erstmalige Anwendung auf Phelps zurückgeht. In der nun schon häufig zitierten Arbeit \textcite{Phelps1968} verwendet Phelps ein mikroökonomisches Modell um den klar makroökonomischen Zusammenhang zwischen Inflation und Arbeitslosigkeit zu modellieren. Die dort notwendige intertemporale Betrachtung (im Kapitel \ref{Neue Makro} haben wir das schlicht "`dynamische Betrachtung"' genannt) der Interaktion der Kennzahlen war eine weitere Neuerung durch Phelps, die bis heute State-of-the-art in den Wirtschaftswissenschaften ist \parencite[S. 9]{Nobelpreis-Komitee2006}.

Die soeben dargestellten Ergebnisse wurden nicht unmittelbar begeistert aufgenommen, wie dies zum Beispiel mit Keynes' General Theory, oder der Theorie der Rationalen Erwartungen von Lucas passierte. Bei ihrem erscheinen Ende der 1960er-Jahre konkurrierten die Ideen von Phelps mit zahlreichen alternativen Ideen. Erst etwas später wurde durch die Stagflation sichtbar, dass die Phillipskurve in ihrer alten Form untragbar wurde und Phelps Erwartungsgestützte Phillipskurve als Alternative zielführender sei.

Für die Ende der 1060er Jahre noch unumschränkt dominierende keynesianische Theorie waren Phelps' Ergebnisse gewissermaßen schockierend: Die Phillipskurve war zwar wenig theoretisch begründet, spielte aber in der keynesianisch geprägten Wirtschaftspolitik eine wichtige Rolle. Das Ergebnis, dass Geldpolitik in der langen Frist als nachfrageorientierte Wirtschaftspolitik wirkungslos sei, beschränkte das keynesianische Framework. Die Mikrofundierung der Makroökonomie beschritt methodisch gänzlich neue Wege. 

Kurz zusammengefasst kann Phelps' frühes wirken so beschrieben werden: Er entwickelte noch in der Tradition der neoklassischen Synthese die "`Goldene Regel der Akkumumlation"'. Mit seinem Umzug an die University of Pennsylvania emanzipierte er sich aber vom Keynesianismus und es folgten Arbeiten die zur Revolution der Phillipskurve führen sollten. Diese Arbeiten umfassten Konzepte, die bis heute in der Mainstream-Ökonomie State-of-the-Art sind. Erstens, war er ein Vorreiter bei der Mikrofundierung der Makroökonomie und zweitens, etablierte er (adaptive) "`Erwartungen"' in den Modellen der Ökonomie. Beides spielte später bei den "`Neuen Klassikern"' eine wesentliche Rolle, wenn auch in der Form der \textit{rationalen} statt der \textit{adaptiven} Erwartungen. Er nahm also die Kritikpunkte der Neuen Klassiker am Keynesianismus vorweg. Man beachte, dass das bahnbrechende Werk \textcite{Phelps1968} noch vor der Neu-Klassischen Revolution erschien \parencite{Lucas1972, Lucas1976}. Er wurde aber kein Vertreter dieser "`Neuen Klassiker"', sondern war sich immer der Unvollständigkeit der Märkte bewusst, die sich in unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten ausdrückte. Er nahm der Wirtschaftspolitik damit die Illusion einer funktionierenden Feinsteuerung der Wirtschaft, ohne dabei aber einer vollkommenen Marktgläubigkeit zu verfallen. Ich denke man kann ihn daher getrost als eigentlichen Begründer des "`Neu-Keynesianismus"' bezeichnen.

Die eben genannten Arbeiten zur Unvollständigkeit der Märkte, unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten wurden später als Antwort auf die empirischen Defizite der "`Neuen Klassiker"' ausgearbeitet. Sie bilden den Kern der Ersten Generation des Neu-Keynesianismus.


\section{"'Kern"' des Neukeynesianismus}
\label{Kern}

Diese Arbeiten stellen den Kern der Neu-Keynesianer 1. Generation dar, weil hier erstmals die zwei wesentlichen Punkte des Neu-Keynesianismus zusammengefügt werden: Erstens, die Monopolistische Konkurrenz im Zusammenhang mit den Nominalen Rigiditäten und den Menu Costs und zweitens, die Nicht-Neutralität der Geldpolitik aus dem Spannungsverhältnis Nominale vs. Reale Rigiditäten und das daraus ableitbare Nicht-Vorhandensein der Klassischen Dichotomie. \parencite{RomerDavid1993}

\subsection{Nominale Rigiditäten}
\label{Nominale Rigiditäten}

Die Arbeiten von Phelps waren, wie gerade erwähnt, der Ursprung des Neu-Keynesianismus und wurden zeitlich vor der neu-klassischen Revolution formuliert. Die meisten neu-keynesianischen Arbeiten der 1. Generation entstanden allerdings als direkte Antworten auf die aufkommenden aber mit starren Annahmen unterlegten Arbeiten der "`Neuen Klassiker"'.

\textcite{Lucas1976} gab den Anstoß zur "`Neuen klassischen Makroökonomie"' und damit zur Theorie der rationalen Erwartungen. \textcite{Sargent1975} steuerten ihre berühmte "`policy-ineffectiveness proposition"' bei, also die Annahme, dass jegliche Wirtschaftspolitik ohne Effekt verpufft. Bereits 1977 folgten - als direkte Antwort - zwei Arbeiten \textcite{Taylor1977, Fischer1977} von Ökonomen, die eben nicht Teil der "`Neuen Klassik"' sein wollten, aber die Überlegenheit einzelner Elemente daraus akzeptierten. Das realisiert man bereits wenn man nur das Abstract der beiden Artikel liest. Sinngemäß steht da, dass aktive Geldpolitik sehr wohl eine Wirkung haben kann, da Löhne in der kurzen Frist rigide sind. Dies sei unabhängig von der Annahme rationaler Erwartungen. Zwei typisch neu-keynesianische Elemente kommen hier vor. Erstens, das Vorhandensein von (nominalen) Rigiditäten und damit die Wirksamkeit von aktiver Wirtschaftspolitik und somit die Ablehnung der "`policy-ineffectiveness proposition"' und zweitens, die implizite Akzeptanz der Annahme rationaler Erwartungen.
Die Annahme "`Adaptiver Erwartungen"' im Sinne von \textcite{Phelps1968} bedeutet, dass Entscheidungsträger, also zum Beispiel die Zentralbank, für einen Unterschied zwischen tatsächlicher Inflationsrate und erwarteter Inflationsrate sorgen kann. Die "`Rationalen Erwartungen"' nach \textcite{Lucas1976} gehen hier weiter und behaupten, dass es keine Differenz zwischen tatsächlicher und erwarteter Inflationsrate geben kann. Wenn nämlich die Zielinflation (oder zu dieser Zeit noch die Geldmengenziel - "`money supply rule"') bekannt ist, dann wissen die Haushalte ebensogut wie die Entscheidungsträger in welcher Form auf überschießende oder zu niedrige Inflation reagiert wird.  Sowohl \textcite{Fischer1977} als auch \textcite{Taylor1977} akzeptieren die Existenz von Rationalen Erwartungen. Die von den Neuen Klassikern daraus abgeleitete Wirkungslosigkeit von Geldpolitik hingegen lehnen sie hingegen strikt ab. Das Argument dafür ist, dass es langfristige Verträge gibt die Löhne (in \textcite{Fischer1977}), bzw. Preise (in \textcite{Taylor1977}) festsetzen. Geldpolitik hingegen kann laufend vorgenommen werden. Das Ergebnis ist, das diese nominale Lohn- und Preisrigidität, das Geldpolitik in der kurzen Frist sehr wohl wirksam ist. Die beiden genannten Werke gelten heute noch als ein Eckpfeiler der Neu-Keynesianischen Theorie. Das Ergebnis ist in gewisser Weise paradox, denn waren es nicht die "`alten"' Keynesianer, die behaupteten, dass Geldpolitik wirksam ist ("`money matters"'), wenn Preise und Löhne rigide sind? \textcite[S. 166]{Taylor1977} sind sich dessen bewusst. Aber Sie schränken ein, dass die postulierten Zusammenhänge ganz andere waren und nur ihre Theorie mit der Annahme Rationaler Erwartungen vereinbar sei. Oder wie es \textcite[S. 166]{Taylor1977} ausdrücken: \textit{"'By adopting the framework of rational expectations, we hope to have produced not a new wine but an old wine in a new and more secure bottle."'}
Die Akzeptanz der Gültigkeit der Annahme Rationaler Erwartungen ist übrigens bis heute ein Streitpunkt zwischen den "`alten"' Keynesianern (also den Vertretern der Neoklassischen Synthese) und den Neu-Keynesianern, aber auch innerhalb der Gruppe der Neu-Keynesianer. Für die Vertreter der Neoklassischen Synthesen ist diese Annahme schlicht unrealistisch. Innerhalb der Gruppe der Neu-Keynesianer gibt es durchaus auch Zweifler am Konzept der Rationalen Erwartungen.

\textcite{Taylor1979, Taylor1980} verallgemeinerte die Aussagen dahingehend, dass Rigiditäten auch außerhalb der strengen Annahmen fixer Laufzeiten bei Arbeitsverträgen auftreten. In seinem Modell geht er davon aus, dass Verträge gestaffelt neu für eine gewisse Zeitperiode ausverhandelt werden. Das Modell wird dementsprechend als "`Staggered contracts"', oder "`Taylor contracts"' bezeichnet. Im einfachsten Fall kann man davon ausgehen, dass Löhne in Arbeitsverträgen nur alle zwei Jahre angepasst werden. Das heißt jedes Kalenderjahr wird eine Hälfte der Arbeitsverträge an die beobachtete Inflation angepasst. Taylor konnte so zeigen, dass durch diese künstlich modellierte Lohnrigidität eine Abweichung vom langfristigen Gleichgewicht entsteht. \textcite{Blanchard1983} wendete dieses Modell auf die Preissetzung von Waren an und stellte fest, dass bei längeren Herstellungsketten der Effekt der Rigidität größer ist. Diese frühen Modelle der nominalen Rigiditäten waren noch nicht mikroökonomisch fundiert \parencite[S. 194]{Fischer1977}, was auch von Seiten der Neuen Klassiker recht rasch zu entsprechender Kritik führte. Dies wurde später durch die Arbeit von \textcite{Rotemberg1987} behoben. Die Annahme nominaler Lohnrigiditäten (nicht nominaler Preisrigiditäten) wurde später als unzureichend kritisiert \parencite{Mankiw1990}, weil die Reallöhne während Rezessionen steigen würden, was einerseits empirischen Beobachtungen widerspricht und andererseits dazu führt, dass Wirtschaftskrisen bei Personen mit sicheren Jobs sehr populär wären \parencite[S. 371]{Snowdon2005}. Erst in Verbindung mit der Annahme "`Monopolistischer Märkte"' (Imperfekte Märkte), "`Nicht-kostenloser Preisanpassung"' (Menu Costs), und "`Friktionen auf den Arbeitsmärkten"', die allesamt ebenfalls als Neu-Keynesianische Markenzeichen in diesem Kapitel noch besprochen werden, lassen sich rigide Löhne rechtfertigen.
Die nominale Preisrigidität überlebte aber und ist bis heute Teil der aktuellen Mainstream-Modelle! In den  Neu-Keynesianischen DSGE-Gesamtmodellen, die in Kapitel \ref{Neue Neoklassische Synthese} beschrieben werden, wird nominale Preisrigidität mittels "`staggered price setting"'-Modell von \textcite{Calvo1983} modelliert. Er bezieht sich dabei direkt auf die Arbeiten von \textcite{Taylor1979, Taylor1980}, macht daraus aber ein stochastisches Modell. Das heißt, Unternehmen können die Preise und Löhne nicht mehr nach Ablauf einer gewissen Zeitspannen anpassen, sondern erst jeweils nach einem zufällig langem Zeitraum. Das heißt die Preisanpassungen nach einem exogenen Schock finden noch unregelmäßiger statt. Dies bildet empirische Beobachtungen noch besser ab. 

Überzeugende Empirische Evidenz für die Existenz von nominalen Preisrigiditäten konnte man erst mit dem Aufkommen von Mikro-Datensätzen um die Jahrtausendwende erstellen. \textcite{Nakamura2008} fanden heraus, dass nominale Preise etwa neun bis elf Monate im Durchschnitt Bestand haben, dass es also nominale Rigiditäten tatsächlich gibt. Die beiden kritisieren aber auch, dass in den am häufigsten zitierten Modellen von \textcite{Taylor1980} und \textcite{Calvo1983} bei Preisänderungen stets von Preis\textit{erhöhungen} ausgeht. In ihrer empirischen Studie fanden \textcite[S. 1442]{Nakamura2008} hingegen heraus, dass mehr als ein Drittel aller Preisänderungen im Beobachtungszeitraum aber Preissenkungen waren.


Die modelltheoretischen Grundlagen wie man nominale Rigiditäten \textit{berücksichtigen} kann, waren also schon früh durch \textcite{Taylor1977, Fischer1977}, bzw. \textcite{Calvo1983} geschaffen worden, wie soeben dargestellt. Unbeantwortet hingegen blieb bislang die Frage, wie es zu diesen nominalen Rigiditäten überhaupt \textit{kommen kann}. Die Antwort darauf lieferten \textcite{Mankiw1985b, Akerlof1985, Parkin1986, RomerDavid1990} und \textcite{Ball1988}. 

Das erste, berühmt gewordene, Anwendungsbeispiel sind die sogenannten "`Menu Costs"', also "`Speisekarten-Kosten"'. Die Ausgangsannahme ist, dass die Anpassung von Preisen selbst Kosten verursacht. Daraus leitet sich der von \textcite{Mankiw1985b} geprägte Begriff der "`Menu Costs"' ab. Dahinter steht folgende exemplarische Idee: Das Drucken neuer Speisekarten kostet Geld. Gastronomen müssen also abschätzen, ab welchem Ausmaß von Preiserhöhungen Mehrerlöse entstehen, die den Druckkostenaufwand wieder ausgleichen\footnote{Die Idee entstand in den 1980er Jahren, also vor der großen digitalen Revolution. Heute wären diese "`Menu Costs"' im Wortsinn vermutlich wesentlich geringer als 1985.}. Eine sehr einfache Idee, die für die meisten individuellen Unternehmen kaum von Belang ist. \textcite{Akerlof1985, Mankiw1985b, Parkin1986} zeigten aber jeweils, dass diese kleinen individuellen Effekte zu großen makroökonomischen Effekten führen können. \textcite{Rotemberg1987} nannte diese Erkenntnis "`PAYM insights"', angelehnt an die Anfangsbuchstaben der vier Autoren. Konkret führt auf Märkten mit Monopolistischer Konkurrenz (siehe nächstes Kapitel) das individuell Nutzen-maximierende Verhalten der einzelnen Unternehmer\footnote{\textcite[S. 823]{Akerlof1985} nennen es \textit{"'Insignifikant"' suboptimales Verhalten}, bzw. \textit{Nahe-Rationales Verhalten}} dazu, dass Preisanpassungen an den Gleichgewichtspreis erst bei größeren Preissprüngen vorgenommen werden.  Kommt es also zu einem (geringen) Rückgang der aggregierten Nachfrage, werden Unternehmen bei monopolistischer Konkurrenz ihre Preise aufgrund der "`Menu Costs"' zunächst nicht anpassen, sondern stattdessen, trotz niedriger Nachfrage, den ehemaligen Gleichgewichtspreis verlangen. Gesamtwirtschaftlich optimal wäre es, wenn die Unternehmen ihren Preis senken würden. Das würde man auch in der neoklassischen Analyse von Monopolmärkten erwarten. Da Unternehmen aber "`Menu Costs"' ausgesetzt sind, werden die höheren Preis beibehalten. Für Unternehmen ist dieses Verhalten Nutzen-maximierend, weil die Preis-Anpassungskosten höher wären als der zusätzliche Gewinn, den Unternehmen bei geringerem Preis aber höherer abgesetzter Menge, erhalten würden \parencite[S. 372]{Snowdon2005}. Gesamtwirtschaftlich ist das Ergebnis aber suboptimal, weil die abgesetzte Menge beim rigiden Preis viel geringer ist als die theoretische Gleichgewichtsmenge.\footnote{Dieser Ansatz mit Monopolistischer Konkurrenz und de-facto Preissetzung ist nicht unähnlich frühen Post-Keynesianischen Ansätzen (vgl. \ref{Post-Keynes})! Auch wenn Neu-Keynesianer dies nur ungern zugeben würden.} 

Bleibt man auf Ebene eines einzelnen Unternehmens ist die Erkenntnis zwar durchaus interessant, sie scheint aber weitgehend folgenlos für die Gesamtwirtschaft. Das ist aber ein Trugschluss. Die wichtigste Erkenntnis der "`PAYM-insights"' ist, dass kleine - auf natürliche Schwankungen zurückzuführende - Rückgänge bei der aggregierten Nachfrage zu deutlichen Schwankungen beim gesamtwirtschaftlichen Output führen\parencite[S. 375]{Snowdon2005}. Da solche Schwankungen unerwünscht sind, argumentieren \textcite{Akerlof1985}, dass aktive Wirtschaftspolitik\footnote{\textcite[S.837]{Akerlof1985} schreiben konkret nur von Geldpolitik} sehr wohl einen stabilisierenden und damit wünschenswerten Effekt hat.


\subsection{Reale Rigiditäten}
\label{Reale Rigiditäten}

Anfang der 1990er Jahre wurde das Konzept der Rigiditäten verfeinert. Denn zwar konnte durch die "`PAYM-insights"' gezeigt werden, dass es theoretisch möglich ist, dass nominale Rigiditäten zu großen Schwankungen im Gesamtoutput führen, aber eben auch, dass dies in der Realität sehr unwahrscheinliche sei. Wie \textcite[S. 183]{RomerDavid1990} herausarbeiten mussten dazu ganz bestimmte Bedingungen erfüllt sind. So würde es, zum Beispiel, beim Auftreten von nominalen Preisrigiditäten nur dann zu großen Effekten auf den Gesamtoutput kommen, wenn der Arbeitsmarkt gleichzeitig extrem elastisch wäre. Gastronomen würden demnach zwar bei Nachfragerückgängen ihre Preise auf den Speisekarten unverändert lassen, aber gleichzeitig sofort Köche und Kellner kündigen?! Ein eher unrealistisches Szenario. Stattdessen führten \textcite{RomerDavid1990, Ball1988, Ball1989} \textit{reale} Rigiditäten als notwendige Ergänzung zu \textit{nominalen} Rigiditäten ein.

Zunächst muss einmal abgegrenzt werden, wie sich reale Rigiditäten von nominalen Rigiditäten unterscheiden. Nominale Rigiditäten haben wir schon als "`Menu Costs"' kennengelernt. Allgemein könnte man nominale Rigiditäten definieren als zeitlich begrenzte, geringfügige Abweichungen vom Marktgleichgewicht, die allerdings keinen langfristigen Bestand haben und damit keine langfristige ökonomische Begründung. Nehmen wir zum Beispiel die "`Menu Costs"': Die Verkaufspreise nicht bei jeder geringfügigen Änderung der Preise vorgelagerter Waren zu ändern ist rational. Wenn die Preisänderung vorgelagerten Waren aber beständig und/oder groß genug ist, wird es zu Preisanpassungen an das Marktgleichgewicht kommen. Man könnte nominale Rigiditäten auch als die Geschwindigkeit, mit der sich Löhne und Preise an das Gleichgewicht anpassen, definieren \parencite[S. 270]{Blanchard2003}. Bei nominalen Rigiditäten kommt der Effekt der Geldpolitik ins Spiel. Dazu ein einfaches Beispiel: Stellen Sie sich vor Sie möchten einen Apfel kaufen. Dieser sei mit 1EUR/Stück angeschrieben. Was glauben Sie was passiert, wenn in diesem Moment die Geldmenge verdoppelt wird (und alles andere gleich bleibt)? Intuitive Antwort: "`Dann kostet der Apfel 2EUR/Stück"'. Das ist auch intuitiv richtig. Nur was, wenn es eben nicht augenblicklich nach Ausweitungen der Geldmenge zu entsprechenden Preisanpassungen kommt? Dann ist die Geldpolitik eben "`Nicht-Neutral"'. Diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise kann sich die Wirtschaftspolitik zunutze machen und eben "`Geldpolitik"' betreiben. 
Glaubt man, wie die Neuen Klassiker, an die "`Klassische Dichotomie"' zwischen Geldmarkt und Gütermarkt auch in der kurzen Frist, ist diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise nicht vorhanden. Dann gibt es keine wirksame Geldpolitik. Die Neu-Keynesianer hingegen lehnen diese "`Klassische Dichotomie"' zumindest für die kurze Frist ab. Damit akzeptieren sie das temporäre Auseinanderlaufen von realen und nominalen Werten und eben auch die Wirksamkeit von Geldpolitik!

Von realen Rigiditäten spricht man, wenn es rationale Gründe gibt, warum sich Preise auch in der langen Frist nicht an das eigentliche Marktgleichgewicht anpassen. Reale Rigiditäten sind also - im Gegensatz zu nominalen Rigiditäten - kein vorübergehendes Phänomen. Sie bleiben langfristig bestehen, weil die Marktteilnehmer aus verschiedenen, individuell-nutzemaximierenden Gründen, keine Anreize haben von ihrer Preissetzung abzuweichen. Und zwar auch dann nicht, wenn die Preise nicht die allgemeinen Gleichgewichtspreise sind. Mit diesen Gründen beschäftigte sich ein ganzes Forschungsfeld, das in den folgenden Unterkapiteln beleuchtet wird. 

Vorgezogen beleuchten wir an dieser stelle die Arbeit von \textcite{RomerDavid1990}, die in ihrer Arbeit den Unterschied zwischen und die Bedeutung von nominalen und realen Rigiditäten mit den Forschungsfeldern identifizierten. Mit ihren Überlegungen grenzten \textcite{RomerDavid1990} den Neu-Keynesianismus ein weiteres Mal entscheidend als eigene ökonomische Denkrichtung ab und trugen mit der Verbindung der einzelnen Elemente dazu bei, dass der Neu-Keynesianismus als einheitliches Gesamtmodell gesehen werden kann. Davor waren Beiträge stets als ablehnende Antwort gegenüber den Neuen Klassikern entstanden, die aber eher unabhängig voneinander gesehen werden mussten. Betrachten wir den Inhalt dieses, für den Neu-Keynesianismus, wichtigen Beitrags: \textcite[S. 183]{RomerDavid1990} heben gleich zu Beginn hervor, "`dass \textit{reale} Rigiditäten nicht das gleiche sind wie \textit{nominale} Rigiditäten"'. Bis Ende der 1980er Jahre entstanden zwar viele Forschungsarbeiten zu Rigiditäten, diese unterschieden aber nicht zwischen realen und nominalen Effekten. Im nächsten Schritt erstellen die beiden ein interessantes aber komplexes Modell. Dessen Grundaussage lautet wie folgt: Erstens, nominale Rigiditäten ("`Menu Costs"') können realistischerweise nur zu kleinen gesamtwirtschaftlichen Schwankungen führen. Zweitens, reale Rigiditäten können alleinstehend kaum existieren: Auf Märkten ohne jegliche nominale Rigidität, kommt es immer zur Anpassung an das Marktgleichgewicht. Würde man hier aufhören, wäre die Essenz: Rigiditäten spielen keine Rolle. Aber jetzt kommt der Clou aus \textcite{RomerDavid1990}: Treten nominale Rigiditäten auf, so können auch reale Rigiditäten existieren. In diesem Fall verstärken die realen Rigiditäten die nominalen Rigiditäten und es kann zu großen Schwankungen im Gesamtoutput kommen. Diese wiederum rechtfertigen - wie schon im Paper von \textcite{Akerlof1985} - den Einsatz aktiver Wirtschaftspolitik.
Die soeben beschriebenen Ergebnisse sind in \textcite{RomerDavid1990} natürlich nicht bloß als plausibles Narrativ formuliert, sondern aus einem formal-theoretischen Modellrahmen abgeleitet. Die dort dargestellten Beispiele zeigen, dass bei realen Rigiditäten vor allem Arbeitsmarkt-Effekte - in geringerem Ausmaß Gütermarkt-Effekte - große Wohlfahrtsverluste (Schwankungen im Gesamtoutput) verursachen. Das zentrale Beispiel in \textcite{RomerDavid1990} betrachtet ein Modell, in dem der Arbeits- als auch der Gütermarkt berücksichtigt werden, und in dem Rigidität bei Gütermarkt-Preisen durch rigide Reallöhne verursacht werden. Das Modell bedient sich zwei Annahmen. 

Erstens, die Autoren nehmen an, dass Unternehmen "`Effizienz-Löhne"' bezahlen. Das sind Löhne, die etwas höher sind als der Gleichgewichtslohn um die Arbeitnehmer zu besserer Leistung zu motivieren\footnote{Details dazu im Unterkapitel \ref{RR_AM}}. Zusätzlich geht man davon aus, dass Arbeitsmärkte recht unelastisch sind. Das würde implizieren, dass sich Löhne pro-zyklisch verhalten: Bei guter Konjunktur sind freie Arbeitskräfte gefragt aber rar. Durch die Unelastizität der Arbeitsmärkte, müssen die Löhne überproportional angehoben werden, um zusätzliches Personal zu finden. Akzeptiert man die Existenz von Effizienz-Löhnen, kann man erklären wie es gleichzeitig zur Unelastizität von Löhnen und zur Azyklizität der Real-Löhne kommen kann. Beides entsprach in den 1980er Jahren nämlich empirischen Beobachtungen \parencite{RomerDavid1990}. 
Zweitens, funktioniert das Modell nur wenn man annimmt, dass die Rigidität der Reallöhne, Rigidität bei realen Preisen verursacht (und nicht etwa umgekehrt). Dann, und nur dann, tritt nämlich die folgende Wirkungskette ein: Ein Nachfrage-Schock führt nur zu einem geringen Anstieg der Reallöhne und somit zu einem geringen Anstieg der Grenzkosten der Unternehmen. Das heißt, die Unternehmen haben wenig Motivation ihre Preise zu senken. Im Aggregat ist das neue realisierte Gleichgewicht aber dennoch deutlich unterschiedlich vom Marktgleichgewicht bei perfekten Wettbewerb. Der Gesamtoutput fällt im Modell also erheblich. 

Man merkt schon, dass das ganze Modell etwas konstruiert wirkt, wenn auch mit plausiblen Werten und Annahmen. Insgesamt ist der Ansatz der Versuch die zu einfache Sichtweise der "`Neuen Klassiker"' zu durchbrechen. Indem man einzelne, zu beobachtende Marktvorgänge, die nicht dem perfekten Wettbewerb abbilden, modelliert, kommt man zu großen Abweichungen beim Gesamtergebnis. Der Artikel von \textcite{RomerDavid1990} ist auch deshalb so entscheidend für den Neu-Keynesianismus, weil es die drei wesentlichen Elemente, die im Neu-Keynesianismus zuvor jeweils einzeln analysiert wurden, zusammenführt:
\begin{itemize}
	\item Rigiditäten: "`Menu Costs"', aber auch "`Sticky Wages"' verhindern die sofortige Anpassung an den Gleichgewichtspreis.
	\item Nicht Neutralität des Geldes: Abgeleitet durch die akzeptierte Existenz der Rigiditäten, muss es in der kurzen Frist einen Unterschied zwischen nominalen und realen Werten geben. Diese Differenz kann man durch Geldpolitik künstlich steuern. Die "`Klassische Dichotomie"' zwischen Geldmarkt und Realmarkt ist zumindest in der kurzen Frist Illusion
	\item Monopolistische Konkurrenzmärkte: \textit{Die} Neuerung in der \textit{Modellierung} war die Berücksichtigung nicht perfekter Märkte. Die Annahme, dass sich Preise nicht ausschließlich durch Angebot und Nachfrage und einen Walrasianischen Auktionator ergeben, führt überhaupt erst zur Möglichkeit von Rigiditäten. Ist aber im Hinblick auf die meisten Gütermärkte und den Arbeitsmarkt realistischer.
\end{itemize}
Die Einbettung dieses Rahmenwerks in die Modellwelt der Neuen Klassiker, die DSGE-Modelle der "`Real Business Cycle"'-Theorie führte schließlich zum "`Neu-Keynesianismus der 2. Generation"' (Neue Neoklassische Synthese, vgl. Kapitel \ref{Neue Neoklassische Synthese}), der heute noch weitgehend den Mainstream in der Ökonomie darstellt.

Vielleicht haben Sie sich beim Lesen schon gefragt: Okay, es gibt einen Unterschied zwischen den Nominalen Rigiditäten - die verzögerte Anpassung der Preise - und Realen Rigiditäten. Was sind aber jetzt Reale Rgiditäten in der Praxis? Das Ausgangsbeispiel mit den Löhnen, die während Zeiten der Deflation nicht sinken, ist nämlich höchst theoretisch. Deflation trat selbst während der "`Great Recession"' nach 2008 nur sehr vereinzelt auf.
Mit den verschiedenen Quellen Realer Rigiditäten beschäftigen sich die nächsten Unterkapitel. Es ist an dieser Stelle aber darauf hingewiesen, dass die dort vorgestellten Arbeiten nicht ursprünglich als Quellen Realer Rigiditäten identifiziert wurden. Der Entstehungsweg war anders herum: Man versuchte empirisch beobachtbare Abweichungen vom ökonomischen Gleichgewicht zu erklären. Erst \textcite{RomerDavid1990} vereinte dieses Sammelsurium an Erklärungen unter dem Begriff Reale Rigiditäten \textcite[S. 4]{Mankiw1991})


\subsubsection{Unvollkommenheiten am Arbeitsmarkt}
\label{RR_AM}

Der Arbeitsmarkt spielt bei den Neu-Keynesianern eine zentrale Rolle. Erstens, bei der Frage warum Real-Löhne scheinbar deutlich weniger prozyklisch agieren, als in der klassischen Ökonomie angenommen. Diese Frage ist allgemein zentral für die Existenz von Rigiditäten. Der zweiten Frage widmen wir uns hier: Wie kann es zu unfreiwilliger Arbeitslosigkeit kommen?\footnote{Der Arbeitsmarkt spielt, drittens, eine wesentliche Rolle in der Suchtheorie die gesondert in Kapitel \ref{Suchtheorie} behandelt wird.}

Der Arbeitsmarkt wurde in der VWL sehr lange als ein "`gewöhnlicher"' Markt, analog zum Gütermarkt betrachtet. Löhne ergeben sich hier wie Preise aus Angebot und Nachfrage und Treffen sich im Gleichgewicht. Aus heutiger Sicht erscheint es überraschend wie spät in der Makroökonomie erstmals darüber nachgedacht wurde, ob die Produktivität eines Arbeitnehmers mit höheren Löhnen steigen könnte und in der Folge Arbeitgeber einen Anreiz haben, höhere Löhne (als den Gleichgewichtslohn) zu bezahlen? In der Psychologie/Betriebswirtschaftslehre spielten solche Überlegungen früher eine Rolle wie die berühmte Literatur von \textcite{Maslow1943, Herzberg1966, McClelland1961, McGregor1960} zeigt. In der Volkswirtschaftslehre wurde dies erstmals in den 1970er Jahren diskutiert. Wie schon mehrmals beschrieben, zerbrach zu der Zeit der keynesianische Konsens der Makroökonomie. Neben dem Anstieg der Inflation und dem damit verbundenen Ende der Theorie der Phillips-Kurve, war auch ein Anstieg der Arbeitslosenraten zu beobachten. Mit der keynesianischen Theorie war dies nicht in Einklang zu bringen, dort wurde Arbeitslosigkeit schließlich durch eine Unterauslastung der Wirtschaft bei gleichzeitig rigiden Löhnen verursacht. Während einer Deflation steigt dann der reale Wert rigider Nominallöhne. Die realen Löhne wären dann höher als der Gleichgewichtslohn, was in Arbeitslosigkeit resultiert.
In den 1970er Jahren war man aber weit entfernt von Deflation, womit dieser Erklärungsansatz scheiterte. Die Neuen Klassiker machten es sich einfach und behaupteten es gäbe keine unfreiwillige Arbeitslosigkeit. Ein Ansatz, der von Neu-Keynesianern - wie schon in Kapitel \ref{Neue Makro} beschrieben - geradezu lächerlich gemacht wurde. Allerdings hatten die Neu-Keynesianer zunächst keine eigene Erklärungen parat. Erste Ansätze gingen von empirischen Beobachtungen aus: Man konnte sehen, dass selbst in Zeiten hohen Arbeitskräfteangebots kaum ein Unternehmen versuchte die Real-Löhne seiner Mitarbeiter zu kürzen. Dies wäre in der klassischen Theorie eigentlich zu erwarten, denn ein Überangebot an Arbeitskräften sollte gleichzeitig zu einem sinkenden Gleichgewichtslohn führen. Erste Erklärungsansätze gingen folglich in die Richtung, dass die Arbeitnehmer in Gewerkschaften stark organisiert wären und daher wie ein Monopolist auftraten. Die resultierenden zu hohen Löhne verursachten dann Arbeitslosigkeit (QUELLE VVV). Dies war aber mit der Empirie nicht vereinbar sobald man Arbeitsmärkte im Detail betrachtete. So waren in den USA verhältnismäßig wenig Arbeitnehmer Mitglieder in Gewerkschaften.
Weit erfolgreicher war die Effizienzlohn-Hypothese, die zunächst von \textcite{Stiglitz1976b} vorgeschlagen wurde. Diese hat eine Vorbedingung: Es \textit{muss} einen positiven Zusammenhang zwischen der Höhe des Lohnes und der Arbeitsproduktivität des Arbeitnehmers geben. Für diesen Fall zeigt \textcite{Stiglitz1976b}, dass es unter der Prämisse der Gewinnmaximierung rational ist, dass ein Lohn bezahlt wird, der üblicherweise höher ist als der eigentliche Gleichgewichtslohn. Warum? Ein Gewinn-maximierendes Unternehmen wird jenen Lohn bezahlen, bei dem das Verhältnis aus Arbeitsproduktivität und Lohn maximal ist. Steigt die Arbeitsproduktivität zumindest in einem gewissen Bereich überproportional zur Lohnhöhe wird das optimale Verhältnis aus Arbeitsproduktivität und Lohn nicht beim ökonomischen Gleichgewichtslohn, sondern bei einem höheren Lohn erreicht. Dieser Lohn wird "`Effizienzlohn"' genannt.
Ein höherer Lohn als der Gleichgewichtslohn erklärt warum es zu unfreiwilliger Arbeitslosigkeit kommen kann. Der eben beschriebene "`Effizienzlohn"' erklärt zusätzlich warum Arbeitssuchende sich nicht einfach in einen Job hinein-reklamieren können, indem sie ihre Arbeitskraft zu einem niedrigeren Lohn anbieten\footnote{Das "`Insider-Outsider-Modell"' argumentiert hier allerdings, dass die Lohnkosten für neue, unternehmens-externe, Mitarbeiter aufgrund von Kosten der Personalsuche, Einschulungskosten und auch sozialen Kosten, im Sinne von Demotivation durch hohe Fluktuation, in Wahrheit wesentlich höher sind als der Gleichgewichtslohn am Markt. In der Folge ist es nicht so einfach möglich sich in ein Unternehmen hinein zu reklamieren indem man einen niedrigeren Lohn anbietet wie hier beschrieben.} (und somit den eigentlichen Gleichgewichtslohn wieder herstellen würden). Die Arbeitgeber gehen davon aus, dass höhere Löhne gleichzeitig höhere Produktivität bedeuten. Sie wollen daher gar keine Arbeitnehmer die zum niedrigeren Gleichgewichtslohn arbeiten, sondern sie wollen Arbeitnehmer bei denen das Verhältnis zwischen Lohn und Produktivität optimal ist. 
 
So weit so gut. Bleibt die Frage der mikroökonomischen Fundierung: Warum sollte es überhaupt einen Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität geben? Aus praktischer Sicht scheint die Antwort klar und intuitiv und man würde eher die Gegenfrage stellen? Warum soll es \textit{keinen} derartigen Zusammenhang geben? Die rein ökonomische Antwort auf diese Gegenfrage lautet: Weil auf einem perfekten Markt unendlich viele Arbeitgeber unendlich vielen Arbeitnehmern gegenüberstehen. Und wenn ein Arbeitnehmer die im Arbeitsvertrag vereinbarten Pflichten nicht erfüllt, wird er solange gegen einen anderen ausgetauscht, bis die Anforderungen erfüllt werden. Die Realität ist weder so hart, noch so einfach. Daher wurden in der Folge verschiedene Modelle \parencite{Yellen1984} entwickelt, die die Annahme zum Zusammenhang von Lohnhöhe und Arbeitsproduktivität formal plausibilisierten. Fünf davon haben sich als etabliert: 

Der erste Ansatz ist das sogenannte "`Shirking-Modell"'\footnote{"'Shirking"' (engl.) entspricht in etwa dem deutschen "`sich drücken"'. Dementsprechend lautete der nicht geläufige, deutsche Name: "`Drückeberger-Modell"'}. Es basiert, ebenso wie das zweite Modell, auf das in Kürze eingegangen wird, auf dem Problem der Informationsasymmetrie, die wir ja schon im Kapitel \ref{cha: Marktversagen} kennen gelernt haben. Das bekannteste Shirking-Modell ist wohl die Shapiro-Stiglitz-Hypothese \parencite{ShapiroStiglitz1984}. Demnach können Arbeitgeber ihre Arbeitnehmer nicht zu hundert Prozent monitoren. Dementsprechend entsteht ein Moral Hazard Problem. Die Arbeitnehmer können sich dazu entschließen weniger zu arbeiten als im Vertrag vorgesehen. Die Arbeitgeber werden nicht alle Drückeberger identifizieren können, aber jene, die sie beim "`shirken"' erwischen, werden sie kündigen. Auf einem perfekten Wettbewerbsmarkt wäre das aber für den ertappten Drückeberger wenig problematisch. Schließlich gibt auf so einem Markt keine unfreiwillige Arbeitslosigkeit und es wird der Gleichgewichtslohn bezahlt. Das heißt, jeder Drückeberger findet sofort einen neuen Job zu gleichem Lohn.  Bei \textcite{ShapiroStiglitz1984} könnten Arbeitgeber folglich beschließen einen höheren Lohn - den "`Effizienz-Lohn"' - zu bezahlen. Dann hätten Arbeitnehmer nämlich den Anreiz fleißig zu arbeiten. Der sonstige drohende Jobverlust wäre jetzt nämlich problematisch für die Dienstnehmer, da alternativ nur der niedrigere Gleichgewichtslohn bleibt. Wenn aber alle Arbeitgeber so vorgehen und als Anreiz höhere Löhne bezahlen, gibt es ausschließlich den "`Effizienz-Lohn"' am Markt. Da dieser höher ist als der Gleichgewichtslohn wird der Markt nicht vollständig geräumt. Mit anderen Worten: Es kommt zu unfreiwilliger Arbeitslosigkeit. Die Arbeitnehmer haben übrigens in diesem Modell auch dann keinen Anreiz zu "`shirken"', wenn der "`Effizienz-Lohn"' den Gleichgewichtslohn vollständig verdrängt hat. Schließlich droht nun als Konsequenz des erwischt werdens nach wie vor die Kündigung. Da es nun aber unfreiwillige Arbeitslosigkeit gibt, können sich Arbeitnehmer nicht darauf verlassen sofort wieder einen Job zu finden. Zusammengefasst: Das "`Shirking-Modell"' geht also davon aus, dass höhere Löhne bezahlt werden, weil sich die Unternehmen dadurch höhere Arbeitsproduktivität - in der Form von weniger "`shirking"' - versprechen. Folglich bekommen alle Arbeitnehmer einen "`Effizienz-Lohn"' angeboten, der höher als der Gleichgewichtslohn ist. Dies resultiert darin, dass es stabil einen bestimmten Prozentsatz unfreiwillige Arbeitslosigkeit gibt.

Als zweite Erklärung dient das Adverse-Selektions-Modell. Auch hier wird der Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe dadurch erklärt, dass es eine Informationsasymmetrie zwischen Arbeitgeber und Arbeitnehmer gibt. Im Gegensatz zum "`Shirking-Modell"' bezieht es sich aber auf einen Informationsmangel seitens der Arbeitgeber \textit{vor} Vertragsabschluss. Im diesbezüglich am häufigsten zitierte Paper von \textcite{Weiss1980} argumentiert, dass Arbeitgeber mit hohen ausgeschriebenen Löhnen ein Signal an den Markt senden: Wer diesen Job annimmt, muss eine Produktivität in entsprechender Höhe leisten. Arbeitnehmer würden sich demnach ausschließlich auf Stellen bewerben, bei denen das angebotene Gehalt ähnlich hoch ist wie ihr persönlicher Mindestlohn. Unternehmen werden in diesem Modell ihre existierende Lohnstruktur in so einem Fall selbst dann nicht nach unten anpassen, wenn der Gleichgewichtslohn am Markt fällt. Dann nämlich würden sofort die besten Mitarbeiter freiwillig kündigen, da diese auf dem Arbeitsmarkt die besten Jobaussichten haben. Diese Rigidität der Löhne führt insgesamt zu höheren Löhnen als dem Gleichgewichtslohn, was wiederum in unfreiwilliger Arbeitslosigkeit resultiert.

Der dritte Ansatz ist das "`Labor-Turnover-Model"' und findet bereits bei \textcite{Phelps1968} Erwähnung. Also in jenem Journal-Artikel, der einen der ersten Anstöße zum Neu-Keynesianismus darstellt (vgl. Kapitel \ref{micmac}). Ausgangspunkt ist, dass die Suche nach Arbeitnehmern sowie deren Einschulung ein kostenintensiver Prozess ist. Eingeschulte Arbeitnehmer liefern also eine höhere Produktivität als neue Arbeitnehmer. Um Arbeitnehmer nun davon abzuhalten von sich aus zu kündigen, wird ein höherer Lohn als der Gleichgewichtslohn bezahlt. Da alle Unternehmen so agieren, ist der Marktlohn ein "`Effizienz-Lohn"'. Da dieser höher ist als der marktäumende Gleichgewichtslohn, führt dies zu einem gewissen Niveau an unfreiwilliger Arbeitslosigkeit.
Der Wirkungszusammenhang ist damit ähnlich wie beim "`Shirking-Modell, hier zahlen die Unternehmer aber einen höheren Lohn um freiwillige Abgänge zu verhindern, anstatt, wie beim "`Shirking"', Untätigkeit zu vermeiden.

Außerdem gibt es, viertens, das nicht-rationale, Soziologische- oder Fairness-Modell. Gott-sei-Dank wird sich so mancher Leser denken. Schließlich waren die bisher genannten Modelle strikt am neoklassischen Nutzenmaximierer ausgerichtet, mit der Folge, dass das darin gezeichnete Menschenbild ein nicht gerade sehr positives ist. Stichwort "`Shirking"': Der nutzenmaximierende Arbeitnehmer versucht möglichst wenig zu arbeiten. \textcite{Solow1980} hatte als erster argumentiert, dass die Gründe für Lohn-Rigiditäten vielleicht viel einfacher in "`sozialen Konventionen"' oder "`empathischem Verhalten"' zu finden seinen, als in nutzenmaximierenden Verhalten \parencite[S. 204]{Yellen1984}.  \textcite{Akerlof1982} war der erste, der dies systematisch untersuchte. Er startet damit eine Reihe von Publikationen, die er gemeinsam mit seiner Ehefrau Janet Yellen fortsetzte \parencite{Akerlof1984, AkerlofYellen1987, AkerlofYellen1988, AkerlofYellen1990}. Die Inhalte analysieren psychologische und soziologische Gründe dafür, dass Unternehmen höhere Löhne als den Gleichgewichtslohn bezahlen. Akerlof und Yellen heben Faktoren wie Moral, soziale Verantwortung und faire Löhne. Außerdem kritisieren sie, dass menschliche Arbeitskraft nicht wie nicht-menschliche Inputs modelliert werden sollte, da die oben genannten Faktoren eben dazu führen, dass es signifikante Unterschiede in der Bewertung von Menschen wie Maschinen gibt \parencite[S. 392]{Snowdon2005}. Die Arbeit von \textcite{Akerlof1982} war hierfür die erste, die solche Prozesse formalisierte. Es sollte dabei erwähnt werden, dass es Akerlof und Yellen (und anderen Ökonomen) nicht darum geht menschliches Verhalten nicht formal zu modellieren. Sondern im Gegenteil, ihre Modelle sind ebenso formal-mathematisch aufgebaut. Aber es geht darum grundsätzlich anzuerkennen, dass Arbeitnehmer als Menschen negative Gefühle entwickeln, wenn sie sich unfair behandelt fühlen und in der Folge als Konsequenz - also durchaus rational erklärbar - eine geringere Arbeitsproduktivität zeigen. Arbeitgeber wissen dies und bezahlen deshalb einen "`fairen"' Lohn, eben um die Arbeitnehmer bei Laune zu halten. Als Resultat entwickeln \textcite{AkerlofYellen1990} ihre "`fair wage-effort hypothesis"'. Darin optimieren Arbeitnehmer ihre individuelle Nutzenfunktion, indem sie ihre Arbeitsproduktivität an das Verhältnis zwischen Reallohn und als fair empfundenen Lohn anpassen. Dies macht es für Arbeitgeber sinnvoll einen höheren Lohn als den Gleichgewichtslohn zu bezahlen, weil dann eben die Arbeitsproduktivität höher ist. Das Resultat ist das gleiche wie bei den drei zuvor genannten Ansätzen zum Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität: Die über dem Gleichgewichtslohn liegenden "`Effizienz-Löhne"' führen dazu, dass der Arbeitsmarkt nicht vollständig geräumt wird. Mit anderen Worten: Unfreiwillige Arbeitslosigkeit entsteht. 

Eigentlich handelt es sich beim hier dargestellten Teilkapitel nur um ein kleines Rädchen im  Neu-keynesianischen Rahmenwerk. Dennoch ist der Input von Akerlof und Yellen bei Betrachtung des gesamten Neu-Keynesianismus interessant. Kann er doch als der Versuch gesehen werden, im Neukeynesianismus eine verhaltensökonomische Facette zu integrieren und ihn wieder stärker Richtung Keynesianismus auszurichten. Ihren Artikel \textit{Rational Models of Irrational Behavior} schließen \textcite{AkerlofYellen1987} wie folgt: "`The bad press that Keynesian theory has recently received from maximizing, super-rational theory is simply undeserved."' Und die beiden argumentieren weiter, dass die Annahmen der keynesianischen Theorie mit den Ergebnissen der modernen Psychologie und Soziologe übereinstimmen. Auch bei seiner Nobelpreisrede im Jahr 2001 stellte Akerlof \textcite{Nobelpreis-Komitee2001} das Thema "`Verhaltensorientierung in der Makroökonomie"' in den Vordergrund. An dieser Stelle zeigt sich meines Erachtens auch sehr schön einer der Angriffspunkte auf die moderne Makroökonomie: Wir haben nun fünf verschiedene Ansätze gesehen zu denen jeweils dutzende Journal-Artikel geschrieben wurden, die jeweils mit verschiedenen modell-theoretischen Annahmen und formalen mathematischen Methoden versuchen zu erklären, warum es einen Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe gibt!? Nur einer davon, nämlich jener von Akerlof und Yellen, beruft sich auf Argumente, die man aus menschlichem Eigenschaften -  wie Fairness, Neid oder Rache - ableiten muss. Dieses menschliche Verhalten an sich kann man als solches aber nicht modellieren. Keynes hätte dieses Verhalten als "`animal spirits"' zusammengefasst. Alle anderen genannten Ansätze bemühen sich die Vorgänge als Ergebnis individuell-rationalen Verhaltens zu modellieren. Sie wirken etwas konstruiert und weniger natürlich, passen aber dafür perfekt in das formal-mathematische und streng rationale Konzept der modernen Makro-Ökonomie. Letzteres hat sich schlussendlich als Mainstream-Makroökonomie durchgesetzt. Tatsächlich gibt es aber Vertreter des Neu-Keynesianismus, die zwar in dessen Frühphase wichtige Beiträge zu dessen Entwicklung beigesteuert haben, den Übergang zur "`Neuen neoklassischen Synthese"' aber nicht mitgegangen sind. Zu nennen sind hier vor allem Joseph Stiglitz, George Akerlof, Janet Yellen und Paul Krugman. Auf der anderen Seite gibt es Vertreter des frühen Neukeynesianismus, die diesen - gemeinsam mit jungen Wirtschaftswissenschaftlern - zur "`Neuen neoklassischen Synthese"' weiterentwickelt haben. Hier sind vor allem John Taylor, David Romer und Greg Mankiw zu zählen. Gerade Anfang der 1990er Jahre etablierte sich die "`Neue neoklassische Synthese"' (vgl. Kapitel \ref{Neue Neoklassische Synthese}) und die Ansätze von \textcite{AkerlofYellen1990} gerieten eher in Vergessenheit.

\subsubsection{Unvollkommenheiten am Finanzmarkt}
Die Finanzmärkte gelten gemeinhin als jene Märkte, auf denen das klassische Konzept des Perfekten Marktes noch am ehesten zutrifft. Dennoch gibt es offensichtliche Unvollkommenheiten auf diesen Märkten. Die "`Neu-Keynesianer"' entdeckten dieses Forschungsgebiet in den 1980er-Jahren auf verschiedene Weisen. 

Erstens, als Verstärker von Nachfrage-Schocks auf den Realmärkten. Unternehmen, die nur wenig Eigenkapitalpuffer aufweisen, hängen in großem Ausmaß von den Kredit-Märkten ab. Im Fall eines Nachfrage-Schocks müssen diese Unternehmen also auf den Fremdkapitalmärkten aktiv werden um liquide zu bleiben. Grundsätzlich gesunde Unternehmen, die nur über zu wenig Eigenkapitalausstattung verfügten, können im Fall von unvollkommener Information nicht von "`ungesunden"' Unternehmen unterschieden werden. In der Folge hat der Finanzmarkt keine ausschließlich bereinigende Wirkung, sondern auch "`gesunde"' Unternehmen schlittern in die Insolvenz - der Gesamtoutput fällt weiter, die Krise wird durch unvollkommene Finanzmärkte also verstärkt \parencite[S. 13]{Mankiw1991}.
  
Der zweite Grund ist etwas komplexer: Es geht um den direkten Einfluss der Kreditmärkte auf die Gesamtnachfrage über den Transmissionsmechanismus des Geldes: In der Theorie führt expansive (restriktive) Geldpolitik zu sinkenden (steigenden) Zinssätzen. Diese werden über die Finanzmärkte - konkret über die Finanzintermediäre, also konkret Kredite von Banken Unternehmen und Haushalte - weitergegeben und führen wiederum zu mehr (weniger) Konsumausgaben und weniger (mehr) Sparen und damit zu einem neuen Gleichgewicht des Gesamtoutputs. Was aber passiert wenn es auf Ebene der Finanzintermediäre zu Marktunvollkommenheiten kommt? Hier wurden ebenfalls zwei Forschungsansätze sehr bekannt. 

Erstens, jener von \textcite{Bernanke1988}. Die beiden erstellten ein theoretisches Modell, in dem gezeigt wird, dass Verwerfungen beim Transmissionsmechanismus des Geldes zu großen Auswirkungen bei der Gesamtnachfrage führen können. Zuvor schon untersuchte \textcite{Bernanke1983} entsprechende Zusammenhänge konkret für die "`Great Depression"'. Er erweiterte im wesentlichen die Monetaristischen Arbeiten von \textcite{Friedman1968}, die gezeigt hatten, dass die Great Depression durch eine Unterversorgung mit Geld, verschlimmert wurde. \textcite{Bernanke1983} führte aus, dass es in Finanzkrisen nicht genug ist auf Ebene der Zentralbanken für ausreichend Liquidität zu sorgen, sondern, dass auch auf Ebene der Geschäftsbanken weiterhin für Liquidität gesorgt werden muss. Mit anderen Worten: Der Interbankenmarkt, also das Geschäft zwischen Banken, sowie das Geschäft zwischen Banken und Endkunden, muss während Wirtschaftskrisen aufrechterhalten bleiben. Andernfalls fällt die Gesamtnachfrage und die Verwerfungen auf den Finanzmärkten führen zu einer realwirtschaftlichen Krise. Das ist übrigens Wissen, das uns im Jahr 2007 - im Morgengrauen der "`Great Recession"' - wohl vor einer noch schlimmeren Krise bewahrt hat. Die Zentralbanken - allen voran die US-amerikanische Federal Reserve mit Ben Bernanke an der Spitze - verringerten nicht nur die Leitzinssätze und stellten billiges Zentralbankengeld zur Verfügung, nein, auch die Regierungen sagten zu, eventuell ausfallende Bankinstitute aufzufangen. Dadurch stiegen das Misstrauen und die Interbanken-Zinssätze nur für eine sehr kurze Zeit. Das Vertrauen in das Finanzsystem konnte durch die Zusagen aufrechterhalten bleiben und die Finanzmärkte blieben liquide \parencite[S. 13]{Mankiw1991}.
Das eben genannte Rahmenwerk beschrieb die Bedeutung von Verwerfungen auf den Finanzmärkten bei Wirtschaftskrisen. Das zweite Forschungsgebiet behandelte Unvollkommenheiten beim Transmissionsmechanismus im "`Normalbetrieb"'.  \textcite{Stiglitz1981} analysierten dieses Problem einer "`Realen Rigidität"' im eigentlichen Sinn: Empirisch ließ sich in den 1970er Jahren beobachten, dass es eine Übernachfrage nach Fremdkapital gab. Es wurden also weniger Kredite tatsächlich vergeben, als nachgefragt. Man würde eigentlich erwarten, dass bei einer Übernachfrage nach Kapital die Zinsen (als Preis für Kredite) steigen würden und ein Sinken der Nachfrage und/oder Steigen des Angebots zu einem neuen Gleichgewicht führen würden. Auslöser dieser "`Kredit-Rationierung"'\footnote{Wenn dieses Problem während Wirtschaftskrisen auftritt, spricht man meist von Kredit-Klemme (credit crunch).} ist die Informations-Asymmetrie zwischen Bank als Kreditgeber und Haushalten oder Unternehmen als Kreditnehmer. Da die Banken die Zahlungsfähigkeit ihrer Kunden nicht vollständig beobachten können, leiden sie unter einem typischen "`Adversen Selektions-Problem"', wie schon in Kapitel \ref{cha: Marktversagen} dargestellt. \textcite{Stiglitz1981} zeigen mit einem ganz ähnlichen Ansatz wie in \textcite{Stiglitz1976a}, dass Unternehmen, die zum aktuellen Zinssatz keinen Kredit erhalten, nicht einfach einen anbieten können mehr zu zahlen um den Kredit zu erhalten. In diesem Fall käme der Marktmechanismus in Gange, höhere Preise führen zu weniger Nachfrage und einem neuen Gleichgewicht. \textcite{Stiglitz1981} zeigen aber, dass Banken aber gar keine höheren Zinsen anbieten wollen. Sie können nämlich nur unzureichend zwischen Kreditnehmern mit geringer und hoher Ausfallswahrscheinlichkeit unterscheiden. Würden Sie nun allen Kreditnehmern einen höheren Zinssatz anbieten, würden sie erstens, "`bessere"' Risiken abschrecken einen Kredit aufzunehmen, während "`schlechtere"' Risiken den, für sie, fairen Zinssatz bereit wären zu bezahlen. Dies ist der typische Adverse Selektions Effekt. Und zweitens, würden höhere Zinssätze Kreditnehmer anspornen in risikoreichere Projekte zu investieren. Es gäbe also auch einen negativen Anreiz-Effekt. Aus Sicht einer einzelnen Bank ist es also rational Kredite restriktiv nur an Kunden mit gutem Risikoprofil zu vergeben. Als Ergebnis ist zwar die Anzahl der vergebenen Kredite niedriger als die Anzahl der nachgefragten Kredite und der Zinssatz ist niedriger als der Zinssatz, den die Bank verlangen könnte, aber durch das niedrigere Risikoprofil, das damit erzielt wird, agiert die Bank damit dennoch Gewinn-maximierend. Gesamtwirtschaftlich führt dies aber zu dem Ergebnis eines stabilen Gleichgewichts bei Unterauslastung. Der Markt wird nicht zum Gleichgewichtszins geräumt und die Marktkräfte sorgen auch nicht dafür, dass sich der zu niedrige Zins dem Gleichgewichtszins annähert - der Kreditmarkt unterliegt in einem solchen Fall einer realen Rigidität.


\subsubsection{Unvollkommenheiten am Gütermarkt}
Wir haben nun bereits gesehen, dass es gute Gründe gibt warum reale Rigiditäten auftreten. Wir haben bisher mit dem Arbeitsmarkt, sowie dem Finanzmarkt zwei Teilmärkte betrachtet. Nun analysieren wir den Gütermarkt selbst. Warum kann es sein, dass Preise von Gütern und Dienstleistungen vom Marktpreis langfristig abweichen? Das ist natürlich eine ziemlich starke Annahme und sie benötigt drei Voraussetzungen. Erstens, es darf kein "`vollständiger Konkurrenzmarkt"' vorliegen, sondern ein "`monopolistischer Konkurrenzmarkt"'. Eine typische neu-keynesianische Annahme, der wir uns im nächsten Unterkapitel widmen (siehe Kapitel \ref{Monopol}). Zweitens, es muss zu Koordinierungsfehlern kommen. Das heißt, die Beschaffung von Informationen über Preise von alternativen Produkten und das Ausweichen auf diese Alternativen muss einen Aufwand bedeuten. Auch dieser Punkt ist typisch neu-keynesianisch und wird in Kapitel \ref{Suchtheorie} behandelt. Drittens, es muss nominale Rigiditäten geben. Wie bereits in Kapitel \ref{Nominale Rigiditäten} dargestellt, zeigen \textcite{RomerDavid1990}, dass sich nominale und reale Rigiditäten wechselseitig bedingen. 

Wenn diese Voraussetzungen erfüllt sind, dann kann es zu Abweichungen der tatsächlichen Güterpreise von den eigentlich auf einem Wettbewerbsmarkt zu erwarteten Preisen kommen. Wichtig ist, dass dies auch impliziert, dass die tatsächlichen Güterpreise von den Preisen abweichen können, die man bei einem gegebenen Geldangebot erwarten sollte. Mit anderen Worten: Wenn Güterpreise rigide sind, dann ist es möglich, dass sich diese nicht sofort an Geldmengenänderungen anpassen. Das wiederum bedeutet, dass geldpolitische Maßnahmen zumindest kurzfristig zur Steuerung der Wirtschaft eingesetzt werden können. Reale Rigiditäten sind also auch ein wichtige Voraussetzung dafür, die Wirksamkeit der Geldpolitik mit einem theoretischen Rahmenwerk zu unterlegen. 

Wie werden die realen Rigiditäten am Gütermarkt nun aber realisiert? Dazu wird die Gewinnmaximierung-Theorie aus der Mikroökonomie analysiert. Auf einem perfekten Konkurrenzmarkt ist ein einzelner Anbieter Preisnehmer. Die Angebotsfunktion verläuft parallel zur x-Achse. Die Elastizität des Angebots ist unendlich. Das heißt, ein Unternehmer, der einen Gewinn generieren will, indem er auf die Preise eine Aufschlag - häufig auch im Deutschen "`Mark-up"' genannt, wird vom Markt bestraft. Zu einem höheren Preis als den Marktpreis kann man auf vollkommenen Konkurrenzmärkten nichts verkaufen. Jetzt haben wir aber angenommen, dass (auch) die Gütermärkte im "`Neu-Keynesianismus"' monopolistisch sind. Das heißt die Unternehmer können in engem Rahmen (Details im nächsten Unterkapitel) auf den Marktpreis einen Aufschlag verrechnen, der als zusätzlicher Gewinn realisiert wird. Technisch gesehen maximiert ein Unternehmer, der auf einem vollkommenen Konkurrenzmarkt agiert seinen Gewinn indem er solange Güter produziert, solange die Kosten für ein zusätzlich erzeugtes Gut (Grenzkosten) niedriger oder gleich dem Marktpreis sind. Auf einem monopolistischem Konkurrenzmarkt wird der Gewinn maximiert indem man die Differenz zwischen Gesamterlös und Gesamtkosten maximiert (Grenzerlös = Grenzkosten).  Der Vorteil auf dem monopolistischen Konkurrenzmarkt ist, dass die abgesetzte Menge nicht auf Null fällt, sobald man einen höheren Preis als den Gleichgewichtspreis für seine Güter verlangt. Verlangt man einen etwas höheren Preis als den theoretischen Gleichgewichtspreis, befindet man sich zwar nicht mehr im Gewinnmaximum, aber zumindest in der Nähe davon. Der Preis pro Stück wird auf einem monopolistischen Markt etwas höher sein als auf einem perfekten Konkurrenzmarkt. Diese Differenz kann man eben als Aufschlag oder "`Mark-up"' bezeichnen \parencite[S. 379f]{Snowdon2005} \parencite[S. 14]{Mankiw1991}. 

Genau dieser Aufschlag ist eine "`reale Rigidität"' auf Gütermärkten. Die "`Neuen Klassiker"' sehen nur eine Möglichkeit, wie sich sinkende Grenzkosten der Produktion - zum Beispiel in Folge einer Rezession - auswirken können, nämlich in sinkenden Preisen. Auf perfekten Konkurrenzmärkten ist dies tatsächlich die einzige sinnvolle Lösung. Auf monopolistischen Konkurrenzmärkten aber kann es vorkommen, dass sinkende Grenzkosten zu keinen sinkenden Preisen führen. Wenn nämlich die geringeren Kosten durch im gleichen Ausmaß steigenden "`Mark-ups"' ausgeglichen werden, bleiben die Preise konstant. Dass genau das auf Gütermärkten passiert, argumentieren die Neu-Keynesianer und sprechen dann von realen Rigiditäten auf Gütermärkten. Die "`Mark-ups"' müssten in so einem Fall aber antizyklisch verlaufen. Also während einer Rezession steigen und während einer Boom-Phase fallen. Warum das passieren soll war Gegenstand der Forschung in diesem Bereich.

\textcite{Stiglitz1984} identifiziert eine technische Möglichkeit, wie es sein kann, dass die Güterpreise unverändert bleiben, wenn die Grenzkosten (der Produktion) fallen. Wenn nämlich die Elastizität der Nachfrage abnimmt, könnten Unternehmen rational entscheiden ihre Mark-ups zu erhöhen. Die Preise als Summe der Kosten und Mark-ups könnten so auch während einer Rezession konstant bleiben oder sogar steigen \parencite[S. 351]{Stiglitz1984}. Eine technisch elegante Lösung, die die Frage aber nur einfach nur auf eine andere Ebene verschiebt: Warum sollte die Elastizität der Nachfrage in Rezessionen sinken?
\textcite{Stiglitz1984} liefert in weiterer Folge verschiedene Ansätze warum Preise in einer Rezession steigen könnten. Zwei davon bedienen sich der Argumentation, dass unvollkommene Information dazu führen könnte, dass es für Unternehmen in einem Wettbewerbsmarkt vorteilhafter sein könnte die Güterpreise konstant zu belassen oder sogar anzuheben. Erstens, weil man mit höheren Preisen höhere Qualität verbindet, zweitens, weil Kunden sich nicht auf die kostspielige Suche nach alternativen Anbietern machen wollen. Unternehmen würden dann von Preisanpassungen nach unten an das Gleichgewicht nicht profitieren, weil die Kunden ohnehin auch ei höheren Preisen kaufen. Beide Ansätze wirken nicht sehr überzeugend.
\textcite{Stiglitz1984} liefert aber noch einen interessanten Ansatz, der in weiterer Folge einen eigenen Forschungszweig begründen sollte. Er betrachtet die Gütermärkte als oligopolistische Märkte und das Verhalten der Unternehmen analysiert er mit Methoden der Spieltheorie. Der erste dieser Ansätze zur Erklärung steigender Güterpreise während einer Rezession geht davon aus, dass Oligopolisten (oder Monopolisten) zu Beginn einer Rezession natürlich weniger Güter produzieren müssen. Dadurch werden Überkapazitäten in der Produktion frei. Die Überkapazitäten sind ein Abschreckungsmittel gegenüber potentiellen neuen Mitbewerbern. Steigende Überkapazitäten würden folglich steigende Preise erlauben. Der zweite Ansatz geht davon aus, dass Oligopol-Unternehmen explizite oder implizite Preisabsprachen mit ihren Konkurrenten machen. Das heißt, jeder Oligopolist bezieht aufgrund der Vereinbarung einen entsprechend höheren Gewinn und beobachtet gleichzeitig den Markt. Am Markt können aber nur die Preisabweichungen der Konkurrenten, nicht die Nachfrageänderungen beobachtet werden. Preissenkungen können nun zwei Gründe haben, entweder man will aus dem Kartell ausscheren um sich selbst dadurch einen "`Übergewinn"' sichern (gleichzeitig aber damit das bestehen des Kartells zu gefährden), oder man reagiert einfach auf eine sinkende Nachfrage. Da Unternehmer nicht zwischen diesen beiden Gründen unterscheiden können, kann es aus rational sein, selbst dann beim höheren Preis zu bleiben um die rentable Kartellabsprache nicht zu gefährden. Diese spieltheoretischen Ansätze fanden ab den 1980er Jahren immer stärker Eingang auch in die Makroökonomie. 
Insbesondere \textcite{Rotemberg1986} erweiterte die spieltheoretischen Ansätze zur Erklärung realer Rigiditäten. Sie verwendeten hierbei das spieltheoretische Modell, das \textcite{Friedman1971} für mikroökonomische Fragestellungen entwickelten. Demnach kommt es auf Oligopol-Märkten zu impliziten Preisabsprachen, also Preissetzungen über dem Gleichgewichtspreis, obwohl die Konkurrenten sich darüber nicht austauschen, sondern stattdessen nur beobachten. \textcite{Rotemberg1986} wendeten dieses Modell auf das makroökonomische Problem der Realen Rigiditäten an. Die beiden zeigten theoretisch fundiert, dass auf Oligopol-Märkten der Wettbewerb zwischen den Unternehmen in Hochkonjunktur-Zeiten stärker ausgeprägt ist. Für Oligopolisten zahlt es sich nämlich vor allem dann aus die impliziten Preisabsprachen zu brechen, wenn die Nachfrage hoch ist. Dann kann ein einzelner Unternehmer durch Unterbieten des Oligopol-Preises einen hohen individuellen Gewinn erzielen. Freilich zu dem Preis, dass das System impliziter Absprachen zusammenbricht und die konkurrierenden Unternehmen zusammen weniger Gewinn einfahren. Preiskriege finden also vor allem in Hochkonjunktur-Phasen und auf Märkten mit wenigen Anbietern statt \parencite[S. 391]{Rotemberg1986}. Preise auf solchen Märkten sind also - entgegen den gängigen Annahmen - antizyklisch. Im Resultat ergibt sich daraus die Erklärung für die nicht fallenden Preise während Rezessionen. In diesen Phasen sind die Mark-ups nämlich gering und ein Ausscheren aus dem Preiskartell würde keine lohnenden Gewinne bringen. In Rezessionen bleiben die Preise dementsprechend verhältnismäßig hoch. Dem noch nicht genug zeigen \textcite{Rotemberg1986}, dass Preiskriege auf Oligopol-Märkten durch Verflechtungen mit Märkten mit höherem Wettbewerb, insgesamt zu antizyklischen Preisentwicklungen führen.

Das prozyklische Verhalten der Mark-ups ist ein bis heute diskutiertes und beforschtes Thema. So setzen moderne Neu-Keynesianische DSGE-Modelle, sowie noch modernere Heterogeneous-Agent New Keynesian (HANK) Modelle (vgl. Kapitel \ref{Neue Neoklassische Synthese}) die Existenz der Prozyklizität der Mark-ups voraus. Während ältere Modelle den Konjunkturzyklus vor allem mit Reallohn-Rigiditäten erklären, führen die genannten neuesten Modelle (HANK) die beobachteten Nachfrageschocks auf Preis Rigiditäten zurück \parencite[S. 3]{Nekarda2020}. Dabei ist die empirische Bestimmung von Mark-ups ein bis heute nicht endgültig gelöstes Forschungsproblem. Beginnend mit \textcite{Bils1987} gibt es bis heute \parencite[S. 4f]{Nekarda2020} verschiedene Ansätze, wie man die Zyklizität der Mark-ups misst. Leider gibt es dabei auch unterschiedliche empirische Ergebnisse. Ein Beweis, dass das Thema der Rigiditäten auf Gütermärkten ein bis heute in der Makroökonomie aktuelles und umstrittenes ist. 


\subsection{Monopolistische Konkurrenz}
\label{Monopol}
Die Neuen Klassiker etablierten die Mikrofundierung der Makroökonomie, auf die die Neu-Keynesianer aufbauen. So konnte das grundsätzlich mikroökonomische Thema der Marktformen in der Makroökonomie berücksichtigt werden. Die Annahme, dass auf den Märkten generell "`Vollständige Konkurrenz"' ("`Perfekter Wettbewerb"') herrscht, ist grundsätzlich so alt wie die Wirtschaftswissenschaft selbst. Implizit ging bereits Adam Smith davon aus, dass sich auf Märkten eine große Zahl von Anbietern und Nachfragern treffen und einen Gleichgewichtspreis finden. Explizit ausgesprochen und analysiert wurde dies von Leon Walras und seinem Konzept des "`Allgemeinen Gleichgewichts"'. Dieses spielt ja bis heute - vor allem bei den Neuen Klassikern - als "`Walrasianischer Auktionator"' eine große Rolle. In der Mikroökonomik analysierten Gerard Debreu und Kenneth Arrow in den 1950er Jahren das "`Allgemeine Gleichgewicht"' unter Einbeziehung der Finanzmärkte mit modernen mathematischen Methoden. 
Natürlich wusste man aber, dass selbst Märkte, auf denen es sowohl viele Anbieter als auch viele Nachfrager gibt, in vielen Fällen keine perfekten Konkurrenzmärkte sind. Auf perfekten Konkurrenzmärkten herrscht unendliche Preiselastizität. Das heißt die kleinste Abweichung vom Gleichgewichtspreis durch einen Anbieter führt dazu, dass dieser Anbieter schlagartig kein einziges Gut mehr verkauft. Das ist mit unserer tagtäglichen Erfahrung nicht vereinbar. Man versetze sich dazu nur in folgendes Beispiel: Sie gehen in einen Supermarkt und wollen dort Güter des täglichen Bedarfs kaufen. Auf einem vollständigen Konkurrenzmarkt müssten Sie zu jeder Zeit den Gleichgewichtspreis von jedem Gut erheben. Das wäre zeitaufwändig und damit auch teuer. Stattdessen akzeptieren Sie mit dem Besuch im Supermarkt implizit, dass der Verkäufer einen Preis festsetzt. Auch wenn Ihnen manchmal vielleicht bewusst ist, dass Sie eine gute Verhandlungsbasis für einen niedrigeren Preis hätten (z.B.: Das gleiche Gut kostet bei einem Konkurrenten weniger). Der Supermarkt wiederum ist sich seiner Position ebenso bewusst. Er weiß, dass er in einem gewissen Rahmen die Preise wählen kann, ohne dass die Kunden sich sofort abwenden und für ein bestimmtes Produkt in den Konkurrenz-Supermarkt wechseln nur um ein paar Cent zu sparen. Wichtig ist hier die Betonung auf "`in einem gewissen Rahmen"'. Der Name "`Monopolistische Konkurrenz"' ist nämlich etwas irreführend. Die Anbieter auf einem derartigen Markt haben nämlich \textit{keine} Monopolstellung! Im Gegenteil, es handelt sich in der Regel um Märkte mit ausgeprägter Konkurrenz. Allerdings sind die einzelnen Anbieter eben "`Preissetzer"' und nicht "`Preisnehmer"'. Das heißt, der Preis der Produkte wird vom Verkäufer festgesetzt und nicht vom Markt in dem Sinne, dass Abweichungen vom Gleichgewichtspreis sofort zu einem totalen Rückgang der abgesetzten Menge führen. Die Miteinbeziehung von "`imperfektem Wettbewerb\footnote{"Imperfekter Wettbewerb"' und "`Monopolistische Konkurrenz"' wird in der VWL häufig gleichgesetzt. So auch hier. In Wirklichkeit aber umfasst "`Imperfekter Wettbewerb"' mehr, nämlich jegliche Abweichung von perfekten Konkurrenzmärkten, also auch Oligopole oder Monopole}"' berücksichtigt ein Phänomen innerhalb der Volkswirtschaftslehre, das man in der Betriebswirtschaftslehre schon lange kennt: Im Marketing spricht man von "`horizontaler Differenzierung"', das Unternehmen erlaubt unterschiedliche Preise zu verlangen, die auf den ersten Blick exakt die gleichen Bedürfnisse befriedigen. Denken Sie nur an die Preisunterschiede am Automarkt. aus volkswirtschaftlicher Sicht spielt das Thema der Marktformen in der Mikroökonomie eine entscheidende Rolle. Da die Makroökonomie seit der Revolution durch die Neuen Klassiker "`mikrofundiert"' ist, spielen Marktformen auch in der Makroökonomie eine Rolle.  
In der Mainstream-Makroökonomie hatte man aber lange Zeit ein Problem damit, vom Konzept des "`Walrasianischen Auktionators"' abzugehen. Bei Keynesianers und Monetaristen spielten Überlegungen zur Marktform, mangels Mikrofundierung ihrer Modelle, keine Rolle. Aber auch die Neuen Klassiker griffen - übrigens vehement bis heute -  ausschließlich auf das Konzept der perfekten Konkurrenzmärkte zurück. Die Ursprünge der "`Monopolistischen Konkurrenz"' liegen dennoch schon recht weit zurück: Fast zeitgleich veröffentlichten \textcite{Chamberlin1933} und \textcite{Robinson1933} ihre Werke "`The Theory of Monopolistic Competition"' und "`The Economics of Imperfect Competition"'. Die Motivation hinter diesen beiden Werken ist grundverschieden von jener im modernen Neu-Keynesianismus. Beide Arbeiten zielen schließlich rein auf mikroökonomische Überlegungen ab. Aber die grundlegende Idee ist identisch: Nämlich, dass auf einem Markt mit vielen Anbietern und Nachfragern, ein einzelner Anbieter seinen Verkaufspreis in engen Grenzen wie ein Monopolist festsetzen kann. Joan Robinson wurde später zu einer zentralen Figur des "`Post-Keynesianismus"'. In dieser Schule, die eben nicht zum Mainstream gehört,  ist die Berücksichtigung "`Imperfekten Wettbewerbs"' eine zentrale Annahme. Nämlich in der Form eines sogenannten "`Mark-up"'. Mehr dazu im Kapitel \ref{Post-Keynes}. Entsprechend den riesigen inhaltlichen Differenzen zwischen "`Neu-Keynesianern"' und "`Post-Keynesianern"' ist es wenig überraschend, dass erstere es ablehnen, dass das Konzept der "`Monopolistischen Konkurrenz"' von den "`Post Keynesianern"' übernommen wurde. "`Post-Keynesianer haben ein breites Spektrum an Modellen mit Imperfektem Wettbewerb, aber im Detail sind sie nicht sehr ähnlich zu den Neu-Keynesianischen Modellen"' \textcite[S. 439]{Snowdon2005}, meinte Greg Mankiw in einem Interview direkt darauf angesprochen, ob nicht die Post-Keynesianer in diesem Bereich Vorreiter waren? Abgesehen von seiner Aussage, muss man aber doch bemerken, dass das Element der "`Monopolistischen Konkurrenz"' jenes innerhalb der heutigen Mainstream-Ökonomie ist, das am weitesten vom Dogma der "`Marktgläubigkeit"' abweicht. 

Die Idee "`Imperfekten Wettbewerb"' in makroökonomischen Modellen umzusetzen ist \textit{das} Alleinstellungsmerkmal der Neu-Keynesianer schlechthin. Wie bereits erwähnt ist dieses Element nämlich sowohl den Keynesianern, als auch den Monetaristen und den Neuen Klassikern fremd. 
Die Existenz von Monopolistischen Märkten steht in engen Zusammenhang mit den im letzten Kapitel behandelten Rigiditäten. Tatsächlich machen Überlegungen zu Rigiditäten nur dann Sinn, wenn man "`Imperfekten Wettbewerb"' berücksichtigt, andernfalls wären die anbietenden Unternehmen nämlich Preisnehmer und Überlegungen zu Rigiditäten würde jegliche Grundlage fehlen!

Was ist die Motivation der Neu-Keynesianer "`Imperfekten Wettbewerb"' in Makro-Modellen zu berücksichtigen? Was ändert sich dadurch in den Modellen? Und was sind die Auswirkungen auf die Ergebnisse der Modelle? Nun die Motivation war wohl primär empirisch gegeben. Der Neu-Keynesianismus wurde ja praktisch aus der Idee heraus geboren, dass Marktunvollkommenheiten auf Märkten eine wichtige Rolle spielen. Eine Tatsache, die relativ unumstritten ist, aber von den Neu-Klassikern nicht berücksichtigt wurde. Greifen wir das kurz angesprochene Beispiel des Automarktes auf. Ökonomisch ausgedrückt besteht der Zweck eines Automobiles jemand von A nach B zu bringen. Das ist aber nicht mit den empirisch leicht zu beobachtenden Preisunterschieden am Automarkt zu vereinbaren\footnote{Man könnte einwenden der Zweck eines Autos besteht eben nicht nur im Transport von A nach B, sondern auch darin Status zu vermitteln, Sport zu betreiben, etc. Allerdings gibt es Preisdifferenzierungen auch bei fast allen anderen Produkten, die nicht als Statussymbol etc. gesehen werden}. Die Annahme, dass zumeist homogene Güter auf perfekten Konkurrenzmärkten gehandelt werden, erscheint also einfach nicht der Realität zu entsprechen.
Auf Modelle hat die Berücksichtigung entscheidende Auswirkungen. Aus technischer Sicht ändert sich die Nachfragefunktion dahingehen, dass diese nicht mehr perfekt elastisch ist, wie auf "`Perfekten Konkurrenzmärkten"'. Das heißt, Änderungen im Preis einer einzelnen Firma führen zwar dazu, dass die Firma weniger Produkte verkauft, aber nicht mehr schlagartig gar keine Produkte mehr. Die Firmen können daher ihren Gewinn individuell maximieren (Grenzertrag = Grenzkosten). Die Firmen können ihren Preis - wenn auch in einem gewissen Rahmen - frei wählen, sie sind Preissetzer. Das heißt sie müssen nicht sofort auf Änderungen des Marktpreises reagieren um überhaupt noch Produkte zu verkaufen\footnote{Dies ermöglicht erst, dass Preise in irgendeiner Form rigide sind, ist also Voraussetzung für die Aspekte, die wir als "`Rigiditäten"' oder "`Menu Costs"' kennengelernt haben.}. Märkte mit "`perfekter Konkurrenz"' sind somit effizienter als Märkte mit monopolistischer Konkurrenz: Die Gewinn-optimale Output-Menge ist niedriger als auf "`Perfekten Konkurrenzmärkten"'. Die Preise sind etwas höher, womit positive Gewinne möglich sind, was wiederum dazu führt, dass auch nicht-effiziente Firmen überleben können. Diese Aspekte sind rein mikroökonomischer Natur. \textcite{Hart1982} analysierte theoretisch, die Auswirkung der Berücksichtigung "`monopolistischer Konkurrenz"' auf die Ergebnisse makroökonomischer Modelle.

Die Anfänge "`Monopolistische Konkurrenz"' in makroökonomischen Mainstream-Modellen zu berücksichtigten waren schwierig. Ein erstes, rein technisches Modell, \textit{wie} monopolistische Konkurrenz berücksichtigt werden kann, lieferten \textcite{Dixit1977}. Darin zu finden ist ein bis heute gängiges Instrumentarium, noch ohne Bezug zu makroökonomischen Modellen. Erst in den 1980er Jahre wurden deren Modell langsam wiederentdeckt und in makroökonomischen Modellen angewendet. In der Realität war es ja schon seit jeher unumstritten, dass Preise vom Verkäufer festgesetzt werden. Also hat man in frühen neu-keynesianischen Makro-Modellen \parencite[S. 97]{Hart1982} zunächst ebenfalls Preise festgesetzt, mit der Konsequenz, dass die Nachfrage und das Angebot entsprechend schwanken sollten. Das war aber nicht in Einklang zu bringen mit der Annahme rationalen Verhaltens. Aus welchen Gründen sollte zum Beispiel ein Anbieter seine Preise nicht anpassen, wenn er mehr verkaufen möchte? Oliver Hart\footnote{Dies ist tatsächlich jener Oliver Hart, der 2016 den Wirtschafts-Nobelpreis für ein ganz anderes Thema, nämlich die Vertragstheorie, erhalten hat.} war der erste, der im Jahr 1982 dieses Problem umging, indem er eben die Annahme des perfekten Wettbewerbs und die damit verbundenen vollkommenen Elastizitäten, fallen ließ \parencite[S. 110]{Hart1982}. In seinem Modell führen Änderungen der aggregierten Nachfrage zu Änderungen im Gesamt-Output, die nicht sofort durch Änderungen im Preisniveau- und Zinsänderungen sofort wieder ausgeglichen werden. 
Geldpolitik ist damit - entgegen den Annahmen der "`Neuen Klassikern"' - ein wirksames Mittel der wirtschaftspolitischen Steuerung. Ein bahnbrechendes Ergebnis: In Neu-klassischen Modellen mit perfektem Wettbewerb ist es nämlich genau umgekehrt: Änderungen der aggregierten Nachfrage führen zu Preisniveau- und Zinsänderungen, die wiederum dafür sorgen, dass der Output gleich bleibt und sofort nur Preisniveau-Änderungen resultieren. 
Das Ergebnis seines mathematisch-theoretischen Modells war erst der Anfang einer langen Reihen von theoretischen und später auch empirischen Arbeiten im Bereich des "`Imperfekten Wettbewerbs"'. Es zeigte außerdem, dass die Ökonomie bei Unterauslastung zu einem Gleichgewicht finden kann. Eine Erkenntnis, die der "`Neuen Klassik"' ebenso klar widerspricht.  Damit ist Arbeitslosigkeit weder notwendigerweise ausschließlich freiwillig, wie bei den Neuen Klassikern, aber auch nicht ein vorübergehendes Phänomen, wie bei Keynes. Hart zeigt im Paper außerdem, dass budget-neutrale Fiskalpolitik (also die Stimulierung der Wirtschaft durch staatliche Ausgaben, die aber durch Steuern gegenfinanziert wird und somit nicht zu Budgetdefiziten führt) wirksam sein kann. Außerdem schließt er seinen Artikel damit, dass durchaus Keynesianische, aber auch Neoklassische und eben auch Post-Keynesianische Elemente darin vorkommen. 
Die Arbeit von \textcite{Hart1982} lieferte wichtige \textit{theoretische} Ergebnisse. Denken wir noch einmal zurück: Was war die ursprüngliche Motivation der "`Neu-Keynesianer"'? Sie fanden sich in einer Welt, in der die "`Neuen Klassiker"' ein theoretisch überlegenes Modell lieferten. Mit rationalen Erwartungen, Mikrofundierung und mathematisch fundierten Ergebnissen. Alleine die Modellannahmen, insbesondere, dass es auch allen Märkten stets zu effizienten Gleichgewichten käme und in der Folge Geldpolitik und Fiskalpolitik keinerlei Wirksamkeit hätten und die Wirtschaft auf einem stabilen langfristigen Wachstumskurs verläuft, waren unrealistisch. Hier sprangen die Neu-Keynesianer auf den Zug auf: Die Wirklichkeit zeigte deutlich, dass Schwankungen der aggregierten Nachfrage zu Änderungen beim Gesamtoutput führten. Also zu Konjunkturschwankungen, die im ausgeprägten Fall Wirtschaftskrisen darstellten. Die Neu-Keynesianer wollten zwar die Mikrofundierung, die rationalen Erwartungen und die eleganten mathematischen Modelle übernehmen, hielten aber die Modellannahmen für unrealistisch. Und hier kommen nun ihre punktuellen Lösungen ins Spiel: Der soeben vorgestellte Artikel von Hart zeigte, dass es Gleichgewichte ohne "`Perfekten Wettbewerb"' geben konnte - quasi aus rein mathematischer Sicht. Aber sind diese Ergebnisse in der Realität auch wirklich relevant? Dies analysierten \textcite[S. 647]{Blanchard1987}: \textit{"'Monopolistic competition provides a convenient conceptual framework in which to think about price decisions, and appears to describe many markets more accurately than perfect competition. But, how important is monopolistic competition for macroeconomics?"'}. Konkret stellen sich zwei Fragen: Erstens, Kann die Berücksichtigung von "`Monopolistischer Konkurrenz"' tatsächlich erklären warum Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Diese Frage hatte \textcite{Hart1982} eben nur auf theoretischer Ebene behandelt. \textcite{Blanchard1987} analysierten ob der Effekt groß genug sei um empirisch eine Rolle zu spielen. Und sie kamen zu dem ernüchternden Ergebnis, dass dies nicht der Fall sei. Aber \textcite{Blanchard1987} erweiterten ihr Modell und untersuchten, zweitens, ob Interaktionen zwischen mehreren Marktunvollkommenheiten - konkret das auftreten "`Monopolistischer Konkurrenz"' und gleichzeitig das Auftreten von "`Rigiditäten"' -  dazu führen konnte, dass Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Dies konnten die beiden mittels Modell bejahen. Obwohl die beiden Autoren am Schluss ihrer Arbeit \parencite[S. 663]{Blanchard1987} die Limitationen ihrer Arbeit anführen, gehen sie richtigerweise davon aus, dass ihr Beitrag im Speziellen und "`Monopolistische Konkurrenz"' im Allgemeinen hilft, die empirisch beobachtbaren Konjunkturschwankungen zu erklären. Die im Paper vollzogene gemeinsame Analyse von "`Menu Costs"' und "`Monopolistischer Konkurrenz"' zeigt auch, dass makroökonomische Realitäten schwer zu modellieren sind. Es gibt nicht den einen Auslöser von Konjunkturschwankungen. Erst die Kombinationen von mehreren Marktunvollkommenheiten führen zu empirisch ansprechenden Ergebnissen. Manche dieser Marktunvollkommenheiten treten zudem nur wechselseitig auf, bedingen sich also gegenseitig. So wie wir gesehen haben, dass Rigiditäten nur dann sinnvollerweise auftreten können, wenn die Voraussetzungen eines "`perfekten Konkurrenzmarktes"' nicht erfüllt sind. Heutige Mainstream-Makro-Modelle berücksichtigen in der Folge üblicherweise sowohl "`Rigiditäten"' als auch "`Monopolistische Konkurrenzmärkte"'.





\section{Koordinationsversagen}
\label{Suchtheorie}
Schon in einer der ersten als Neukeynesianisch zu bezeichnenden Arbeiten wird - in überraschend detaillierter Weise - darauf eingegangen, dass sich auf Märkten in der Realität selten ein perfektes Gleichgewicht einstellt \parencite[S. 683]{Phelps1968} und zwar unabhängig von all den genannten Gründen wie Rigiditäten und Abweichungen vom perfekten Wettbewerbsmarkt. Der Grund ist mit etwas Hausverstand leichter zu verstehen als theoretisch zu modellieren: Die neoklassische Theorie geht davon aus, dass bei perfektem Wettbewerb der Markt durch einen "`Walrasianischen Auktionator"' vollständig geräumt wird und sich in der Folge im Gleichgewicht befindet. Jeder Nachfrager, der mit dem entsprechenden Gleichgewichtspreis einverstanden ist, findet einen Anbieter. Der Hausverstand sagt uns allen: Das klingt schön und gut, aber die Realität funktioniert nicht so perfekt. Anbieter und Nachfrage finden aus verschiedensten Gründen nicht zusammen. Die Koordination zwischen Angebot und Nachfrage ist in der Realität ein komplizierter Prozess. Denken wir an den Arbeitsmarkt: Da gibt es ständig gleichzeitig offene Stellen und Arbeitssuchende die vermeintlich perfekt "`zusammenpassen"' aber sich gegenseitig nicht finden - man spricht von "`Koordinationsversagen"' ("`Coordination Failure"').
Grundsätzlich stellt dies auch eine Reale Rigidität dar \parencite[S. 11]{RomerDavid1993} und könnte daher auch teil des letzten Unterkapitel sein. Allerdings geht die Theorie des Koordinationsversagens über die übrigen Ansätze bei Realen Rigiditäten hinaus.
Die Arbeiten zur Koordinationsversagen wurden vor allem um die frühen 1980er Jahre verfasst. Allgemein werden die Theorien oft als "`Suchtheorien"' bezeichnet. Ökonomisch etwas genauer als "`Theorien der Friktion auf Suchmärkten"'. Diese sind häufig mikroökonomisch motiviert. In der Makroökonomie spricht man meist von der "`Matching Theory"'. Obwohl die Theorien auf Fragestellungen verschiedenster ökonomischer Konzepte anwendbar sind, verbindet man meist vor allem eine Arbeitsmarkttheorie damit.
Bekannt geworden ist in diesem Zusammenhang vor allem das "`Diamond-Coconut"'-Modell. Hierbei gibt es auf einer Insel ausschließlich auf Palmen wachsende Kokosnüsse als Nahrungsquelle und zusätzlich die Vorgabe, dass niemand eine Kokosnuss essen darf, die er selbst geerntet hat, es ist also Handel notwendig, wenn man was essen will. Das ernten einer Kokosnuss erfordert das mühsame Erklettern einer entsprechenden Palme. Damit werden die "`Suchkosten"' dargestellt. Bevor man Handel tätigen kann, muss man Kosten und Mühen (ökonomisch: irgendeine Form von Nutzenverlust) auf sich nehmen. Die Koordination zwischen den beiden Marktseiten verläuft nicht ohne Kosten und Reibung. Dies sorgt dafür, dass es zu keinem eindeutigen Marktgleichgewicht kommt, stattdessen entstehen mehrere verschiedene Gleichgewichte, die aber allesamt nicht perfekt effizient sind. Wissenschaftlich wurde dies von \textcite{Diamond1982} formuliert. Ob zwei Marktteilnehmer zueinander finden wird hierbei durch einen Zufallsprozess bestimmt, der durch eine Poisson-Verteilung abgebildet wird. Diese Art der Modellierung wurde von \textcite{Mortensen1978} erstmals angewendet und erwies sich als ein wichtiger Baustein in der "`Matching Theory"'. 

\textcite{Pissarides1985}
\textcite{Mortensen_Pissarides1994}

HIER WEITER

Arbeitslosigkeit als Suchproblem



Nobelpreisrede Diamond zur Verbindung zu Mortensen (Power of Poisson-distribution)



Microeconomic foundations for wage and price setting
Phelps (1968a, 1970b) derived aggregate wage-setting behavior from detailed modeling of the 
behavior of individual agents. Jobs and workers are heterogeneous and there is imperfect 
information on both sides of the market. Markets are assumed to be almost atomistic, but there is 
no “Walrasian auctioneer” that instantaneously finds the wages (and prices) that clear all markets 
(as went the metaphor used in earlier general equilibrium analysis). Instead, wages are set by 
firms that are able to exercise temporary monopsony power. Workers and firms meet randomly 
at a rate determined by the number of unemployed workers searching for a job and the number of 
vacancies, according to a function that would today be recognized as a matching function. 
Phelps’s work here is a precursor of the search and matching theory of unemployment, where 
Peter Diamond, Dale Mortensen, and Christopher Pissarides have made especially important 
contributions. \parencite{Nobelpreis-Komitee2006}  See, for example, Diamond (1984), Mortensen (1982a, b), Mortensen and Pissarides (1994), and Pissarides 
(2000).


\subsection{Diamond, Mortensen und Pisaridis}

Widersprach rasch der "`Neuen Klassik"' indem sie die natürliche Arbeitslosigkeit erweiterte.



\section{Der Neu-Keynesianische Konjunkturzyklus}

Nach:
\textcite{Yellen1984}
\textcite[S. 396]{Snowdon2005}



\section{Wirkung und Bedeutung des Neu-Keynesianismus}
Eventuell "`New Keynesian Business Cycle Theorie"' Seite 396 in Snowdon/Vane

Interessant, dass Fiskalpolitik praktisch keine Rolle spielt. Aussage dazu von Mankiw in: \parencite[S. 446]{Snowdon2005} Zwar Akzeptiert man, dass Staatsausgaben kurzfristig prozyklisch wirken \parencite[S. 408]{Snowdon2005}, allerdings verursachen sie langfristig Kosten und sind dann schädlich für Wachstum. Geldpolitik hingegen verursacht nur Kosten, wenn daraus Inflation entsteht. Daraus leitete sich in weiterer Folge \parencite[S. 413]{Snowdon2005} die Politik der Inflationssteuerung her, die heute durch praktisch alle führenden Zentralbanken praktiziert wird. (Allerdings auch durch Monetaristen und Neue Klassiker)

Gewachsen: Anfang Nominale Rigiditäten --> Monopolisitsiche Konkurrenz (Imperfect Competition) --> Menu Costs --> Real Rigidtäten --> Fehlende Klassische Dichotomie --> Nicht-Neutralität des Geldes.

Dazu Speziall-Forschungsgebiete: 

1. Reale Rigiditäten: Arbeitsmarkt, Kreditmarkt, Gütermarkt
2. Monopolistische Konkurrenz: Coordination Failures (Überbegriff)
