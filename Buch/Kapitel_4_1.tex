%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neu-Keynesianismus} \label{cha: Neu Keynes}

Der "`Neu-Keynesianismus"' weniger eindeutig von anderen Schulen abzugrenzen als etwa der Keynesianismus oder der Monetarismus. Dies gilt sowohl in inhaltlicher Sicht, also auch in zeitlicher Sicht. Inhaltlich lässt sich der Neu-Keynesianismus am ehesten negativ abgrenzen. Einige Ökonomen erkannten, dass die Theorien der Keynesianer nicht mehr zeitgemäß waren. Sie akzeptierten aber auch nicht die starren Annahmen der "`Neuen Klassiker"'. Diese Ökonomen könnte man "`Neu-Keynesianer"' nennen. Die "`Neu-Keynesianer"' sind dementsprechend als keine geschlossene Gruppierung von Ökonomen entstanden, sondern behandelten eher zerstreut einzelne Fragen der Ökonomie. Dazu gehörten zum Beispiel Fragen der Inflation (Phillips-Kurve), der Arbeitslosigkeit (Suchproblem, Natürliche Arbeitslosigkeit) und des Marktversagens (Informationsasymmetrien, Natürliche Monopole). 
Auch was die Personen betrifft ist die Abgrenzung schwierig. So ist der Erzliberale \textsc{John Taylor} - Präsidenten der Mont Pelerin Society von 2018 - 2020 - inhaltlich zweifelsohne als ein früher Vertreter des Neu-Keynesianismus anzusehen. Ebenso aber steuerte auch Joseph Stiglitz - scharfer Kritiker von liberaler Wirtschaftspolitik - wichtige Beiträge zum "`Neu-Keynesianismus"' bei.

Eine interessante Einordnung des "`Neu-Keynesianismus"' nimmt \textcite[S. 21]{RomerDavid1993} vor. Er beschränkt sich hierbei auf zwei Fragen um die verschiedenen Schulen voneinander abzugrenzen. Erstens, \textit{Sind die (meisten) Märkte im wesentlichen perfekte Konkurrenzmärkte, auf denen es zu einer Markträumung im Sinne des Walrasianischen Gleichgewichts kommt?} und zweitens, \textit{Hat die "`klassische Dichotomie"', also das Zusammenhang zwischen Realwirtschaft und Finanzwirtschaft im Sinne der Quantitätsgleichung des Geldes, Gültigkeit?}.

Die "`Neu-Klassiker"' würden beide Fragen mit Ja beantworten. Die "`Neu-Keynesianer"' hingegen mit Nein. Keynesianer würden die erste Frage bejahen, die zweite allerdings verneinen. Während die Monetaristen ebenso die erste Frage bejahen würden, bei der zweiten aber wohl unstimmig wären.

Insgesamt umfasst der "`Neu-Keynesianismus"' vor allem folgende Eigenschaften, die ihn gegen andere Schulen abgrenzen. Die Neu-Keynesianer akzeptieren die Vorteile der Mikrofundierung der Makroökonomie, die sie als Fortschritt gegenüber den rein makroökonomische fundierten Modellannahmen der Keynesianer und Monetaristen sehen. Ähnlich ist es im Hinblick auf Rationale Erwartungen. Auch diese akzeptieren die Neu-Keynesianer weitgehend. In diesen beiden Punkten grenzen sie sich vom Keynesianismus und Monetarismus ab. Die Neu-Keynesianer gehen aber auch von der Existenz von Rigiditäten auf verschiedenen Märkten aus. Was dazu führt - wie wir später sehen - ,dass aktiver Wirtschaftspolitik eine stabilisierende Wirkung zukommen kann. Dies steht in Verbindung mit einer weiteren Annahme der "`Neu-Keynesianer"', nämlich, dass die meisten Märkte imperfekte Märkte sind auf denen sich die Preise wie untere monopolistischer Konkurrenz bilden. Diese beiden Punkte sind wiederum die Abgrenzung gegenüber der "`Neuen klassischen Makroökonomie"'. 

Die schwierigere Abgrenzung erfolgt aber in zeitlicher Hinsicht. Schließlich wird die heutige Mainstream-Ökonomie häufig als "`Neu-Keynesianismus"' bezeichnet. In dieser Logik müsste man den "`Neu-Keynesianismus"' zumindest in zwei Generationen teilen. Zweifelsohne beginnt der "`Neu-Keynesianismus"' nämlich als Antwort auf die "`Neue Klassische Makroökonomie"' ab den frühen 1980er Jahren zu existieren. In Wahrheit sogar schon deutlich früher, nämlich mit der Kritik an der Phillips-Kurve ab Mitte der 1960er Jahre. Dieser "`frühe"' Neu-Keynesianismus wird in diesem Kapitel beschrieben und dauerte bis etwa Anfang der 1990er Jahren. Die Hauptproponenten sind hier \textit{Edmund Phelps, Peter Diamond, Joseph Stiglitz, George Akerlof, Michael Spence, John Taylor und Olivier Blanchard}. Diese Schule war der Gegenpol zur aufstrebenden "`Neuen Klassischen Makroökonomie"' um Lucas, Sargeant und Barro. Die Vertreter dieses frühen Neu-Keynesianismus liefern mit ihren Arbeiten vor allem "`Aufweichungen"' der zu starren Annahmen der "`Neuen Klassiker"'. Sie lehnen in diesem Sinn die Arbeiten der "`Neuen Klassiker"' ab, akzeptieren aber auch, dass der Keynesianismus veraltet ist. Von der Zuordnung der Personen her entwickelte sich der "`Neu-Keynesianismus"' eher aus den Salzwasser-Universitäten (vgl. die entsprechende Einteilung in Kapitel \ref{Neue Makro}), die die Neuen Klassiker ja strikt - und nicht nur auf inhaltlicher Ebene - ablehnten. Es wurden aber nicht alle Keynesianer zu "`Neu-Keynesianern"': James Tobin zum Beispiel bestand darauf ein "`Alt-Keynesianer"' zu sein \parencite[S. 45ff]{Tobin1993}. Edmund Phelps drückte dies so aus: "`I [had] warm personal relations with Jim [James] Tobin and Bob [Robert] Solow as well as with Bob [Robert] Lucas and Tom [Thomas] Sargent – relations that have survived our differences. But I belonged to neither school." \parencite{Phelps2006}.

Ab Anfang der 1990er Jahre kam es zunehmend zu einer Verschmelzung von "`Neu-Keynesianismus"' und "`Neuer Klassischer Makroökonomie"'. Diese wird in diesem Buch im nächsten Kapitel als "`Neue Neoklassische Synthese"' beschrieben. Da es eher eine Verdrängung der "`starren"' Neuen Klassischen Makroökonomie durch junge Vertreter des "`Neu-Keynesianismus"' ist, wird sie aber häufig auch einfach "`Neu-Keynesianismus"' genannt\footnote{Man könnte sie auch Zweite Generation des "`Neu-Keynesianismus"' nennen}. Die Hauptvertreter sind hier \textsc{John Taylor}\footnote{der aber auch zur ersten Generation der Neu-Keynesianer gezählt werden muss} \textsc{David Romer, Greg Mankiw, Paul Krugman und Jordi Gal\i}. Der Unterschied zwischen der ersten Generation der Neu-Keynesianer und der zweiten Generation ("`Neue Neoklassische Synthese"') ist, dass die Letztgenannte vor allem die Methoden der "`Neuen Klassiker"', insbesondere "`Dynamische Stochastische General Equilibrium"'-Modelle aus der "`Real Business Cycle"'-Theorie, übernommen hat und um ursprünglich keynesianische Elemente, nämlich "`Monopolistische Konkurrenz"', "`Rigide Löhne und Preise"' und "`Nicht-Neutralität der Geldpolitik"' (und Fiskalpolitik) in der kurzen Frist, übernommen hat. Mehr dazu aber im nächsten Kapitel

Allgemein aber täuscht der Name "`Neu-Keynesianismus"' auf jeden Fall: Er ist nicht etwa eine Weiterentwicklung des Keynesianismus. Schon die hier beschriebene "`Erste Generation der Neu-Keynesianer"' akzeptierte inhaltlich und methodologisch die Fortschritte durch die "`Neuen Klassiker"', bestand aber auf der Bedeutung von Fiskal- und vor allem Geldpolitik, sowie der Existenz von Marktversagen. 

\section{Vorläufer der Gegenrevolution: Formen des Marktversagens} \label{cha: Marktversagen}

Ab dem nächsten Unterkapitel \ref{micmac} werden eindeutig neu-keynesianische makroökonomische Inhalte dargestellt. In diesem Unterkapitel werden wichtige ökonomische Beiträge dargestellt, die aber nicht so eindeutig einer ökonomischen Schule zugeordnet werden können. Die Aspekte des Unterkapitel \ref{Info} behandeln das mikroökonomische Thema der Marktversagensform Informationsasymmetrie. Das zweite Unterkapitel \ref{Disease} behandelt auf der einen Seite eine weitere Marktversagensform, nämlich jene der natürlichen Monopole. Zum anderen wird die sehr unkonventionelle Betrachtung eines makroökonomischen Problems thematisiert: Der Kostenkrankheit. Beide Unterkapitel werden hier platziert, weil sie Beispiele darstellen, in denen reine Marktlösungen nicht zu gewünschten Ergebnissen führen. Die folgenden Unterkapitel stellen daher frühe Opponenten zum damaligen Zeitgeist dar, der die "`Neue klassische Makroökonomie"' hervorbrachte. 

\subsection{Informationsasymmetrie}
\label{Info}
Mit dem Aufstieg konservativer ökonomischer Theorien in den 1960er und 1970er Jahren war auch ein Ruf nach wachsender Deregulierung verbunden. Hayek und Friedman, die Ikonen des Liberalismus, plädierten für einen in allen Belangen freien Markt. Ihre Aussagen dazu waren nur zum Teil wissenschaftlich fundiert, zum anderen Teil sicherlich auch ideologisch motiviert. Genau in dieser Zeit publizierte George Akerlof sein berühmtes Paper, das meist abgekürzt "`Market for Lemons"' genannt wird \parencite{Akerlof1970}. Er begründete damit eine Reihe von Arbeiten in denen gezeigt wurde, dass unter bestimmten Umständen - auf scheinbar ganz normalen, also kompetitiven Märkten - kein befriedigendes Marktgleichgewicht zustande kommt. Dieses Marktversagen kann dann auftreten, wenn zwischen Angebots- und Nachfrageseite asymmetrische Information herrscht. Eine Marktseite ist also über die Marktbedingungen besser informiert, als die andere. Selbst wenn, abgesehen von Informationsasymmetrien, ein vollkommener Markt vorliegt, tritt dieses Problem in einer freien Marktwirtschaft auf. \textcite{Akerlof1970} beschrieb das Problem anhand des Gebrauchtautomarktes. Die Verkäufer gebrauchter Autos kennen den Zustand ihres Wagens in der Regel recht gut. Verkäufer von "`Montagsautos"'  also Autos, die regelmäßig technische Probleme aufweisen, werden die Schwachstellen ihrer mangelhaften Fahrzeuge - die im amerikanischen Englisch häufig "`Lemons"' genannt werden - gerne unerwähnt lassen. Käufer haben nur wenig Möglichkeiten die Qualität eines Gebrauchtwagens zu bewerten. Dies führt zum Problem der Adversen Selektion (negative Risikoauslese). Wenn Käufer nicht zwischen guten und schlechten Gebrauchtwagen unterscheiden können, werden diese zum gleichen Preis gehandelt. Es ist aber jeden klar, dass ein Gebrauchtwagen guter Qualität mehr Wert ist als einer schlechter Qualität. Die Anbieter guter Gebrauchtwagen werden keinen fairen Preis angeboten bekommen, weil sie die hohe Qualität ihres Autos nicht beweisen können. Käufer wiederum werden nur den fairen Preis eines schlechten Autos bereit sein zu bezahlen, da sie nur in diesem Fall sicher sein können, nicht über den Tisch gezogen zu werden. Gute Autos werden von schlechten verdrängt. Als Resultat werden am Gebrauchtwagenmarkt ausschließlich Autos schlechter Qualität gehandelt. Der Markt für gute Gebrauchtwagen bricht zusammen \parencite[S. 490]{Akerlof1970}. Unterteilt man den Markt nicht in gute und schlechte Autos, sondern in ein kontinuierlich verlaufendes Qualitätsspektrum, so werden  ausgezeichnete Autos von guten verdrängt, gute von mittleren und schließlich Autos mittlerer Qualität von schlechten Autos - bis der Gebrauchtwagenmarkt komplett zusammenbricht. Trotz eines grundsätzlich perfekten Wettbewerbsmarktes mit rationalen Teilnehmern, kommt kein Gleichgewicht zustande.

Das Standardbeispiel von Akerlof wurde weltberühmt, wenn es auch nicht das beste Beispiel ist für einen Markt mit asymmetrischer Information. Bei den meisten materiellen Gütern ist eine Qualitätsprüfung in einem gewissen Ausmaß durchaus möglich, so auch bei Gebrauchtwagen. Schon \textcite[S. 492]{Akerlof1970} selbst nannte als weitere Beispiele für adverse Selektion vor allem immaterielle Güter. Insbesondere auf Versicherungsmärkten ist das Problem einer negativen Risikoauslese immanent. Welchem Unfall- oder Gesundheitsrisiko eine Person ausgesetzt ist, ist eine höchstpersönliche Angelegenheit und von Versicherungen nicht so einfach festzustellen. In einem weiteren bahnbrechenden Paper zeigen \textcite{Stiglitz1976a}, dass es auf Versicherungsmärkten tatsächlich zu adversen Selektionsprozessen und in weiterer Folge zu Marktversagen kommen wird, wenn Versicherungen versuchen allen Versicherungsnehmern den gleichen Vertrag, mit der gleichen Risikoprämie, anzubieten. Personen mit hoher Schadeneintrittswahrscheinlichkeit (hohe Risiken) werden so einen Vertrag gerne unterzeichnen. Personen mit geringer Schadeneintrittswahrscheinlichkeit (niedrige Risiken) werden hingegen die Prämie als zu hoch einstufen und den Vertragsabschluss ablehnen. Die notwendige Subventionierung der hohen Risiken durch die niedrigen Risiken bleibt damit aus. Der adverse Selektionsprozess kommt in Gang und der Markt versagt auf gleiche Weise wie im oben genannten Gebrauchtwagen-Beispiel. Ein Markt-Gleichgewicht mit einem Vertrag für alle unterschiedlichen Risikogruppen - genannt Pooling-Gleichgewicht - wird also nicht zustande kommen. Versicherungen könnten aber versuchen unterschiedliche Verträge für unterschiedliche Risikogruppen anzubieten. Sportliche Autofahrer wählen einen Vollversicherungsschutz selbst dann noch, wenn die Prämie höher, also risikoadäquat, ist. Vorsichtige Wenigfahrer verzichten auf einen vollumfänglichen Risikoschutz und genießen dafür die niedrigere, aber ebenfalls risikoadäquate, Prämie. \textcite[S. 634ff]{Stiglitz1976a} zeigen, dass solche "`Separating-Gleichgewichte"' durchaus möglich sind. Die Versicherungsnehmer handeln hierbei nutzenmaximierend, filtern sich jedoch selbst in die für sie passenden Tarifklassen, was auch "`Screening"' genannt wird.

In der Realität gibt es auch auf Versicherungsmärkten viele Möglichkeiten, wie man die Qualität seiner Kunden feststellen kann. Traditionell in der Kraftfahrzeug-versicherung mit Bonus-Malus-Stufen. Verursachte Schäden führen zu höheren Prämien. In der privaten Krankenversicherung mit verpflichtenden ärztlichen Untersuchungen vor Vertragsabschluss. Modernere Ansätze ermöglichen es KfZ-Versicherungsnehmern ihren sicheren Fahrstil in Form von GPS-Daten zu beweisen und der Versicherung damit ein geringes Schadeneintrittsrisiko zu signalisieren. Im Gesundheitsbereich kann man einen gesunden Lebensstil mit Smartphone-Apps, die den Aktivitätsgrad messen, signalisieren und damit die Prämie senken. Allesamt Maßnahmen, die Informationsasymmetrien abbauen, allerdings vielleicht zum Preis der totalen Überwachung?!  
Im Bereich der allgemeinen Gesundheits- und Unfallversicherung ist das Thema der adversen Selektion nach wie vor brandaktuell. In Kontinentaleuropa ging man den Weg der staatlichen Pflichtversicherung aller Erwerbstätigen, die einen bestimmten Teil ihres Einkommens, als Sozialversicherung abgeben müssen und im Sinne eines Solidarbeitrags somit die medizinische Versorgung aller Bürger finanzieren. Im angelsächsischen Raum ist dies noch immer als "`sozialistisches Gedankengut"' verpönt, hier setzt man auf privatwirtschaftliche Lösungen. Im Fall der medizinischen Versorgung sieht es aber so aus als wäre der staatliche Eingriff von Vorteil. So haben die USA das teuerste Gesundheitssystem der Welt und dennoch ist ein beträchtlicher Teil der Bevölkerung unversichert nach wie vor unversichert.

Bereits \textcite{Spence1973} zeigte ein effizientes Mittel, um die Probleme der adversen Selektion zu lösen: Das Signaling. Als Anwendungsbeispiel wählte \textcite{Spence1973} den Arbeitsmarkt. Die Suche nach guten Arbeitnehmern ist schwierig. Nicht zuletzt deswegen weil ein Arbeitgeber weniger Information zur Produktivität hat als die potenziellen Arbeitnehmer selbst. Letztgenannte stehen als Bewerber in Konkurrenz zur Stelle, die der Arbeitgeber anbietet. Diese können versuchen dem Arbeitgeber zu signalisieren, dass sie die beste Wahl für die ausgeschriebene Stelle sind. Das benötigen sie entsprechende Mittel, genannt Signale. Je schwieriger (ökonomisch: je kostenintensiver) ein Signal zu erwerben ist, desto besser ist das Signal. Ein schwaches Signal wäre es zum Beispiel adäquat gekleidet und freundlich beim Vorstellungsgespräch aufzutreten. Allerdings sollten das zumindest einige Bewerber schaffen. Somit kann sich ein einzelner dadurch nicht von den anderen abheben, es liegt ein schwaches Signal vor. Wesentlich schwieriger - weil zeit- und damit kostenintensiv - ist es eine fundierte Ausbildung zu absolvieren. Eine vollwertige Ausbildung dauert mehrere Jahre in denen man schlecht bezahlt an einem starken Signal arbeitet. Dieses Signal, zum Beispiel in Form eines Abschlusszeugnisses, kann man dann verwenden um beim Bewerbungsgespräch die asymmetrische Information des potenziellen Arbeitgebers zu verringern \parencite{Spence1973}
Es bleibt allerdings auch hier die Frage nach der Regulierung. Denn der Arbeitgeber muss ein schwaches Signal von einem starken Signal unterscheiden können. Kann er das nicht wird das Problem nur um eine Stufe verlagert: Die adverse Selektion findet dann auf Ebene der Signale statt. In Kontinentaleuropa waren Ausbildungen unter anderem deshalb lange Zeit staatlichen Institutionen vorbehalten. Mit der Privatisierung von Bildungsinstitutionen treten diese in Konkurrenz zueinander. Es ist also kurzfristig individuell rational, zum Beispiel als Privatuniversität, ein eigentlich starkes Signal, zum Beispiel einen Master-Abschluss, für einen verhältnismäßig geringen Aufwand zu verleihen. Die Nachfrage nach diesem Bildungsprodukt - und damit der Gewinn der Hochschule - werden hoch sein. Erst langfristig werden die Arbeitgeber die mangelnde Qualität der Ausbildung, durch die geringere Produktivität der Arbeitnehmer, feststellen. Das Signal "`Master-Abschluss"' reicht dann nicht mehr aus, das Problem der asymmetrischen Information kann durch dieses Signal nicht mehr behoben werden.

Ein weiteres Anwendungsbeispiel der Probleme asymmetrischer Information, das ebenfalls den Arbeitsmarkt betrifft, wird in der Effizienzlohnhypothese (Adverse Selektion) und im "`Shirking"'-Modell (Moral Hazard) behandelt. Die Folgen sind makroökonomisch bedeutsam und werden daher in Unterkapitel \ref{RR_AM} behandelt.

Adverse Selektion beschreibt das Problem Asymmetrischer Information \textit{vor} Vertragsabschluss. Beim nun behandelten Problem des "`Moral Hazard"' (moralisches Risiko) tritt das Problem asymmetrischer Information erst \textit{nach} Vertragsabschluss auf. Das Problem, dass eine Person nach dem Abschluss einer Vereinbarung sein Verhalten ändert um sich besser zu stellen, ist jedem von uns seit Kindertagen bekannt. In den 1960er Jahren wurde dieses Verhalten allerdings erstmals ökonomisch analysiert \parencite{Arrow1963, Dickerson1963} und zwar am Beispiel medizinischer Versorgung: Je besser der Versicherungsschutz, desto umfangreicher waren die in Anspruch genommenen Leistungen. \textcite{Pauly1968} zeigte, dass nicht unmoralisches Verhalten der Auslöser für die Verhaltensänderung sein muss, sondern dass die Verhaltensänderung wirtschaftlich rationales Verhalten darstellen kann. Das ist nicht unwesentlich, da unmoralisches Verhalten - in Regeln formalisiert und in Gesetze gegossen - sanktioniert werden kann. Ökonomisch rationales Verhalten, das zu keinem stabilen Marktergebnis führt, ist hingegen Marktversagen. Vor allem im Versicherungsbereich ist dieses Problem wohlbekannt. Das Versicherungsunternehmen hat keine Möglichkeit sicher festzustellen, welches Ausmaß an Leistungen vom Versicherungsnehmer gerechtfertigt in Anspruch genommen werden darf und durch welche Verhaltensweisen der Versicherungsnehmer zusätzliche Versicherungsleistungen "`provoziert"'. Im Bereich der Versicherungen gibt es allerdings ein einfache Maßnahme, die Moral Hazard weitgehend einschränkt: Die Selbstbehalte. Wie Sie wahrscheinlich schon bei eigenen Versicherungsverträgen bemerkt haben, gibt es Selbstbehalte praktisch in jedem Versicherungsvertrag verankert, der hohen Versicherungsschutz umfasst.

Eine Ableitung aus dem Moral Hazard-Problem hat ist im Rahmen der "`Great Recession"', also der globalen Wirtschaftskrise nach 2008 in den Mittelpunkt öffentlicher Diskussion getreten: Das \textit{Principal Agent}-Problem. Dieses Problem zielt darauf ab, dass Eigentümer eines Unternehmens und dessen Manager unterschiedliche Ziele verfolgen. Die Aktionäre (="'Principals"'), also die Eigentümer, einer großen Bank haben das Ziel, dass das Unternehmen \textit{langfristig} seinen Wert maximiert. Die Manager (="'Agents"') haben hingegen befristete Verträge und damit Anreize \textit{kurzfristige} Gewinne zu erzielen, da diese die Höhe ihrer Prämie bestimmen. Die Aktionäre, als Arbeitgeber der Manager, sehen sich also dem Risiko ausgesetzt, dass die Manager nach Vertragsabschluss andere Ziele verfolgen, als ursprünglich vereinbart. Eine noch extremere Ausprägung resultiert im "`Too big to fail"'-Problem. Hier wird Managern vorgeworfen, bewusst sehr riskante Geschäfte einzugehen, weil sie im Wissen handeln, im Falle eines Defaults vom Staat finanziell unterstützt werden zu müssen, da andererseits systemweite Verwerfungen drohen. Dieses Problem ist eher dem "`Neuen Institutionalismus"' (Kapitel \ref{Neue Institut}) zuzuordnen und wurde erstmals von \textcite{Jensen1976} beschrieben. Ihre wissenschaftliche Abhandlung dazu erfuhr große Resonanz und gilt als einer der meist-zitierten Journalartikel in den Wirtschaftswissenschaften.


\subsection{Die Kostenkrankheit und angreifbare Märkte}
\label{Disease}
Ein extrem unkonventioneller Ökonom war William Baumol. Er lieferte bedeutende Beiträge in verschiedenen Disziplinen der Volkswirtschaft, sowohl in der Mikroökonomie als auch in der Makroökonomie. Es ist sympathisch, dass er ideologisch kaum einzuordnen ist, da er in so vielen grundverschiedenen Bereichen forschte. Er scheint sich nur der Wissenschaft verpflichtet gefühlt zu haben. Zwei wichtige Beiträge werden hier - in historisch umgekehrter Reihenfolge - dargestellt.

Eine in der praktischen Umsetzung unheimlich mächtige Idee ist Baumol's \textit{Theorie der angreifbaren Märkte}. Eine der vier zentralen Marktversagensformen ist jene der \textit{Natürlichen Monopole}\footnote{Die anderen drei sind: \textit{Externe Effekte}, \textit{Die Tragik der Allmende} und die im letzten Unterkapitel dargestellten \textit{Informationsasymmetrien}.}. Ökonomen sprechen von einem natürlichen Monopol, wenn die Durchschnittskosten über den gesamten Mengenbereich über den Grenzkosten liegen. Anders ausgedrückt heißt das nichts anderes als, dass ein natürliches Monopol dann vorliegt, wenn die Fixkosten einen hohen Anteil der Gesamtkosten ausmachen. Dies ist überall dort der Fall wo eine große Infrastruktur geschaffen werden muss um das Produkt oder die Dienstleistung überhaupt anbieten zu können. Wenn die Infrastruktur aber einmal existiert, fallen nur mehr verhältnismäßig geringe Kosten für den laufenden Betrieb an. Beispielen dafür begegnen wir täglich. Das gesamte Verkehrsnetz, vor allem im Bereich der Eisenbahnen, ist ein Beispiel für ein natürliches Monopol. Aber auch das Strom- und Gasnetz, die Wasserversorgung oder das Kanalnetz, sowie das Telefonnetz und Postdienstleistungen sind Beispiele dafür. Stellen sie sich vor sie möchten als Privatunternehmen in den Eisenbahnverkehr einsteigen. Bevor sie auch nur eine Fahrt starten könnten, müssten sie Eisenbahnschienen verlegen. Ein eventuell bereits vorhandenes Schienennetz gehört schließlich nicht ihnen und sie können damit vom Gebrauch leicht ausgeschlossen werden. Jetzt ist es jedoch aus ökonomischen, aber auch aus raumplanerischen und nicht zuletzt ökologischen Gründen, völlig undenkbar, dass jeder potenzielle Anbieter sein eigenen Schienennetz erstellt. Ein einziges Schienennetz pro Strecke ist die einzig sinnvolle Lösung. Der Eigentümer dieses Schienennetzes ist per Definition ein Monopolist. Da er weder aus rechtlichen Gründen, noch aufgrund ökonomischer Überlegenheit zum Monopolisten wurde, sondern aufgrund der Eigenschaft Eigentümer der notwendigen Infrastruktur zu sein, nennt man so eine Situation ein \textit{Natürliches Monopol}. Aufmerksamen Lesern mag vielleicht aufgefallen sein, dass alle gerade oben genannten Dienstleistungen früher auch in den westlichen Marktwirtschaften fast ausschließlich von staatlichen Unternehmen angeboten wurden. Dass dies heute nur mehr zum Teil der Fall ist, ist nicht zuletzt auf die bahnbrechende Arbeit von \textcite{Baumol1982, Baumol1982b} zurückzuführen. Darin zeigen die Autoren die wesentliche Bedeutung von Markteintritts- und Marktaustrittsbeschränkungen.  Wenn man nun - eine zugegebenermaßen zunächst recht theoretisch wirkende Überlegung - diese Beschränkungen abschafft, so führt schon allein dies, laut \textcite{Baumol1982}, dazu, dass sich der bisherige Monopolist potenzieller Konkurrenz ausgesetzt sieht und in weiterer Folge seine Preissetzung nicht mehr wie ein Monopolist vornimmt, sondern wie ein Anbieter auf einem perfekten Konkurrenzmarkt. Alleine das Abschaffen von Markteintritts- und Marktaustrittsbeschränkungen führt also dazu, dass aus einem Monopolmarkt praktisch ein perfekter Konkurrenzmarkt wird \parencite[S. 2]{Baumol1982b}. Damit verbunden sind all die positiven Effekte: Niedrigerer Preis und höhere ausgebrachte Menge. Der Grund liegt einfach darin, dass die erhöhten Gewinnmöglichkeiten auf Monopolmärkten leicht erkannt und von Konkurrenten ausgenutzt werden können, wenn es keine Zugangsbeschränkungen gibt. Als Resultat verschwindet der Monopolvorteil aufgrund von Marktkräften und der Markt wird zum perfekten Konkurrenzmarkt. Der zweite Wohlfahrtseffekt ist ebenso bedeutend. Monopolmärkte tendieren dazu nicht ökonomisch effizient zu arbeiten. Wer der Chance hat eine Monopolrente abzuschöpfen und sich keinem Konkurrenzdruck ausgesetzt sieht, wird mit der Zeit immer weniger effizient arbeiten. Nach \textcite{Leibenstein1966} nennt man dieses Problem das Auftreten von \textit{X-Efficiency}. Wer früher einen Telefonanschluss haben wollte, musste diesen bei der staatlichen Behörde beantragen und war nicht selten deren Willkür und damit langen Wartezeiten ausgesetzt. Heute, am offen zugänglichen Telekommunikationsmarkt, wird man als potenzieller Kunde geradezu umgarnt von Anbietern\footnote{Das Beispiel hinkt zugegebenermaßen, weil im angesprochenen Bereich der technische Fortschritt wohl einen größeren Beitrag zur besseren Marktentwicklung beigetragen hat.}. 
Kommen wir zurück zum entscheidenden Punkt: Was sind Markteintritts- und Marktaustrittsbeschränkungen überhaupt? Gemeint sind damit Kosten, die ein Unternehmen leisten muss, bevor es überhaupt auf einem Markt als Anbieter auftreten kann. In einem natürlichen Monopol stellen diese Kosten also zum Beispiel die Herstellungskosten für das Schienennetz dar. Das Abschaffen dieser Markteintritts- und Marktaustrittsbeschränkungen bedeutet aber nicht, dass die Kosten auf Null reduziert werden, sondern, dass ein am Markteintritt interessiertes Unternehmen keinerlei Benachteiligungen haben darf gegenüber dem Monopolinhaber \parencite[S. 3f.]{Baumol1982b}. Ökonomisch ist das Thema mit dem Satz "`Abschaffen von Markteintritts- und Marktaustrittsbeschränkungen"' abgedeckt. Auf anderer - nämlich rechtlicher - Ebene wurde damit ein riesiges Feld geöffnet: Das Wettbewerbs- und Regulierungsrecht wurde damit wesentlich erweitert. Es muss nämlich ein Rahmen geschaffen werden, der den bisherigen Monopolisten und damit Inhaber der Infrastruktur dazu bringt, allen interessierten Mitbewerbern zu fairen Preisen Zutritt zum Markt zu gewähren. Dies funktioniert meist so, dass die Infrastruktur in ein eigenes staatliches Unternehmen ausgegliedert wird und eine neu geschaffenen Kontrollbehörde den Marktzutritt zu fairen Preisen zusichert. Auf sehr umkämpften Märkten, vor allem auf den mobilen Telekommunikationsmärkten, wurden in der Folge Lizenzen an die Höchstbieter vergeben, wobei die Vergabeverfahren ein eigenes Forschungsfeld aufmachten: Die sogenannten \textit{Auktionstheorie}, die vor allem durch spieltheoretische Überlegungen geprägt ist. In diesem Bereich wurde 2020 der Nobelpreis für Wirtschaftswissenschaften an \textit{Paul Milgrom} und \textit{Robert Wilson} vergeben. Ein anderes Forschungsfeld, das auch auf spieltheoretischen Überlegung basiert, ist jenes des sogenannten \textit{Marktdesigns}. Dabei geht es um die Gestaltung der Märkte, die ursprünglich natürliche Monopolmärkte waren. In diesem Bereich wurde bereits 2012 der Nobelpreis für Wirtschaftswissenschaften an \textit{Alvin Roth} und \textit{Lloyd Shapley} vergeben\footnote{Beide Themen werden im Kapitel \ref{cha: Spieltheorie} näher behandelt.}. Wie man sieht stieß das Konzept insgesamt einen riesigen Forschungsbereich an. Dahingehend ist es überraschend, dass William Baumol selbst niemals Nobelpreisträger wurde, obwohl er das hohe Alter von 95 Jahren erreichte. 

Insgesamt war das Modell theoretisch extrem einflussreich. Interessanterweise spielt es eher den polit-ökonomisch liberalen Tendenzen der 1980er Jahre in die Hände. Schließlich wird einer Form des Marktversagens nicht mehr durch staatliche Eingriffe entgegengetreten, sondern durch eine reine Marktlösung, gepaart mit gesetzlichen Vorgaben. Die Theorie passt damit eher zur \textit{Neuen Institutionsökonomik} (Kapitel \ref{Neue Institut}) oder sogar zur \textit{Neuen Klassischen Makroökonomie} (Kapitel \ref{Neue Makro}). Dabei ist Baumol zumindest letztgenannter Schule keineswegs zuzurechnen. Auch praktisch war die Umsetzung sofort ein Erfolgsmodell. Die Umsetzung erfolgte praktisch nahtlos an das Erscheinen des Buches \textcite{Baumol1982}. Vorreiter war Großbritannien, wo Margaret Thatcher noch in den 1980er Jahren Transportunternehmen privatisieren ließ. Anfang der 1990er Jahre erfolgte die große Privatisierungswelle in den westlichen Industriestaaten vor allem auf den Telekommunikationsmärkten, dem Eisenbahnwesen und der Versorgungsindustrie (vor allem Elektrizität). Aus heutiger Sich ist diese Umsetzung insgesamt wohl eine Erfolgsgeschichte. Vor allem wenn man auf sicherlich vorhandene X-Effizienzen in verkrusteten, staatlichen Unternehmen denkt. Mittlerweile kann man aber auch wesentliche Kritikpunkte nicht von der Hand weisen. So unterblieben in vielen Ländern, als Antwort auf die Liberalisierung, wichtige Investitionen in die Infrastruktur. Das traurigste Negativbeispiel ist wohl die Privatisierung der Eisenbahn in Großbritannien. Ende der 1990er-Jahre kam es dort zu zwei schweren Zugunglücken, die auf den schlechten Zustand der Infrastruktur und damit letztendlich auf die Liberalisierung des Eisenbahnverkehrs, zurückgeführt werden. Auch abgesehen davon funktionierte der private Eisenbahnmarkt in Großbritannien schlecht und so gibt es seit 2020 Tendenzen den Betrieb wieder gänzlich zu verstaatlichen. Es gibt aber auch den USA und in Kontinentaleuropa zahlreiche Negativbeispiel. Das Problem liegt hierbei häufig darin, dass die Umsetzung ganz wesentlich von der Schaffung eines rechtlichen Rahmenwerkes abhängt. Diese Regulierung zu schaffen ist schwierig und in der Praxis häufig gescheitert.

Einen weiteren, gänzlich anderen aber ebenso unkonventionellen, Beitrag zur Ökonomie lieferte \textit{William Baumol} bereits Mitte der 1960er Jahre \parencite{Baumol1965, Baumol1967}. Das exogene Wachstumsmodell von Robert Solow war schon als Standardmodell etabliert, als William Baumol einen interessanten Problempunkt darin aufzeigte. Er unterscheidet zwei Arten von Unternehmungen. Eine, die stark Technologie-getrieben ist und eine zweite, bei der Technologie nur eine untergeordnete Rolle spielt. Baumol selbst nannte diese "`stagnierenden Sektor"' und "`progressiven Sektor"' \parencite[S. XX]{Baumol2012}. Man könnte sie aber auch grob als Produktionsunternehmen und Dienstleistungsunternehmen bezeichnen. Entscheidend ist, dass erstgenannte vom technologischen Fortschritt profitieren. Mit besserer Technologie steigt die Produktivität in diesem Sektor an. Im zweitgenannten Sektor gibt es hingegen keine Produktivitätszugewinne durch verbesserte Technologie. Baumol's klassisches Beispiel dafür - er war ein bekennender Kunst- und Musikliebhaber \parencite[S. 228]{Krueger2001Interview} - war jenes vom Streichquartett, das für ein Musikstück heute noch immer so lange braucht wie vor 200 Jahren. Für die Herstellung eines Stahlträgers werden hingegen heute wesentlich weniger Arbeitsstunden benötigt, als im frühen 19. Jahrhundert. Daraus kann man ableiten, dass der Aufwand (und damit die Kosten) für ein Streichquartett gleich geblieben sind, während die Kosten für einen Stahlträger relativ dazu gesunken sind. Bei einer angenommenen Inflation von zum Beispiel 4\% würden Dienstleistungen dementsprechend eine höhere Inflation erfahren, als Produktionsgüter. Oder mit anderen Worten: Dienstleistungen wie medizinische Leistungen, Pflege und Ausbildung werden im Verhältnis zu Produktionsgütern wie Kleidung, Fernseher und Autos immer teurer. In der Ökonomie ist dieses Phänomen als die \textit{Kostenkrankheit}\footnote{Auch als \textit{Baumol's Disease} oder \textit{Bowen's curse} bezeichnet\parencite[S. 3]{Baumol2012}.} bekannt.
Die Folge der Kostenkrankheit ist, dass sich die Anteile an den Gesamtausgaben immer stärker von den Industriegütern zu den Dienstleistungen verschieben. Laut \textcite[S. 43]{Baumol2012} ist dies aber kein Problem. Wenn die Produktivität für einen Teil der geleisteten Arbeitsstunden steigt, für einen Teil konstant bleibt, aber für keinen Teil sinkt, dann steigt die Produktivität auch im Durchschnitt. Damit steigt insgesamt die Produktion und damit das GDP. Dass sich das Verhältnis zwischen Gütern und Dienstleistungen dadurch verschiebt und damit zum Beispiel der Anteil der Kosten für Gesundheitsdienstleistungen immer weiter steigt, ist zwar ein Faktum, aber im Endeffekt von sekundärer Bedeutung. Die Kostenkrankheit wäre demnach eher ein Verteilungsproblem als ein Problem stagnierender Produktivität. 

\textcite{Nordhaus2008} hat die möglichen Auswirkungen der Kostenkrankheit analysiert, zusammengefasst und empirisch untersucht. Er kam zu dem Schluss, dass sich die Baumolsche Kostenkrankheit in sechs Symptomen zeigt. Die Kosten und Preise müssten auf stagnierenden Märkten überdurchschnittlich stark steigen (1), der reale Output in diesen Sektoren sollte aber nur unterdurchschnittlich steigen (2). Wenn stagnierende Märkte Preis-unelastisch sind, dann müsste der Anteil dieser Märkte am Gesamt-BIP steigen (3), dies würde das Gesamtwachstum der Produktivität reduzieren (4). Unternehmen auf stagnierenden Märkten werden zunehmend Kosten- und Preisdruck ausgesetzt und schließlich vermehrt finanzielle Probleme haben (5). Zuletzt stellt sich die Frage, wie sich die Kostenkrankheit auf die Gesamtbeschäftigung auswirkt (6). \textcite{Nordhaus2008} kommt zu dem Ergebnis, dass für die USA seit 1948 die ersten drei Symptome eindeutig zu erkennen sind. Auch das Wachstum in der Produktivität hat sich seit 1948 verlangsamt. Interessanterweise könnte Nordhaus nicht bestätigen, dass vor allem Unternehmen in progressiven Sektoren vom technologischen Fortschritt profitieren und im Umkehrschluss Unternehmen im stagnierenden Sektor in Probleme geraten. Stattdessen werden die Vorteile, die in progressiven Sektoren beobachtet werden, tendenziell an den Endkunden weitergegeben. Das heißt Produkte im aus dem progressiven Sektor werden relativ gesehen immer günstiger. Der letzte Punkt ist nicht eindeutig zu beantworten. Es gibt Industrien, in denen technologischer Fortschritt zu einem Ersatz der Arbeitskräfte durch Maschinen führt. Eine relativ dazu steigende Anzahl an Personen ist dann in stagnierenden Sektoren tätig. Es gibt aber auch Industrien, wo der technologische Fortschritt zu einer steigenden Nachfrage nach Arbeitskräften führt \parencite{Nordhaus2008}.

Das Konzept hat damit übrigens auch spannende wirtschaftspolitische Auswirkungen. Da der Staat fast ausschließlich für die zur Verfügung-Stellung von Dienstleistungen zuständig ist, der private Sektor hingegen nach wie vor einen gehörigen Anteil an Produktionsunternehmen hat, müsste es dazu kommen, dass die staatlichen Leistungen im Verhältnis zu privatwirtschaftlichen Leistungen im Durchschnitt immer teurer werden. Das heißt, die Staatsquote müsste mit steigender Produktivität ebenfalls steigen, wenn die staatlichen Dienstleistungen in gleicher Qualität und Quantität aufrechterhalten werden sollen. Dies wäre auch eine Erklärung für das Wagner'sche Gesetz, das postuliert, dass mit steigendem Wohlstand auch die Staatsquote zunimmt. Es steht aber auch im Spannungsverhältnis mit wachsendem Budgetdruck und dem Trend zu sinkenden Staatsquoten.
 

\section{Phelps: Mikrofoundation der Makroökonomie}
\label{micmac}

Man findet wohl kaum einen Namen, der den Übergang von "`Keynesianismus"' zu "`Neu-Keynesianismus"' besser repräsentiert als \textsc{Edmund Phelps}. Ökonomisch geprägt wurde er in einem eindeutig keynesianischen Umfeld: Er verfasste bei James Tobin seine Dissertation und arbeitete Mitte der 1960er Jahre mit Robert Solow, Paul Samuelson und Franco Modigliani zusammen. Also alles eindeutig keynesianische Ökonomen, die wir aus Kapitel \ref{Synthese} kennen. Laut seines autobiografischen Artikels \textcite[S. 93]{Heertje1995} war diese Zeit, inklusive Gastprofessur am Massachusetts Institute of Technologie (MIT), die prägendste seiner Karriere. Er selbst war innerhalb weniger Jahre ein international anerkannter Ökonom. Schon 1961 veröffentlichte er sein erstes bedeutendes Werk: \textit{The Golden Rule of Accumulation} \parencite{Phelps1961}. Ein bemerkenswerter Artikel, den der damals erst 28-jährige Phelps im American Economic Review veröffentlichte. Gerade einmal sieben Seiten lang, beginnt dieser - so wie im Englischen normalerweise Märchen  - mit "`Once upon a time"'. In weiterer Folge wechseln sich mathematische Formeln mit Dialogen zwischen dem König und dem Volk der Solovians ab \parencite[S. 640]{Phelps1961}. So witzig und amüsant die Geschichte des Artikels, so bahnbrechend ist auch deren Inhalt. Diese Arbeit kann als direkter Anschluss an die Wachstumstheorie Solow's gesehen werden. In deren Zentrum steht folgende hypothetische Überlegung: Wenn die gesamte aktuelle Wirtschaftsleistung für die Investition (Investition = Sparen!) in neue Produktionsgüter verwendet wird, dann wird nichts für den aktuellen Konsum ausgegeben. Wird hingegen die gesamte aktuelle Wirtschaftsleistung für Konsum verwendet, werden im Umkehrschluss keinerlei neuen Investitionen getätigt. Beide Extrembetrachtungen führen also zu keinem sinnvollen Gleichgewicht. Das heißt aber auch, dass dazwischen irgendein optimales Verhältnis zwischen Sparen/Investieren auf der einen Seite und Konsumieren auf der anderen Seite bestehen muss. Dieses erreicht man eben durch \textit{The Golden Rule of Accumulation}. Diese wird erreicht - solange man einige vereinfachenden Annahmen zulässt - wenn die Wachstumsrate des BIPs dem Zinssatz entspricht. Bereits Phelps nannte diese natürliche Wachstumsrate "`nachhaltig"' \parencite[S. 638]{Phelps1961}. Weiters zeigt Phelps formal, dass diese Wachstumsrate erzielt wird, wenn die Summe der Investitionen der Summe der Profite entspricht, also alle Profite investiert werden. Umgekehrt werden im Optimum alle Löhne konsumiert. Zusammengefasst: Wenn alle Löhne konsumiert werden und alle Profite investiert werden, befindet sich die Ökonomie auf einem nachhaltigen Wachstumspfad. Die Wachstumsrate entspricht dann dem Zinssatz. Insgesamt erinnert das Ergebnis an die Arbeiten von Wicksell und Hayek. Die formale Herleitung durch Phelps war aber zu diesem Zeitpunkt - im Jahre 1961 - eine bahnbrechende Erweiterung des Solow-Wachstumsmodells.

Das bisher in diesem Unterkapitel dargestellte, entspricht noch vollständig dem keynesianischem Denken aus Kapitel \ref{Synthese}. Im Jahr 1966 wechselte Phelps von Yale an die University of Pennsylvania (Penn). Mit dem Umzug konzentrierte er sich auf neue Themen, nämlich auf die theoretische Fundierung der Phillipskurve. Seine Arbeiten dazu sollten später die ersten Zweifel am dominierenden, keynesianschen Framework begründen. Im Nachhinein kann man getrost sagen, dass damit die Grundlagen für den "`Neu-Keynesianismus"' geschaffen wurden.

Wie in Kapitel \ref{sec: Phillips} dargestellt, war der vermeintliche, negative Zusammenhang zwischen Inflation und Arbeitslosigkeit zwar nicht Bestandteil der ursprünglichen keynesianischen Theorie. Aber in weiterer Folge vor allem in der keynesianischen Wirtschaftspolitik ein fixer Bestandteil. Unabhängig voneinander waren Milton Friedman und eben Edmund Phelps bereits ab Mitte der 1960er Jahre die ersten Ökonomen, die den Zusammenhang zwischen Inflation und Arbeitslosigkeit in Frage stellten. Wohlgemerkt zu einer Zeit, in der der Zusammenhang empirisch noch recht gut beobachtet werden konnte. Das in den 1970er Jahren diese Korrelation weitgehend verschwand gab den Kritikern Friedman und Phelps natürlich gehörig Auftrieb. Phelps hatte seine Kritik dabei - im Gegensatz zu Friedman - mathematisch-formal unterlegt. 

Der Artikel mit dem unscheinbaren Titel "`Money-Wage Dynamics and Labor-Market Equilibrium"' \parencite{Phelps1968} stellte die bis dahin unbestrittene Phillipskurve nicht nur infrage, sondern legte die Grundlage für eine ganz neue Sicht auf die Wirtschaftswissenschaften. Interessant ist, dass gleich mehrere Punkte, die natürlich ineinandergriffen, in diesem Artikel revolutionäre waren:
\begin{enumerate}
\item Die Mikrofundierung der Makroökonomie
\item Die formale Einführung der Erwartungen (als adaptive Erwartungen) als Notwendigkeit bei Unvollständiger Information
\item Die formale Einführung der Natürlichen Arbeitslosigkeit und "`Effizienzlöhne"'
\end{enumerate}
Bemerkenswert ist insbesondere, dass alle drei genannten Punkte bis heute fixer Bestandteil der Mainstream-Modelle sind. Die heutigen DSGE-Modelle sind mikrofundiert, beinhalten das Konzept der Erwartungen (wenn auch der rationalen statt der adaptiven) und akzeptieren einen gewissen Prozentsatz an Arbeitslosigkeit als Gleichgewichtszustand. Natürlich wurden alle drei Konzepte seit 1968 wesentlich erweitert, aber im Gegensatz zu den Arbeiten anderer großen Ökonomen, fällt auf, dass Phelps' Arbeiten bis heute, 50 Jahre später, kaum an Gültigkeit verloren. Keynes' Multiplikator ist heute höchst umstritten, Friedman's Geldmengensteuerung betreibt keine Zentralbank der Welt mehr und selbst die späteren Arbeiten von Robert Lucas wurden größtenteils von der Realität überholt. Phelps' bahnbrechende Erkenntnisse sind hingegen bis heute die Grundlage ökonomischer Modelle und kann daher als Geburtsstunde des "`Neu-Keynesianismus"' gesehen werden.

In seiner Nobelpreis-Biographie schreibt Phelps, dass es seit seiner College-Zeit das Gefühl hatte die wichtigste aktuelle Herausforderung der Wirtschaftswissenschaften sei die Integration der Mikroökonomie in die Makroökonomie \parencite{Phelps2006}. Heute nennen wir dies die Mikrofundierung der Makroökonomie.
Der inhaltliche Ausgangspunkt des oben genannten Artikels \parencite{Phelps1968} ist die Phillipskurve. Phelps beschreibt sie als Naivität der Keynesianer. Wobei er Keynes selbst ausdrücklich in Schutz nimmt: Keynes' Nachfragesteuerung wäre niemals soweit gegangen einen dauerhaft stabilen Zusammenhang zwischen Inflation und Arbeitslosigkeit anzunehmen \parencite{Phelps2006}. Phelps stellt stattdessen einen Zusammenhang zwischen der \textit{erwarteteten} Inflation und Arbeitslosigkeit her. Dieser Zusammenhang sei aber nur in der kurzen Frist stabil. Angenommen die erwartete Inflation läge bei 4\%. Arbeitgeber und Arbeitnehmer würden bei ihren Vertragsverhandlungen diese Inflationserwartung einfließen lassen und die Lohnhöhe entsprechend festlegen. Will die Zentralbank nun die Arbeitslosigkeit senken, kann sie Maßnahmen setzen, die die Inflation auf zum Beispiel 6\% erhöhen. Solange die erwartete Inflation unter der tatsächlichen Inflation liegt, wird die Arbeitslosigkeit sinken und sich somit wie von der Phillipskurve postuliert verhalten. Es ist aber klar, dass die Diskrepanz zwischen tatsächlicher und erwarteter Inflation nur kurzfristig aufrechterhalten werden kann, bevor sich die Erwartung dem tatsächlichen Wert anpasst. Die keynesianische, langfristige Phillipskurve wurde durch die neu-keynesianische, kurzfristige erwartungsgestützte Phillipskurve ersetzt. Als solche findet sie bis heute Eingang in die makroökonomischen Lehrbücher. Nebenbei etablierte Phelps dabei das Konzept der adaptiven Erwartungen, das aber später vom neuklassischen Konzept der rationalen Erwartungen abgelöst werden sollte.
Die zentrale Aussage in \textcite{Phelps1968} lautet, dass durch Geldpolitik die Arbeitslosigkeit nicht dauerhaft beeinflusst werden kann, sehr wohl aber unter Umständen in der kurzen Frist. Geldpolitik funktioniere außerdem über Inflations\textit{erwartungen} und diese passen sich recht schnell an die aktuelle Inflation an. Eine niedrige Inflation wird daher auch nicht langfristig zu höherer Arbeitslosigkeit führen\parencite{Phelps1967}. Daraus könnte man ableiten, dass die zentrale Aufgabe der Zentralbanken die Inflationssteuerung ist. Heute orientieren sich fast alle führenden Zentralbanken tatsächlich primär an den Inflationszielen, dies aber direkt auf Phelps' frühe Arbeiten zurückzuführen ginge aber zu weit, folgten doch noch weitere Arbeiten dazu von anderen Neu-Keynesianern und Neuen Klassikern.
Sehr wohl direkte Folge aus \textcite{Phelps1968} ist hingegen die Idee der "`natürlichen Arbeitslosenrate"', später häufig als NAIRU\footnote{non-accelerating inflation rate of unemployment} bezeichnet. Während die Klassiker davon ausgingen, dass es im Gleichgewicht keine Arbeitslosigkeit gäbe und die Neuen Klassiker meinten im Gleichgewicht gäbe es ausschließlich freiwillige Arbeitslosigkeit, verfolgten die Keynesianer den Ansatz Arbeitslosigkeit sei stets mit nachfrageorientierter Wirtschaftspolitik zu minimieren. Die Neu-Keynesianer gehen davon aus, dass es im Gleichgewicht ein gewisses Maß an unfreiwilliger Arbeitslosigkeit gäbe. Diese "`natürliche Arbeitslosenrate"' wird häufig Milton Friedman zugeschrieben, der einen sehr ähnlichen Ansatz ebenfalls 1968 veröffentlichte \parencite{Friedman1968}. Tatsächlich hatten Friedman und Phelps unterschiedliche Wege gewählt, die sie zu den gleichen Schlussfolgerungen führten. Der Begriff "`natürliche Arbeitslosenrate"' ist wohl Friedman zuzuschreiben, größeren Einfluss in der akademischen Welt hatte aber der Ansatz von Phelps \textcite[S. 9f]{Nobelpreis-Komitee2006}. Die theoretische Bearbeitung der Arbeitslosigkeit wurde später zu einem eigenen Forschungsgebiet innerhalb des "`Neu-Keynesianismus"' und ist in Kapitel \ref{Suchtheorie} dargestellt.
Die zentralen Aussagen der damals neuen Theorie wurden in einer Konferenz aufgearbeitet und schließlich gesammelt als Buch veröffentlicht \textcite{Phelps1970}. Dieses war in weiterer Folge einflussreich und erreichte unter Ökonomen einen hohen Bekanntheitsgrad unter dem Titel "`The Phelps volume"'.

Die Mikrofundierung der Makroökonomie wird ebenfalls häufig als wesentliche Neuerung der "`Neuen Klassischen Makroökonomie"' gesehen. Es ist aus methodischer Sicht \textit{der} große Bruch mit den Theorien der Keynesianer und auch Monetaristen. Tatsächlich ist nicht von der Hand zu weisen, dass die Neuen Klassiker diesen Ansatz als Standard in ökonomischen Modellen etablierten (vgl. Kapitel \ref{Neue Makro}). Aber auch hier gilt, dass die erstmalige Anwendung auf Phelps zurückgeht. In der nun schon häufig zitierten Arbeit \textcite{Phelps1968} verwendet Phelps ein mikroökonomisches Modell um den klar makroökonomischen Zusammenhang zwischen Inflation und Arbeitslosigkeit zu modellieren. Die dort notwendige intertemporale Betrachtung (im Kapitel \ref{Neue Makro} haben wir das schlicht "`dynamische Betrachtung"' genannt) der Interaktion der Kennzahlen war eine weitere Neuerung durch Phelps, die bis heute State-of-the-art in den Wirtschaftswissenschaften ist \parencite[S. 9]{Nobelpreis-Komitee2006}.

Die soeben dargestellten Ergebnisse wurden nicht unmittelbar begeistert aufgenommen, wie dies zum Beispiel mit Keynes' General Theory, oder der Theorie der Rationalen Erwartungen von Lucas passierte. Bei ihrem erscheinen Ende der 1960er-Jahre konkurrierten die Ideen von Phelps mit zahlreichen alternativen Ideen. Erst etwas später wurde durch die Stagflation sichtbar, dass die Phillipskurve in ihrer alten Form untragbar wurde und Phelps Erwartungsgestützte Phillipskurve als Alternative zielführender sei.

Für die Ende der 1060er Jahre noch unumschränkt dominierende keynesianische Theorie waren Phelps' Ergebnisse gewissermaßen schockierend: Die Phillipskurve war zwar wenig theoretisch begründet, spielte aber in der keynesianisch geprägten Wirtschaftspolitik eine wichtige Rolle. Das Ergebnis, dass Geldpolitik in der langen Frist als nachfrageorientierte Wirtschaftspolitik wirkungslos sei, beschränkte das keynesianische Framework. Die Mikrofundierung der Makroökonomie beschritt methodisch gänzlich neue Wege. 

Kurz zusammengefasst kann Phelps' frühes wirken so beschrieben werden: Er entwickelte noch in der Tradition der neoklassischen Synthese die "`Goldene Regel der Akkumumlation"'. Mit seinem Umzug an die University of Pennsylvania emanzipierte er sich aber vom Keynesianismus und es folgten Arbeiten die zur Revolution der Phillipskurve führen sollten. Diese Arbeiten umfassten Konzepte, die bis heute in der Mainstream-Ökonomie State-of-the-Art sind. Erstens, war er ein Vorreiter bei der Mikrofundierung der Makroökonomie und zweitens, etablierte er (adaptive) "`Erwartungen"' in den Modellen der Ökonomie. Beides spielte später bei den "`Neuen Klassikern"' eine wesentliche Rolle, wenn auch in der Form der \textit{rationalen} statt der \textit{adaptiven} Erwartungen. Er nahm also die Kritikpunkte der Neuen Klassiker am Keynesianismus vorweg. Man beachte, dass das bahnbrechende Werk \textcite{Phelps1968} noch vor der Neu-Klassischen Revolution erschien \parencite{Lucas1972, Lucas1976}. Er wurde aber kein Vertreter dieser "`Neuen Klassiker"', sondern war sich immer der Unvollständigkeit der Märkte bewusst, die sich in unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten ausdrückte. Er nahm der Wirtschaftspolitik damit die Illusion einer funktionierenden Feinsteuerung der Wirtschaft, ohne dabei aber einer vollkommenen Marktgläubigkeit zu verfallen. Ich denke man kann ihn daher getrost als eigentlichen Begründer des "`Neu-Keynesianismus"' bezeichnen.

Die eben genannten Arbeiten zur Unvollständigkeit der Märkte, unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten wurden später als Antwort auf die empirischen Defizite der "`Neuen Klassiker"' ausgearbeitet. Sie bilden den Kern der Ersten Generation des Neu-Keynesianismus.


\section{"'Kern"' des Neukeynesianismus}
\label{Kern}

Diese Arbeiten stellen den Kern der Neu-Keynesianer 1. Generation dar, weil hier erstmals die zwei wesentlichen Punkte des Neu-Keynesianismus zusammengefügt werden: Erstens, die Monopolistische Konkurrenz im Zusammenhang mit den Nominalen Rigiditäten und den Menu Costs und zweitens, die Nicht-Neutralität der Geldpolitik aus dem Spannungsverhältnis Nominale vs. Reale Rigiditäten und das daraus ableitbare Nicht-Vorhandensein der Klassischen Dichotomie. \parencite{RomerDavid1993}

\subsection{Nominale Rigiditäten}
\label{Nominale Rigiditäten}

Die Arbeiten von Phelps waren, wie gerade erwähnt, der Ursprung des Neu-Keynesianismus und wurden zeitlich vor der neu-klassischen Revolution formuliert. Die meisten neu-keynesianischen Arbeiten der 1. Generation entstanden allerdings als direkte Antworten auf die aufkommenden aber mit starren Annahmen unterlegten Arbeiten der "`Neuen Klassiker"'.

\textcite{Lucas1976} gab den Anstoß zur "`Neuen klassischen Makroökonomie"' und damit zur Theorie der rationalen Erwartungen. \textcite{Sargent1975} steuerten ihre berühmte "`policy-ineffectiveness proposition"' bei, also die Annahme, dass jegliche Wirtschaftspolitik ohne Effekt verpufft. Bereits 1977 folgten - als direkte Antwort - zwei Arbeiten \textcite{Taylor1977, Fischer1977} von Ökonomen, die eben nicht Teil der "`Neuen Klassik"' sein wollten, aber die Überlegenheit einzelner Elemente daraus akzeptierten. Das realisiert man bereits wenn man nur das Abstract der beiden Artikel liest. Sinngemäß steht da, dass aktive Geldpolitik sehr wohl eine Wirkung haben kann, da Löhne in der kurzen Frist rigide sind. Dies sei unabhängig von der Annahme rationaler Erwartungen. Zwei typisch neu-keynesianische Elemente kommen hier vor. Erstens, das Vorhandensein von (nominalen) Rigiditäten und damit die Wirksamkeit von aktiver Wirtschaftspolitik und somit die Ablehnung der "`policy-ineffectiveness proposition"' und zweitens, die implizite Akzeptanz der Annahme rationaler Erwartungen.
Die Annahme "`Adaptiver Erwartungen"' im Sinne von \textcite{Phelps1968} bedeutet, dass Entscheidungsträger, also zum Beispiel die Zentralbank, für einen Unterschied zwischen tatsächlicher Inflationsrate und erwarteter Inflationsrate sorgen kann. Die "`Rationalen Erwartungen"' nach \textcite{Lucas1976} gehen hier weiter und behaupten, dass es keine Differenz zwischen tatsächlicher und erwarteter Inflationsrate geben kann. Wenn nämlich die Zielinflation (oder zu dieser Zeit noch die Geldmengenziel - "`money supply rule"') bekannt ist, dann wissen die Haushalte ebensogut wie die Entscheidungsträger in welcher Form auf überschießende oder zu niedrige Inflation reagiert wird.  Sowohl \textcite{Fischer1977} als auch \textcite{Taylor1977} akzeptieren die Existenz von Rationalen Erwartungen. Die von den Neuen Klassikern daraus abgeleitete Wirkungslosigkeit von Geldpolitik hingegen lehnen sie hingegen strikt ab. Das Argument dafür ist, dass es langfristige Verträge gibt die Löhne (in \textcite{Fischer1977}), bzw. Preise (in \textcite{Taylor1977}) festsetzen. Geldpolitik hingegen kann laufend vorgenommen werden. Das Ergebnis ist, das diese nominale Lohn- und Preisrigidität, das Geldpolitik in der kurzen Frist sehr wohl wirksam ist. Die beiden genannten Werke gelten heute noch als ein Eckpfeiler der Neu-Keynesianischen Theorie. Das Ergebnis ist in gewisser Weise paradox, denn waren es nicht die "`alten"' Keynesianer, die behaupteten, dass Geldpolitik wirksam ist ("`money matters"'), wenn Preise und Löhne rigide sind? \textcite[S. 166]{Taylor1977} sind sich dessen bewusst. Aber Sie schränken ein, dass die postulierten Zusammenhänge ganz andere waren und nur ihre Theorie mit der Annahme Rationaler Erwartungen vereinbar sei. Oder wie es \textcite[S. 166]{Taylor1977} ausdrücken: \textit{"'By adopting the framework of rational expectations, we hope to have produced not a new wine but an old wine in a new and more secure bottle."'}
Die Akzeptanz der Gültigkeit der Annahme Rationaler Erwartungen ist übrigens bis heute ein Streitpunkt zwischen den "`alten"' Keynesianern (also den Vertretern der Neoklassischen Synthese) und den Neu-Keynesianern, aber auch innerhalb der Gruppe der Neu-Keynesianer. Für die Vertreter der Neoklassischen Synthesen ist diese Annahme schlicht unrealistisch. Innerhalb der Gruppe der Neu-Keynesianer gibt es durchaus auch Zweifler am Konzept der Rationalen Erwartungen.

\textcite{Taylor1979, Taylor1980} verallgemeinerte die Aussagen dahingehend, dass Rigiditäten auch außerhalb der strengen Annahmen fixer Laufzeiten bei Arbeitsverträgen auftreten. In seinem Modell geht er davon aus, dass Verträge gestaffelt neu für eine gewisse Zeitperiode ausverhandelt werden. Das Modell wird dementsprechend als "`Staggered contracts"', oder "`Taylor contracts"' bezeichnet. Im einfachsten Fall kann man davon ausgehen, dass Löhne in Arbeitsverträgen nur alle zwei Jahre angepasst werden. Das heißt jedes Kalenderjahr wird eine Hälfte der Arbeitsverträge an die beobachtete Inflation angepasst. Taylor konnte so zeigen, dass durch diese künstlich modellierte Lohnrigidität eine Abweichung vom langfristigen Gleichgewicht entsteht. \textcite{Blanchard1983} wendete dieses Modell auf die Preissetzung von Waren an und stellte fest, dass bei längeren Herstellungsketten der Effekt der Rigidität größer ist. Diese frühen Modelle der nominalen Rigiditäten waren noch nicht mikroökonomisch fundiert \parencite[S. 194]{Fischer1977}, was auch von Seiten der Neuen Klassiker recht rasch zu entsprechender Kritik führte. Dies wurde später durch die Arbeit von \textcite{Rotemberg1987} behoben. Die Annahme nominaler Lohnrigiditäten (nicht nominaler Preisrigiditäten) wurde später als unzureichend kritisiert \parencite{Mankiw1990}, weil die Reallöhne während Rezessionen steigen würden, was einerseits empirischen Beobachtungen widerspricht und andererseits dazu führt, dass Wirtschaftskrisen bei Personen mit sicheren Jobs sehr populär wären \parencite[S. 371]{Snowdon2005}. Erst in Verbindung mit der Annahme "`Monopolistischer Märkte"' (Imperfekte Märkte), "`Nicht-kostenloser Preisanpassung"' (Menu Costs), und "`Friktionen auf den Arbeitsmärkten"', die allesamt ebenfalls als Neu-Keynesianische Markenzeichen in diesem Kapitel noch besprochen werden, lassen sich rigide Löhne rechtfertigen.
Die nominale Preisrigidität überlebte aber und ist bis heute Teil der aktuellen Mainstream-Modelle! In den  Neu-Keynesianischen DSGE-Gesamtmodellen, die in Kapitel \ref{Neue Neoklassische Synthese} beschrieben werden, wird nominale Preisrigidität mittels "`staggered price setting"'-Modell von \textcite{Calvo1983} modelliert. Er bezieht sich dabei direkt auf die Arbeiten von \textcite{Taylor1979, Taylor1980}, macht daraus aber ein stochastisches Modell. Das heißt, Unternehmen können die Preise und Löhne nicht mehr nach Ablauf einer gewissen Zeitspannen anpassen, sondern erst jeweils nach einem zufällig langem Zeitraum. Das heißt die Preisanpassungen nach einem exogenen Schock finden noch unregelmäßiger statt. Dies bildet empirische Beobachtungen noch besser ab. 

Überzeugende Empirische Evidenz für die Existenz von nominalen Preisrigiditäten konnte man erst mit dem Aufkommen von Mikro-Datensätzen um die Jahrtausendwende erstellen. \textcite{Nakamura2008} fanden heraus, dass nominale Preise etwa neun bis elf Monate im Durchschnitt Bestand haben, dass es also nominale Rigiditäten tatsächlich gibt. Die beiden kritisieren aber auch, dass in den am häufigsten zitierten Modellen von \textcite{Taylor1980} und \textcite{Calvo1983} bei Preisänderungen stets von Preis\textit{erhöhungen} ausgeht. In ihrer empirischen Studie fanden \textcite[S. 1442]{Nakamura2008} hingegen heraus, dass mehr als ein Drittel aller Preisänderungen im Beobachtungszeitraum aber Preissenkungen waren.


Die modelltheoretischen Grundlagen wie man nominale Rigiditäten \textit{berücksichtigen} kann, waren also schon früh durch \textcite{Taylor1977, Fischer1977}, bzw. \textcite{Calvo1983} geschaffen worden, wie soeben dargestellt. Unbeantwortet hingegen blieb bislang die Frage, wie es zu diesen nominalen Rigiditäten überhaupt \textit{kommen kann}. Die Antwort darauf lieferten \textcite{Mankiw1985b, Akerlof1985, Parkin1986, RomerDavid1990} und \textcite{Ball1988}. 

Das erste, berühmt gewordene, Anwendungsbeispiel sind die sogenannten "`Menu Costs"', also "`Speisekarten-Kosten"'. Die Ausgangsannahme ist, dass die Anpassung von Preisen selbst Kosten verursacht. Daraus leitet sich der von \textcite{Mankiw1985b} geprägte Begriff der "`Menu Costs"' ab. Dahinter steht folgende exemplarische Idee: Das Drucken neuer Speisekarten kostet Geld. Gastronomen müssen also abschätzen, ab welchem Ausmaß von Preiserhöhungen Mehrerlöse entstehen, die den Druckkostenaufwand wieder ausgleichen\footnote{Die Idee entstand in den 1980er Jahren, also vor der großen digitalen Revolution. Heute wären diese "`Menu Costs"' im Wortsinn vermutlich wesentlich geringer als 1985.}. Eine sehr einfache Idee, die für die meisten individuellen Unternehmen kaum von Belang ist. \textcite{Akerlof1985, Mankiw1985b, Parkin1986} zeigten aber jeweils, dass diese kleinen individuellen Effekte zu großen makroökonomischen Effekten führen können. \textcite{Rotemberg1987} nannte diese Erkenntnis "`PAYM insights"', angelehnt an die Anfangsbuchstaben der vier Autoren. Konkret führt auf Märkten mit Monopolistischer Konkurrenz (siehe nächstes Kapitel) das individuell Nutzen-maximierende Verhalten der einzelnen Unternehmer\footnote{\textcite[S. 823]{Akerlof1985} nennen es \textit{"'Insignifikant"' suboptimales Verhalten}, bzw. \textit{Nahe-Rationales Verhalten}} dazu, dass Preisanpassungen an den Gleichgewichtspreis erst bei größeren Preissprüngen vorgenommen werden.  Kommt es also zu einem (geringen) Rückgang der aggregierten Nachfrage, werden Unternehmen bei monopolistischer Konkurrenz ihre Preise aufgrund der "`Menu Costs"' zunächst nicht anpassen, sondern stattdessen, trotz niedriger Nachfrage, den ehemaligen Gleichgewichtspreis verlangen. Gesamtwirtschaftlich optimal wäre es, wenn die Unternehmen ihren Preis senken würden. Das würde man auch in der neoklassischen Analyse von Monopolmärkten erwarten. Da Unternehmen aber "`Menu Costs"' ausgesetzt sind, werden die höheren Preis beibehalten. Für Unternehmen ist dieses Verhalten Nutzen-maximierend, weil die Preis-Anpassungskosten höher wären als der zusätzliche Gewinn, den Unternehmen bei geringerem Preis aber höherer abgesetzter Menge, erhalten würden \parencite[S. 372]{Snowdon2005}. Gesamtwirtschaftlich ist das Ergebnis aber suboptimal, weil die abgesetzte Menge beim rigiden Preis viel geringer ist als die theoretische Gleichgewichtsmenge.\footnote{Dieser Ansatz mit Monopolistischer Konkurrenz und de-facto Preissetzung ist nicht unähnlich frühen Post-Keynesianischen Ansätzen (vgl. \ref{Post-Keynes})! Auch wenn Neu-Keynesianer dies nur ungern zugeben würden.} 

Bleibt man auf Ebene eines einzelnen Unternehmens ist die Erkenntnis zwar durchaus interessant, sie scheint aber weitgehend folgenlos für die Gesamtwirtschaft. Das ist aber ein Trugschluss. Die wichtigste Erkenntnis der "`PAYM-insights"' ist, dass kleine - auf natürliche Schwankungen zurückzuführende - Rückgänge bei der aggregierten Nachfrage zu deutlichen Schwankungen beim gesamtwirtschaftlichen Output führen\parencite[S. 375]{Snowdon2005}. Da solche Schwankungen unerwünscht sind, argumentieren \textcite{Akerlof1985}, dass aktive Wirtschaftspolitik\footnote{\textcite[S.837]{Akerlof1985} schreiben konkret nur von Geldpolitik} sehr wohl einen stabilisierenden und damit wünschenswerten Effekt hat.


\subsection{Reale Rigiditäten}
\label{Reale Rigiditäten}

Anfang der 1990er Jahre wurde das Konzept der Rigiditäten verfeinert. Denn zwar konnte durch die "`PAYM-insights"' gezeigt werden, dass es theoretisch möglich ist, dass nominale Rigiditäten zu großen Schwankungen im Gesamtoutput führen, aber eben auch, dass dies in der Realität sehr unwahrscheinliche sei. Wie \textcite[S. 183]{RomerDavid1990} herausarbeiten mussten dazu ganz bestimmte Bedingungen erfüllt sind. So würde es, zum Beispiel, beim Auftreten von nominalen Preisrigiditäten nur dann zu großen Effekten auf den Gesamtoutput kommen, wenn der Arbeitsmarkt gleichzeitig extrem elastisch wäre. Gastronomen würden demnach zwar bei Nachfragerückgängen ihre Preise auf den Speisekarten unverändert lassen, aber gleichzeitig sofort Köche und Kellner kündigen?! Ein eher unrealistisches Szenario. Stattdessen führten \textcite{RomerDavid1990, Ball1988, Ball1989} \textit{reale} Rigiditäten als notwendige Ergänzung zu \textit{nominalen} Rigiditäten ein.

Zunächst muss einmal abgegrenzt werden, wie sich reale Rigiditäten von nominalen Rigiditäten unterscheiden. Nominale Rigiditäten haben wir schon als "`Menu Costs"' kennengelernt. Allgemein könnte man nominale Rigiditäten definieren als zeitlich begrenzte, geringfügige Abweichungen vom Marktgleichgewicht, die allerdings keinen langfristigen Bestand haben und damit keine langfristige ökonomische Begründung. Nehmen wir zum Beispiel die "`Menu Costs"': Die Verkaufspreise nicht bei jeder geringfügigen Änderung der Preise vorgelagerter Waren zu ändern ist rational. Wenn die Preisänderung vorgelagerten Waren aber beständig und/oder groß genug ist, wird es zu Preisanpassungen an das Marktgleichgewicht kommen. Man könnte nominale Rigiditäten auch als die Geschwindigkeit, mit der sich Löhne und Preise an das Gleichgewicht anpassen, definieren \parencite[S. 270]{Blanchard2003}. Bei nominalen Rigiditäten kommt der Effekt der Geldpolitik ins Spiel. Dazu ein einfaches Beispiel: Stellen Sie sich vor Sie möchten einen Apfel kaufen. Dieser sei mit 1EUR/Stück angeschrieben. Was glauben Sie was passiert, wenn in diesem Moment die Geldmenge verdoppelt wird (und alles andere gleich bleibt)? Intuitive Antwort: "`Dann kostet der Apfel 2EUR/Stück"'. Das ist auch intuitiv richtig. Nur was, wenn es eben nicht augenblicklich nach Ausweitungen der Geldmenge zu entsprechenden Preisanpassungen kommt? Dann ist die Geldpolitik eben "`Nicht-Neutral"'. Diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise kann sich die Wirtschaftspolitik zunutze machen und eben "`Geldpolitik"' betreiben. 
Glaubt man, wie die Neuen Klassiker, an die "`Klassische Dichotomie"' zwischen Geldmarkt und Gütermarkt auch in der kurzen Frist, ist diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise nicht vorhanden. Dann gibt es keine wirksame Geldpolitik. Die Neu-Keynesianer hingegen lehnen diese "`Klassische Dichotomie"' zumindest für die kurze Frist ab. Damit akzeptieren sie das temporäre Auseinanderlaufen von realen und nominalen Werten und eben auch die Wirksamkeit von Geldpolitik!

Von realen Rigiditäten spricht man, wenn es rationale Gründe gibt, warum sich Preise auch in der langen Frist nicht an das eigentliche Marktgleichgewicht anpassen. Reale Rigiditäten sind also - im Gegensatz zu nominalen Rigiditäten - kein vorübergehendes Phänomen. Sie bleiben langfristig bestehen, weil die Marktteilnehmer aus verschiedenen, individuell-nutzemaximierenden Gründen, keine Anreize haben von ihrer Preissetzung abzuweichen. Und zwar auch dann nicht, wenn die Preise nicht die allgemeinen Gleichgewichtspreise sind. Mit diesen Gründen beschäftigte sich ein ganzes Forschungsfeld, das in den folgenden Unterkapiteln beleuchtet wird. 

Vorgezogen beleuchten wir an dieser stelle die Arbeit von \textcite{RomerDavid1990}, die in ihrer Arbeit den Unterschied zwischen und die Bedeutung von nominalen und realen Rigiditäten mit den Forschungsfeldern identifizierten. Mit ihren Überlegungen grenzten \textcite{RomerDavid1990} den Neu-Keynesianismus ein weiteres Mal entscheidend als eigene ökonomische Denkrichtung ab und trugen mit der Verbindung der einzelnen Elemente dazu bei, dass der Neu-Keynesianismus als einheitliches Gesamtmodell gesehen werden kann. Davor waren Beiträge stets als ablehnende Antwort gegenüber den Neuen Klassikern entstanden, die aber eher unabhängig voneinander gesehen werden mussten. Betrachten wir den Inhalt dieses, für den Neu-Keynesianismus, wichtigen Beitrags: \textcite[S. 183]{RomerDavid1990} heben gleich zu Beginn hervor, "`dass \textit{reale} Rigiditäten nicht das gleiche sind wie \textit{nominale} Rigiditäten"'. Bis Ende der 1980er Jahre entstanden zwar viele Forschungsarbeiten zu Rigiditäten, diese unterschieden aber nicht zwischen realen und nominalen Effekten. Im nächsten Schritt erstellen die beiden ein interessantes aber komplexes Modell. Dessen Grundaussage lautet wie folgt: Erstens, nominale Rigiditäten ("`Menu Costs"') können realistischerweise nur zu kleinen gesamtwirtschaftlichen Schwankungen führen. Zweitens, reale Rigiditäten können alleinstehend kaum existieren: Auf Märkten ohne jegliche nominale Rigidität, kommt es immer zur Anpassung an das Marktgleichgewicht. Würde man hier aufhören, wäre die Essenz: Rigiditäten spielen keine Rolle. Aber jetzt kommt der Clou aus \textcite{RomerDavid1990}: Treten nominale Rigiditäten auf, so können auch reale Rigiditäten existieren. In diesem Fall verstärken die realen Rigiditäten die nominalen Rigiditäten und es kann zu großen Schwankungen im Gesamtoutput kommen. Diese wiederum rechtfertigen - wie schon im Paper von \textcite{Akerlof1985} - den Einsatz aktiver Wirtschaftspolitik.
Die soeben beschriebenen Ergebnisse sind in \textcite{RomerDavid1990} natürlich nicht bloß als plausibles Narrativ formuliert, sondern aus einem formal-theoretischen Modellrahmen abgeleitet. Die dort dargestellten Beispiele zeigen, dass bei realen Rigiditäten vor allem Arbeitsmarkt-Effekte - in geringerem Ausmaß Gütermarkt-Effekte - große Wohlfahrtsverluste (Schwankungen im Gesamtoutput) verursachen. Das zentrale Beispiel in \textcite{RomerDavid1990} betrachtet ein Modell, in dem der Arbeits- als auch der Gütermarkt berücksichtigt werden, und in dem Rigidität bei Gütermarkt-Preisen durch rigide Reallöhne verursacht werden. Das Modell bedient sich zwei Annahmen. 

Erstens, die Autoren nehmen an, dass Unternehmen "`Effizienz-Löhne"' bezahlen. Das sind Löhne, die etwas höher sind als der Gleichgewichtslohn um die Arbeitnehmer zu besserer Leistung zu motivieren\footnote{Details dazu im Unterkapitel \ref{RR_AM}}. Zusätzlich geht man davon aus, dass Arbeitsmärkte recht unelastisch sind. Das würde implizieren, dass sich Löhne pro-zyklisch verhalten: Bei guter Konjunktur sind freie Arbeitskräfte gefragt aber rar. Durch die Unelastizität der Arbeitsmärkte, müssen die Löhne überproportional angehoben werden, um zusätzliches Personal zu finden. Akzeptiert man die Existenz von Effizienz-Löhnen, kann man erklären wie es gleichzeitig zur Unelastizität von Löhnen und zur Azyklizität der Real-Löhne kommen kann. Beides entsprach in den 1980er Jahren nämlich empirischen Beobachtungen \parencite{RomerDavid1990}. 
Zweitens, funktioniert das Modell nur wenn man annimmt, dass die Rigidität der Reallöhne, Rigidität bei realen Preisen verursacht (und nicht etwa umgekehrt). Dann, und nur dann, tritt nämlich die folgende Wirkungskette ein: Ein Nachfrage-Schock führt nur zu einem geringen Anstieg der Reallöhne und somit zu einem geringen Anstieg der Grenzkosten der Unternehmen. Das heißt, die Unternehmen haben wenig Motivation ihre Preise zu senken. Im Aggregat ist das neue realisierte Gleichgewicht aber dennoch deutlich unterschiedlich vom Marktgleichgewicht bei perfekten Wettbewerb. Der Gesamtoutput fällt im Modell also erheblich. 

Man merkt schon, dass das ganze Modell etwas konstruiert wirkt, wenn auch mit plausiblen Werten und Annahmen. Insgesamt ist der Ansatz der Versuch die zu einfache Sichtweise der "`Neuen Klassiker"' zu durchbrechen. Indem man einzelne, zu beobachtende Marktvorgänge, die nicht dem perfekten Wettbewerb abbilden, modelliert, kommt man zu großen Abweichungen beim Gesamtergebnis. Der Artikel von \textcite{RomerDavid1990} ist auch deshalb so entscheidend für den Neu-Keynesianismus, weil es die drei wesentlichen Elemente, die im Neu-Keynesianismus zuvor jeweils einzeln analysiert wurden, zusammenführt:
\begin{itemize}
	\item Rigiditäten: "`Menu Costs"', aber auch "`Sticky Wages"' verhindern die sofortige Anpassung an den Gleichgewichtspreis.
	\item Nicht Neutralität des Geldes: Abgeleitet durch die akzeptierte Existenz der Rigiditäten, muss es in der kurzen Frist einen Unterschied zwischen nominalen und realen Werten geben. Diese Differenz kann man durch Geldpolitik künstlich steuern. Die "`Klassische Dichotomie"' zwischen Geldmarkt und Realmarkt ist zumindest in der kurzen Frist Illusion
	\item Monopolistische Konkurrenzmärkte: \textit{Die} Neuerung in der \textit{Modellierung} war die Berücksichtigung nicht perfekter Märkte. Die Annahme, dass sich Preise nicht ausschließlich durch Angebot und Nachfrage und einen Walrasianischen Auktionator ergeben, führt überhaupt erst zur Möglichkeit von Rigiditäten. Ist aber im Hinblick auf die meisten Gütermärkte und den Arbeitsmarkt realistischer.
\end{itemize}
Die Einbettung dieses Rahmenwerks in die Modellwelt der Neuen Klassiker, die DSGE-Modelle der "`Real Business Cycle"'-Theorie führte schließlich zum "`Neu-Keynesianismus der 2. Generation"' (Neue Neoklassische Synthese, vgl. Kapitel \ref{Neue Neoklassische Synthese}), der heute noch weitgehend den Mainstream in der Ökonomie darstellt.

Vielleicht haben Sie sich beim Lesen schon gefragt: Okay, es gibt einen Unterschied zwischen den Nominalen Rigiditäten - die verzögerte Anpassung der Preise - und Realen Rigiditäten. Was sind aber jetzt Reale Rgiditäten in der Praxis? Das Ausgangsbeispiel mit den Löhnen, die während Zeiten der Deflation nicht sinken, ist nämlich höchst theoretisch. Deflation trat selbst während der "`Great Recession"' nach 2008 nur sehr vereinzelt auf.
Mit den verschiedenen Quellen Realer Rigiditäten beschäftigen sich die nächsten Unterkapitel. Es ist an dieser Stelle aber darauf hingewiesen, dass die dort vorgestellten Arbeiten nicht ursprünglich als Quellen Realer Rigiditäten identifiziert wurden. Der Entstehungsweg war anders herum: Man versuchte empirisch beobachtbare Abweichungen vom ökonomischen Gleichgewicht zu erklären. Erst \textcite{RomerDavid1990} vereinte dieses Sammelsurium an Erklärungen unter dem Begriff Reale Rigiditäten \textcite[S. 4]{Mankiw1991})


\subsubsection{Unvollkommenheiten am Arbeitsmarkt}
\label{RR_AM}

Der Arbeitsmarkt spielt bei den Neu-Keynesianern eine zentrale Rolle. Erstens, bei der Frage warum Real-Löhne scheinbar deutlich weniger prozyklisch agieren, als in der klassischen Ökonomie angenommen. Diese Frage ist allgemein zentral für die Existenz von Rigiditäten. Der zweiten Frage widmen wir uns hier: Wie kann es zu unfreiwilliger Arbeitslosigkeit kommen?\footnote{Der Arbeitsmarkt spielt, drittens, eine wesentliche Rolle in der Suchtheorie die gesondert in Kapitel \ref{Suchtheorie} behandelt wird.}

Der Arbeitsmarkt wurde in der VWL sehr lange als ein "`gewöhnlicher"' Markt, analog zum Gütermarkt betrachtet. Löhne ergeben sich hier wie Preise aus Angebot und Nachfrage und Treffen sich im Gleichgewicht. Aus heutiger Sicht erscheint es überraschend wie spät in der Makroökonomie erstmals darüber nachgedacht wurde, ob die Produktivität eines Arbeitnehmers mit höheren Löhnen steigen könnte und in der Folge Arbeitgeber einen Anreiz haben, höhere Löhne (als den Gleichgewichtslohn) zu bezahlen? In der Psychologie/Betriebswirtschaftslehre spielten solche Überlegungen früher eine Rolle wie die berühmte Literatur von \textcite{Maslow1943, Herzberg1966, McClelland1961, McGregor1960} zeigt. In der Volkswirtschaftslehre wurde dies erstmals in den 1970er Jahren diskutiert. Wie schon mehrmals beschrieben, zerbrach zu der Zeit der keynesianische Konsens der Makroökonomie. Neben dem Anstieg der Inflation und dem damit verbundenen Ende der Theorie der Phillips-Kurve, war auch ein Anstieg der Arbeitslosenraten zu beobachten. Mit der keynesianischen Theorie war dies nicht in Einklang zu bringen, dort wurde Arbeitslosigkeit schließlich durch eine Unterauslastung der Wirtschaft bei gleichzeitig rigiden Löhnen verursacht. Während einer Deflation steigt dann der reale Wert rigider Nominallöhne. Die realen Löhne wären dann höher als der Gleichgewichtslohn, was in Arbeitslosigkeit resultiert.
In den 1970er Jahren war man aber weit entfernt von Deflation, womit dieser Erklärungsansatz scheiterte. Die Neuen Klassiker machten es sich einfach und behaupteten es gäbe keine unfreiwillige Arbeitslosigkeit. Ein Ansatz, der von Neu-Keynesianern - wie schon in Kapitel \ref{Neue Makro} beschrieben - geradezu lächerlich gemacht wurde. Allerdings hatten die Neu-Keynesianer zunächst keine eigene Erklärungen parat. Erste Ansätze gingen von empirischen Beobachtungen aus: Man konnte sehen, dass selbst in Zeiten hohen Arbeitskräfteangebots kaum ein Unternehmen versuchte die Real-Löhne seiner Mitarbeiter zu kürzen. Dies wäre in der klassischen Theorie eigentlich zu erwarten, denn ein Überangebot an Arbeitskräften sollte gleichzeitig zu einem sinkenden Gleichgewichtslohn führen. Erste Erklärungsansätze gingen folglich in die Richtung, dass die Arbeitnehmer in Gewerkschaften stark organisiert wären und daher wie ein Monopolist auftraten. Die resultierenden zu hohen Löhne verursachten dann Arbeitslosigkeit (QUELLE VVV). Dies war aber mit der Empirie nicht vereinbar sobald man Arbeitsmärkte im Detail betrachtete. So waren in den USA verhältnismäßig wenig Arbeitnehmer Mitglieder in Gewerkschaften.
Weit erfolgreicher war die Effizienzlohn-Hypothese, die zunächst von \textcite{Stiglitz1976b} vorgeschlagen wurde. Diese hat eine Vorbedingung: Es \textit{muss} einen positiven Zusammenhang zwischen der Höhe des Lohnes und der Arbeitsproduktivität des Arbeitnehmers geben. Für diesen Fall zeigt \textcite{Stiglitz1976b}, dass es unter der Prämisse der Gewinnmaximierung rational ist, dass ein Lohn bezahlt wird, der üblicherweise höher ist als der eigentliche Gleichgewichtslohn. Warum? Ein Gewinn-maximierendes Unternehmen wird jenen Lohn bezahlen, bei dem das Verhältnis aus Arbeitsproduktivität und Lohn maximal ist. Steigt die Arbeitsproduktivität zumindest in einem gewissen Bereich überproportional zur Lohnhöhe wird das optimale Verhältnis aus Arbeitsproduktivität und Lohn nicht beim ökonomischen Gleichgewichtslohn, sondern bei einem höheren Lohn erreicht. Dieser Lohn wird "`Effizienzlohn"' genannt.
Ein höherer Lohn als der Gleichgewichtslohn erklärt warum es zu unfreiwilliger Arbeitslosigkeit kommen kann. Der eben beschriebene "`Effizienzlohn"' erklärt zusätzlich warum Arbeitssuchende sich nicht einfach in einen Job hinein-reklamieren können, indem sie ihre Arbeitskraft zu einem niedrigeren Lohn anbieten\footnote{Das "`Insider-Outsider-Modell"' argumentiert hier allerdings, dass die Lohnkosten für neue, unternehmens-externe, Mitarbeiter aufgrund von Kosten der Personalsuche, Einschulungskosten und auch sozialen Kosten, im Sinne von Demotivation durch hohe Fluktuation, in Wahrheit wesentlich höher sind als der Gleichgewichtslohn am Markt. In der Folge ist es nicht so einfach möglich sich in ein Unternehmen hinein zu reklamieren indem man einen niedrigeren Lohn anbietet wie hier beschrieben.} (und somit den eigentlichen Gleichgewichtslohn wieder herstellen würden). Die Arbeitgeber gehen davon aus, dass höhere Löhne gleichzeitig höhere Produktivität bedeuten. Sie wollen daher gar keine Arbeitnehmer die zum niedrigeren Gleichgewichtslohn arbeiten, sondern sie wollen Arbeitnehmer bei denen das Verhältnis zwischen Lohn und Produktivität optimal ist. 
 
So weit so gut. Bleibt die Frage der mikroökonomischen Fundierung: Warum sollte es überhaupt einen Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität geben? Aus praktischer Sicht scheint die Antwort klar und intuitiv und man würde eher die Gegenfrage stellen? Warum soll es \textit{keinen} derartigen Zusammenhang geben? Die rein ökonomische Antwort auf diese Gegenfrage lautet: Weil auf einem perfekten Markt unendlich viele Arbeitgeber unendlich vielen Arbeitnehmern gegenüberstehen. Und wenn ein Arbeitnehmer die im Arbeitsvertrag vereinbarten Pflichten nicht erfüllt, wird er solange gegen einen anderen ausgetauscht, bis die Anforderungen erfüllt werden. Die Realität ist weder so hart, noch so einfach. Daher wurden in der Folge verschiedene Modelle \parencite{Yellen1984} entwickelt, die die Annahme zum Zusammenhang von Lohnhöhe und Arbeitsproduktivität formal plausibilisierten. Fünf davon haben sich als etabliert: 

Der erste Ansatz ist das sogenannte "`Shirking-Modell"'\footnote{"'Shirking"' (engl.) entspricht in etwa dem deutschen "`sich drücken"'. Dementsprechend lautete der nicht geläufige, deutsche Name: "`Drückeberger-Modell"'}. Es basiert, ebenso wie das zweite Modell, auf das in Kürze eingegangen wird, auf dem Problem der Informationsasymmetrie, die wir ja schon im Kapitel \ref{cha: Marktversagen} kennen gelernt haben. Das bekannteste Shirking-Modell ist wohl die Shapiro-Stiglitz-Hypothese \parencite{ShapiroStiglitz1984}. Demnach können Arbeitgeber ihre Arbeitnehmer nicht zu hundert Prozent monitoren. Dementsprechend entsteht ein Moral Hazard Problem. Die Arbeitnehmer können sich dazu entschließen weniger zu arbeiten als im Vertrag vorgesehen. Die Arbeitgeber werden nicht alle Drückeberger identifizieren können, aber jene, die sie beim "`shirken"' erwischen, werden sie kündigen. Auf einem perfekten Wettbewerbsmarkt wäre das aber für den ertappten Drückeberger wenig problematisch. Schließlich gibt auf so einem Markt keine unfreiwillige Arbeitslosigkeit und es wird der Gleichgewichtslohn bezahlt. Das heißt, jeder Drückeberger findet sofort einen neuen Job zu gleichem Lohn.  Bei \textcite{ShapiroStiglitz1984} könnten Arbeitgeber folglich beschließen einen höheren Lohn - den "`Effizienz-Lohn"' - zu bezahlen. Dann hätten Arbeitnehmer nämlich den Anreiz fleißig zu arbeiten. Der sonstige drohende Jobverlust wäre jetzt nämlich problematisch für die Dienstnehmer, da alternativ nur der niedrigere Gleichgewichtslohn bleibt. Wenn aber alle Arbeitgeber so vorgehen und als Anreiz höhere Löhne bezahlen, gibt es ausschließlich den "`Effizienz-Lohn"' am Markt. Da dieser höher ist als der Gleichgewichtslohn wird der Markt nicht vollständig geräumt. Mit anderen Worten: Es kommt zu unfreiwilliger Arbeitslosigkeit. Die Arbeitnehmer haben übrigens in diesem Modell auch dann keinen Anreiz zu "`shirken"', wenn der "`Effizienz-Lohn"' den Gleichgewichtslohn vollständig verdrängt hat. Schließlich droht nun als Konsequenz des erwischt werdens nach wie vor die Kündigung. Da es nun aber unfreiwillige Arbeitslosigkeit gibt, können sich Arbeitnehmer nicht darauf verlassen sofort wieder einen Job zu finden. Zusammengefasst: Das "`Shirking-Modell"' geht also davon aus, dass höhere Löhne bezahlt werden, weil sich die Unternehmen dadurch höhere Arbeitsproduktivität - in der Form von weniger "`shirking"' - versprechen. Folglich bekommen alle Arbeitnehmer einen "`Effizienz-Lohn"' angeboten, der höher als der Gleichgewichtslohn ist. Dies resultiert darin, dass es stabil einen bestimmten Prozentsatz unfreiwillige Arbeitslosigkeit gibt.

Als zweite Erklärung dient das Adverse-Selektions-Modell. Auch hier wird der Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe dadurch erklärt, dass es eine Informationsasymmetrie zwischen Arbeitgeber und Arbeitnehmer gibt. Im Gegensatz zum "`Shirking-Modell"' bezieht es sich aber auf einen Informationsmangel seitens der Arbeitgeber \textit{vor} Vertragsabschluss. Im diesbezüglich am häufigsten zitierte Paper von \textcite{Weiss1980} argumentiert, dass Arbeitgeber mit hohen ausgeschriebenen Löhnen ein Signal an den Markt senden: Wer diesen Job annimmt, muss eine Produktivität in entsprechender Höhe leisten. Arbeitnehmer würden sich demnach ausschließlich auf Stellen bewerben, bei denen das angebotene Gehalt ähnlich hoch ist wie ihr persönlicher Mindestlohn. Unternehmen werden in diesem Modell ihre existierende Lohnstruktur in so einem Fall selbst dann nicht nach unten anpassen, wenn der Gleichgewichtslohn am Markt fällt. Dann nämlich würden sofort die besten Mitarbeiter freiwillig kündigen, da diese auf dem Arbeitsmarkt die besten Jobaussichten haben. Diese Rigidität der Löhne führt insgesamt zu höheren Löhnen als dem Gleichgewichtslohn, was wiederum in unfreiwilliger Arbeitslosigkeit resultiert.

Der dritte Ansatz ist das "`Labor-Turnover-Model"' und findet bereits bei \textcite{Phelps1968} Erwähnung. Also in jenem Journal-Artikel, der einen der ersten Anstöße zum Neu-Keynesianismus darstellt (vgl. Kapitel \ref{micmac}). Ausgangspunkt ist, dass die Suche nach Arbeitnehmern sowie deren Einschulung ein kostenintensiver Prozess ist. Eingeschulte Arbeitnehmer liefern also eine höhere Produktivität als neue Arbeitnehmer. Um Arbeitnehmer nun davon abzuhalten von sich aus zu kündigen, wird ein höherer Lohn als der Gleichgewichtslohn bezahlt. Da alle Unternehmen so agieren, ist der Marktlohn ein "`Effizienz-Lohn"'. Da dieser höher ist als der marktäumende Gleichgewichtslohn, führt dies zu einem gewissen Niveau an unfreiwilliger Arbeitslosigkeit.
Der Wirkungszusammenhang ist damit ähnlich wie beim "`Shirking-Modell, hier zahlen die Unternehmer aber einen höheren Lohn um freiwillige Abgänge zu verhindern, anstatt, wie beim "`Shirking"', Untätigkeit zu vermeiden.

Außerdem gibt es, viertens, das nicht-rationale, Soziologische- oder Fairness-Modell. Gott-sei-Dank wird sich so mancher Leser denken. Schließlich waren die bisher genannten Modelle strikt am neoklassischen Nutzenmaximierer ausgerichtet, mit der Folge, dass das darin gezeichnete Menschenbild ein nicht gerade sehr positives ist. Stichwort "`Shirking"': Der nutzenmaximierende Arbeitnehmer versucht möglichst wenig zu arbeiten. \textcite{Solow1980} hatte als erster argumentiert, dass die Gründe für Lohn-Rigiditäten vielleicht viel einfacher in "`sozialen Konventionen"' oder "`empathischem Verhalten"' zu finden seinen, als in nutzenmaximierenden Verhalten \parencite[S. 204]{Yellen1984}.  \textcite{Akerlof1982} war der erste, der dies systematisch untersuchte. Er startet damit eine Reihe von Publikationen, die er gemeinsam mit seiner Ehefrau Janet Yellen fortsetzte \parencite{Akerlof1984, AkerlofYellen1987, AkerlofYellen1988, AkerlofYellen1990}. Die Inhalte analysieren psychologische und soziologische Gründe dafür, dass Unternehmen höhere Löhne als den Gleichgewichtslohn bezahlen. Akerlof und Yellen heben Faktoren wie Moral, soziale Verantwortung und faire Löhne. Außerdem kritisieren sie, dass menschliche Arbeitskraft nicht wie nicht-menschliche Inputs modelliert werden sollte, da die oben genannten Faktoren eben dazu führen, dass es signifikante Unterschiede in der Bewertung von Menschen wie Maschinen gibt \parencite[S. 392]{Snowdon2005}. Die Arbeit von \textcite{Akerlof1982} war hierfür die erste, die solche Prozesse formalisierte. Es sollte dabei erwähnt werden, dass es Akerlof und Yellen (und anderen Ökonomen) nicht darum geht menschliches Verhalten nicht formal zu modellieren. Sondern im Gegenteil, ihre Modelle sind ebenso formal-mathematisch aufgebaut. Aber es geht darum grundsätzlich anzuerkennen, dass Arbeitnehmer als Menschen negative Gefühle entwickeln, wenn sie sich unfair behandelt fühlen und in der Folge als Konsequenz - also durchaus rational erklärbar - eine geringere Arbeitsproduktivität zeigen. Arbeitgeber wissen dies und bezahlen deshalb einen "`fairen"' Lohn, eben um die Arbeitnehmer bei Laune zu halten. Als Resultat entwickeln \textcite{AkerlofYellen1990} ihre "`fair wage-effort hypothesis"'. Darin optimieren Arbeitnehmer ihre individuelle Nutzenfunktion, indem sie ihre Arbeitsproduktivität an das Verhältnis zwischen Reallohn und als fair empfundenen Lohn anpassen. Dies macht es für Arbeitgeber sinnvoll einen höheren Lohn als den Gleichgewichtslohn zu bezahlen, weil dann eben die Arbeitsproduktivität höher ist. Das Resultat ist das gleiche wie bei den drei zuvor genannten Ansätzen zum Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität: Die über dem Gleichgewichtslohn liegenden "`Effizienz-Löhne"' führen dazu, dass der Arbeitsmarkt nicht vollständig geräumt wird. Mit anderen Worten: Unfreiwillige Arbeitslosigkeit entsteht. 

Eigentlich handelt es sich beim hier dargestellten Teilkapitel nur um ein kleines Rädchen im  Neu-keynesianischen Rahmenwerk. Dennoch ist der Input von Akerlof und Yellen bei Betrachtung des gesamten Neu-Keynesianismus interessant. Kann er doch als der Versuch gesehen werden, im Neukeynesianismus eine verhaltensökonomische Facette zu integrieren und ihn wieder stärker Richtung Keynesianismus auszurichten. Ihren Artikel \textit{Rational Models of Irrational Behavior} schließen \textcite{AkerlofYellen1987} wie folgt: "`The bad press that Keynesian theory has recently received from maximizing, super-rational theory is simply undeserved."' Und die beiden argumentieren weiter, dass die Annahmen der keynesianischen Theorie mit den Ergebnissen der modernen Psychologie und Soziologe übereinstimmen. Auch bei seiner Nobelpreisrede im Jahr 2001 stellte Akerlof \textcite{Nobelpreis-Komitee2001} das Thema "`Verhaltensorientierung in der Makroökonomie"' in den Vordergrund. An dieser Stelle zeigt sich meines Erachtens auch sehr schön einer der Angriffspunkte auf die moderne Makroökonomie: Wir haben nun fünf verschiedene Ansätze gesehen zu denen jeweils dutzende Journal-Artikel geschrieben wurden, die jeweils mit verschiedenen modell-theoretischen Annahmen und formalen mathematischen Methoden versuchen zu erklären, warum es einen Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe gibt!? Nur einer davon, nämlich jener von Akerlof und Yellen, beruft sich auf Argumente, die man aus menschlichem Eigenschaften -  wie Fairness, Neid oder Rache - ableiten muss. Dieses menschliche Verhalten an sich kann man als solches aber nicht modellieren. Keynes hätte dieses Verhalten als "`animal spirits"' zusammengefasst. Alle anderen genannten Ansätze bemühen sich die Vorgänge als Ergebnis individuell-rationalen Verhaltens zu modellieren. Sie wirken etwas konstruiert und weniger natürlich, passen aber dafür perfekt in das formal-mathematische und streng rationale Konzept der modernen Makro-Ökonomie. Letzteres hat sich schlussendlich als Mainstream-Makroökonomie durchgesetzt. Tatsächlich gibt es aber Vertreter des Neu-Keynesianismus, die zwar in dessen Frühphase wichtige Beiträge zu dessen Entwicklung beigesteuert haben, den Übergang zur "`Neuen neoklassischen Synthese"' aber nicht mitgegangen sind. Zu nennen sind hier vor allem Joseph Stiglitz, George Akerlof, Janet Yellen und Paul Krugman. Auf der anderen Seite gibt es Vertreter des frühen Neukeynesianismus, die diesen - gemeinsam mit jungen Wirtschaftswissenschaftlern - zur "`Neuen neoklassischen Synthese"' weiterentwickelt haben. Hier sind vor allem John Taylor, David Romer und Greg Mankiw zu zählen. Gerade Anfang der 1990er Jahre etablierte sich die "`Neue neoklassische Synthese"' (vgl. Kapitel \ref{Neue Neoklassische Synthese}) und die Ansätze von \textcite{AkerlofYellen1990} gerieten eher in Vergessenheit.

\subsubsection{Unvollkommenheiten am Finanzmarkt}
Die Finanzmärkte gelten gemeinhin als jene Märkte, auf denen das klassische Konzept des Perfekten Marktes noch am ehesten zutrifft. Dennoch gibt es offensichtliche Unvollkommenheiten auf diesen Märkten. Die "`Neu-Keynesianer"' entdeckten dieses Forschungsgebiet in den 1980er-Jahren auf verschiedene Weisen. 

Erstens, als Verstärker von Nachfrage-Schocks auf den Realmärkten. Unternehmen, die nur wenig Eigenkapitalpuffer aufweisen, hängen in großem Ausmaß von den Kredit-Märkten ab. Im Fall eines Nachfrage-Schocks müssen diese Unternehmen also auf den Fremdkapitalmärkten aktiv werden um liquide zu bleiben. Grundsätzlich gesunde Unternehmen, die nur über zu wenig Eigenkapitalausstattung verfügten, können im Fall von unvollkommener Information nicht von "`ungesunden"' Unternehmen unterschieden werden. In der Folge hat der Finanzmarkt keine ausschließlich bereinigende Wirkung, sondern auch "`gesunde"' Unternehmen schlittern in die Insolvenz - der Gesamtoutput fällt weiter, die Krise wird durch unvollkommene Finanzmärkte also verstärkt \parencite[S. 13]{Mankiw1991}.
  
Der zweite Grund ist etwas komplexer: Es geht um den direkten Einfluss der Kreditmärkte auf die Gesamtnachfrage über den Transmissionsmechanismus des Geldes: In der Theorie führt expansive (restriktive) Geldpolitik zu sinkenden (steigenden) Zinssätzen. Diese werden über die Finanzmärkte - konkret über die Finanzintermediäre, also konkret Kredite von Banken Unternehmen und Haushalte - weitergegeben und führen wiederum zu mehr (weniger) Konsumausgaben und weniger (mehr) Sparen und damit zu einem neuen Gleichgewicht des Gesamtoutputs. Was aber passiert wenn es auf Ebene der Finanzintermediäre zu Marktunvollkommenheiten kommt? Hier wurden ebenfalls zwei Forschungsansätze sehr bekannt. 

Erstens, jener von \textcite{Bernanke1988}. Die beiden erstellten ein theoretisches Modell, in dem gezeigt wird, dass Verwerfungen beim Transmissionsmechanismus des Geldes zu großen Auswirkungen bei der Gesamtnachfrage führen können. Zuvor schon untersuchte \textcite{Bernanke1983} entsprechende Zusammenhänge konkret für die "`Great Depression"'. Er erweiterte im wesentlichen die Monetaristischen Arbeiten von \textcite{Friedman1968}, die gezeigt hatten, dass die Great Depression durch eine Unterversorgung mit Geld, verschlimmert wurde. \textcite{Bernanke1983} führte aus, dass es in Finanzkrisen nicht genug ist auf Ebene der Zentralbanken für ausreichend Liquidität zu sorgen, sondern, dass auch auf Ebene der Geschäftsbanken weiterhin für Liquidität gesorgt werden muss. Mit anderen Worten: Der Interbankenmarkt, also das Geschäft zwischen Banken, sowie das Geschäft zwischen Banken und Endkunden, muss während Wirtschaftskrisen aufrechterhalten bleiben. Andernfalls fällt die Gesamtnachfrage und die Verwerfungen auf den Finanzmärkten führen zu einer realwirtschaftlichen Krise. Das ist übrigens Wissen, das uns im Jahr 2007 - im Morgengrauen der "`Great Recession"' - wohl vor einer noch schlimmeren Krise bewahrt hat. Die Zentralbanken - allen voran die US-amerikanische Federal Reserve mit Ben Bernanke an der Spitze - verringerten nicht nur die Leitzinssätze und stellten billiges Zentralbankengeld zur Verfügung, nein, auch die Regierungen sagten zu, eventuell ausfallende Bankinstitute aufzufangen. Dadurch stiegen das Misstrauen und die Interbanken-Zinssätze nur für eine sehr kurze Zeit. Das Vertrauen in das Finanzsystem konnte durch die Zusagen aufrechterhalten bleiben und die Finanzmärkte blieben liquide \parencite[S. 13]{Mankiw1991}.
Das eben genannte Rahmenwerk beschrieb die Bedeutung von Verwerfungen auf den Finanzmärkten bei Wirtschaftskrisen. Das zweite Forschungsgebiet behandelte Unvollkommenheiten beim Transmissionsmechanismus im "`Normalbetrieb"'.  \textcite{Stiglitz1981} analysierten dieses Problem einer "`Realen Rigidität"' im eigentlichen Sinn: Empirisch ließ sich in den 1970er Jahren beobachten, dass es eine Übernachfrage nach Fremdkapital gab. Es wurden also weniger Kredite tatsächlich vergeben, als nachgefragt. Man würde eigentlich erwarten, dass bei einer Übernachfrage nach Kapital die Zinsen (als Preis für Kredite) steigen würden und ein Sinken der Nachfrage und/oder Steigen des Angebots zu einem neuen Gleichgewicht führen würden. Auslöser dieser "`Kredit-Rationierung"'\footnote{Wenn dieses Problem während Wirtschaftskrisen auftritt, spricht man meist von Kredit-Klemme (credit crunch).} ist die Informations-Asymmetrie zwischen Bank als Kreditgeber und Haushalten oder Unternehmen als Kreditnehmer. Da die Banken die Zahlungsfähigkeit ihrer Kunden nicht vollständig beobachten können, leiden sie unter einem typischen "`Adversen Selektions-Problem"', wie schon in Kapitel \ref{cha: Marktversagen} dargestellt. \textcite{Stiglitz1981} zeigen mit einem ganz ähnlichen Ansatz wie in \textcite{Stiglitz1976a}, dass Unternehmen, die zum aktuellen Zinssatz keinen Kredit erhalten, nicht einfach einen anbieten können mehr zu zahlen um den Kredit zu erhalten. In diesem Fall käme der Marktmechanismus in Gange, höhere Preise führen zu weniger Nachfrage und einem neuen Gleichgewicht. \textcite{Stiglitz1981} zeigen aber, dass Banken aber gar keine höheren Zinsen anbieten wollen. Sie können nämlich nur unzureichend zwischen Kreditnehmern mit geringer und hoher Ausfallswahrscheinlichkeit unterscheiden. Würden Sie nun allen Kreditnehmern einen höheren Zinssatz anbieten, würden sie erstens, "`bessere"' Risiken abschrecken einen Kredit aufzunehmen, während "`schlechtere"' Risiken den, für sie, fairen Zinssatz bereit wären zu bezahlen. Dies ist der typische Adverse Selektions Effekt. Und zweitens, würden höhere Zinssätze Kreditnehmer anspornen in risikoreichere Projekte zu investieren. Es gäbe also auch einen negativen Anreiz-Effekt. Aus Sicht einer einzelnen Bank ist es also rational Kredite restriktiv nur an Kunden mit gutem Risikoprofil zu vergeben. Als Ergebnis ist zwar die Anzahl der vergebenen Kredite niedriger als die Anzahl der nachgefragten Kredite und der Zinssatz ist niedriger als der Zinssatz, den die Bank verlangen könnte, aber durch das niedrigere Risikoprofil, das damit erzielt wird, agiert die Bank damit dennoch Gewinn-maximierend. Gesamtwirtschaftlich führt dies aber zu dem Ergebnis eines stabilen Gleichgewichts bei Unterauslastung. Der Markt wird nicht zum Gleichgewichtszins geräumt und die Marktkräfte sorgen auch nicht dafür, dass sich der zu niedrige Zins dem Gleichgewichtszins annähert - der Kreditmarkt unterliegt in einem solchen Fall einer realen Rigidität.


\subsubsection{Unvollkommenheiten am Gütermarkt}
Wir haben nun bereits gesehen, dass es gute Gründe gibt warum reale Rigiditäten auftreten. Wir haben bisher mit dem Arbeitsmarkt, sowie dem Finanzmarkt zwei Teilmärkte betrachtet. Nun analysieren wir den Gütermarkt selbst. Warum kann es sein, dass Preise von Gütern und Dienstleistungen vom Marktpreis langfristig abweichen? Das ist natürlich eine ziemlich starke Annahme und sie benötigt drei Voraussetzungen. Erstens, es darf kein "`vollständiger Konkurrenzmarkt"' vorliegen, sondern ein "`monopolistischer Konkurrenzmarkt"'. Eine typische neu-keynesianische Annahme, der wir uns im nächsten Unterkapitel widmen (siehe Kapitel \ref{Monopol}). Zweitens, es muss zu Koordinierungsfehlern kommen. Das heißt, die Beschaffung von Informationen über Preise von alternativen Produkten und das Ausweichen auf diese Alternativen muss einen Aufwand bedeuten. Auch dieser Punkt ist typisch neu-keynesianisch und wird in Kapitel \ref{Suchtheorie} behandelt. Drittens, es muss nominale Rigiditäten geben. Wie bereits in Kapitel \ref{Nominale Rigiditäten} dargestellt, zeigen \textcite{RomerDavid1990}, dass sich nominale und reale Rigiditäten wechselseitig bedingen. 

Wenn diese Voraussetzungen erfüllt sind, dann kann es zu Abweichungen der tatsächlichen Güterpreise von den eigentlich auf einem Wettbewerbsmarkt zu erwarteten Preisen kommen. Wichtig ist, dass dies auch impliziert, dass die tatsächlichen Güterpreise von den Preisen abweichen können, die man bei einem gegebenen Geldangebot erwarten sollte. Mit anderen Worten: Wenn Güterpreise rigide sind, dann ist es möglich, dass sich diese nicht sofort an Geldmengenänderungen anpassen. Das wiederum bedeutet, dass geldpolitische Maßnahmen zumindest kurzfristig zur Steuerung der Wirtschaft eingesetzt werden können. Reale Rigiditäten sind also auch ein wichtige Voraussetzung dafür, die Wirksamkeit der Geldpolitik mit einem theoretischen Rahmenwerk zu unterlegen. 

Wie werden die realen Rigiditäten am Gütermarkt nun aber realisiert? Dazu wird die Gewinnmaximierung-Theorie aus der Mikroökonomie analysiert. Auf einem perfekten Konkurrenzmarkt ist ein einzelner Anbieter Preisnehmer. Die Angebotsfunktion verläuft parallel zur x-Achse. Die Elastizität des Angebots ist unendlich. Das heißt, ein Unternehmer, der einen Gewinn generieren will, indem er auf die Preise eine Aufschlag - häufig auch im Deutschen "`Mark-up"' genannt, wird vom Markt bestraft. Zu einem höheren Preis als den Marktpreis kann man auf vollkommenen Konkurrenzmärkten nichts verkaufen. Jetzt haben wir aber angenommen, dass (auch) die Gütermärkte im "`Neu-Keynesianismus"' monopolistisch sind. Das heißt die Unternehmer können in engem Rahmen (Details im nächsten Unterkapitel) auf den Marktpreis einen Aufschlag verrechnen, der als zusätzlicher Gewinn realisiert wird. Technisch gesehen maximiert ein Unternehmer, der auf einem vollkommenen Konkurrenzmarkt agiert seinen Gewinn indem er solange Güter produziert, solange die Kosten für ein zusätzlich erzeugtes Gut (Grenzkosten) niedriger oder gleich dem Marktpreis sind. Auf einem monopolistischem Konkurrenzmarkt wird der Gewinn maximiert indem man die Differenz zwischen Gesamterlös und Gesamtkosten maximiert (Grenzerlös = Grenzkosten).  Der Vorteil auf dem monopolistischen Konkurrenzmarkt ist, dass die abgesetzte Menge nicht auf Null fällt, sobald man einen höheren Preis als den Gleichgewichtspreis für seine Güter verlangt. Verlangt man einen etwas höheren Preis als den theoretischen Gleichgewichtspreis, befindet man sich zwar nicht mehr im Gewinnmaximum, aber zumindest in der Nähe davon. Der Preis pro Stück wird auf einem monopolistischen Markt etwas höher sein als auf einem perfekten Konkurrenzmarkt. Diese Differenz kann man eben als Aufschlag oder "`Mark-up"' bezeichnen \parencite[S. 379f]{Snowdon2005} \parencite[S. 14]{Mankiw1991}. 

Genau dieser Aufschlag ist eine "`reale Rigidität"' auf Gütermärkten. Die "`Neuen Klassiker"' sehen nur eine Möglichkeit, wie sich sinkende Grenzkosten der Produktion - zum Beispiel in Folge einer Rezession - auswirken können, nämlich in sinkenden Preisen. Auf perfekten Konkurrenzmärkten ist dies tatsächlich die einzige sinnvolle Lösung. Auf monopolistischen Konkurrenzmärkten aber kann es vorkommen, dass sinkende Grenzkosten zu keinen sinkenden Preisen führen. Wenn nämlich die geringeren Kosten durch im gleichen Ausmaß steigenden "`Mark-ups"' ausgeglichen werden, bleiben die Preise konstant. Dass genau das auf Gütermärkten passiert, argumentieren die Neu-Keynesianer und sprechen dann von realen Rigiditäten auf Gütermärkten. Die "`Mark-ups"' müssten in so einem Fall aber antizyklisch verlaufen. Also während einer Rezession steigen und während einer Boom-Phase fallen. Warum das passieren soll war Gegenstand der Forschung in diesem Bereich.

\textcite{Stiglitz1984} identifiziert eine technische Möglichkeit, wie es sein kann, dass die Güterpreise unverändert bleiben, wenn die Grenzkosten (der Produktion) fallen. Wenn nämlich die Elastizität der Nachfrage abnimmt, könnten Unternehmen rational entscheiden ihre Mark-ups zu erhöhen. Die Preise als Summe der Kosten und Mark-ups könnten so auch während einer Rezession konstant bleiben oder sogar steigen \parencite[S. 351]{Stiglitz1984}. Eine technisch elegante Lösung, die die Frage aber nur einfach nur auf eine andere Ebene verschiebt: Warum sollte die Elastizität der Nachfrage in Rezessionen sinken?
\textcite{Stiglitz1984} liefert in weiterer Folge verschiedene Ansätze warum Preise in einer Rezession steigen könnten. Zwei davon bedienen sich der Argumentation, dass unvollkommene Information dazu führen könnte, dass es für Unternehmen in einem Wettbewerbsmarkt vorteilhafter sein könnte die Güterpreise konstant zu belassen oder sogar anzuheben. Erstens, weil man mit höheren Preisen höhere Qualität verbindet, zweitens, weil Kunden sich nicht auf die kostspielige Suche nach alternativen Anbietern machen wollen. Unternehmen würden dann von Preisanpassungen nach unten an das Gleichgewicht nicht profitieren, weil die Kunden ohnehin auch ei höheren Preisen kaufen. Beide Ansätze wirken nicht sehr überzeugend.
\textcite{Stiglitz1984} liefert aber noch einen interessanten Ansatz, der in weiterer Folge einen eigenen Forschungszweig begründen sollte. Er betrachtet die Gütermärkte als oligopolistische Märkte und das Verhalten der Unternehmen analysiert er mit Methoden der Spieltheorie. Der erste dieser Ansätze zur Erklärung steigender Güterpreise während einer Rezession geht davon aus, dass Oligopolisten (oder Monopolisten) zu Beginn einer Rezession natürlich weniger Güter produzieren müssen. Dadurch werden Überkapazitäten in der Produktion frei. Die Überkapazitäten sind ein Abschreckungsmittel gegenüber potentiellen neuen Mitbewerbern. Steigende Überkapazitäten würden folglich steigende Preise erlauben. Der zweite Ansatz geht davon aus, dass Oligopol-Unternehmen explizite oder implizite Preisabsprachen mit ihren Konkurrenten machen. Das heißt, jeder Oligopolist bezieht aufgrund der Vereinbarung einen entsprechend höheren Gewinn und beobachtet gleichzeitig den Markt. Am Markt können aber nur die Preisabweichungen der Konkurrenten, nicht die Nachfrageänderungen beobachtet werden. Preissenkungen können nun zwei Gründe haben, entweder man will aus dem Kartell ausscheren um sich selbst dadurch einen "`Übergewinn"' sichern (gleichzeitig aber damit das bestehen des Kartells zu gefährden), oder man reagiert einfach auf eine sinkende Nachfrage. Da Unternehmer nicht zwischen diesen beiden Gründen unterscheiden können, kann es aus rational sein, selbst dann beim höheren Preis zu bleiben um die rentable Kartellabsprache nicht zu gefährden. Diese spieltheoretischen Ansätze fanden ab den 1980er Jahren immer stärker Eingang auch in die Makroökonomie. 
Insbesondere \textcite{Rotemberg1986} erweiterte die spieltheoretischen Ansätze zur Erklärung realer Rigiditäten. Sie verwendeten hierbei das spieltheoretische Modell, das \textcite{Friedman1971} für mikroökonomische Fragestellungen entwickelten. Demnach kommt es auf Oligopol-Märkten zu impliziten Preisabsprachen, also Preissetzungen über dem Gleichgewichtspreis, obwohl die Konkurrenten sich darüber nicht austauschen, sondern stattdessen nur beobachten. \textcite{Rotemberg1986} wendeten dieses Modell auf das makroökonomische Problem der Realen Rigiditäten an. Die beiden zeigten theoretisch fundiert, dass auf Oligopol-Märkten der Wettbewerb zwischen den Unternehmen in Hochkonjunktur-Zeiten stärker ausgeprägt ist. Für Oligopolisten zahlt es sich nämlich vor allem dann aus die impliziten Preisabsprachen zu brechen, wenn die Nachfrage hoch ist. Dann kann ein einzelner Unternehmer durch Unterbieten des Oligopol-Preises einen hohen individuellen Gewinn erzielen. Freilich zu dem Preis, dass das System impliziter Absprachen zusammenbricht und die konkurrierenden Unternehmen zusammen weniger Gewinn einfahren. Preiskriege finden also vor allem in Hochkonjunktur-Phasen und auf Märkten mit wenigen Anbietern statt \parencite[S. 391]{Rotemberg1986}. Preise auf solchen Märkten sind also - entgegen den gängigen Annahmen - antizyklisch. Im Resultat ergibt sich daraus die Erklärung für die nicht fallenden Preise während Rezessionen. In diesen Phasen sind die Mark-ups nämlich gering und ein Ausscheren aus dem Preiskartell würde keine lohnenden Gewinne bringen. In Rezessionen bleiben die Preise dementsprechend verhältnismäßig hoch. Dem noch nicht genug zeigen \textcite{Rotemberg1986}, dass Preiskriege auf Oligopol-Märkten durch Verflechtungen mit Märkten mit höherem Wettbewerb, insgesamt zu antizyklischen Preisentwicklungen führen.

Das prozyklische Verhalten der Mark-ups ist ein bis heute diskutiertes und beforschtes Thema. So setzen moderne Neu-Keynesianische DSGE-Modelle, sowie noch modernere Heterogeneous-Agent New Keynesian (HANK) Modelle (vgl. Kapitel \ref{Neue Neoklassische Synthese}) die Existenz der Prozyklizität der Mark-ups voraus. Während ältere Modelle den Konjunkturzyklus vor allem mit Reallohn-Rigiditäten erklären, führen die genannten neuesten Modelle (HANK) die beobachteten Nachfrageschocks auf Preis Rigiditäten zurück \parencite[S. 3]{Nekarda2020}. Dabei ist die empirische Bestimmung von Mark-ups ein bis heute nicht endgültig gelöstes Forschungsproblem. Beginnend mit \textcite{Bils1987} gibt es bis heute \parencite[S. 4f]{Nekarda2020} verschiedene Ansätze, wie man die Zyklizität der Mark-ups misst. Leider gibt es dabei auch unterschiedliche empirische Ergebnisse. Ein Beweis, dass das Thema der Rigiditäten auf Gütermärkten ein bis heute in der Makroökonomie aktuelles und umstrittenes ist. 


\subsection{Monopolistische Konkurrenz}
\label{Monopol}
Die Neuen Klassiker etablierten die Mikrofundierung der Makroökonomie, auf die die Neu-Keynesianer aufbauen. So konnte das grundsätzlich mikroökonomische Thema der Marktformen in der Makroökonomie berücksichtigt werden. Die Annahme, dass auf den Märkten generell "`Vollständige Konkurrenz"' ("`Perfekter Wettbewerb"') herrscht, ist grundsätzlich so alt wie die Wirtschaftswissenschaft selbst. Implizit ging bereits Adam Smith davon aus, dass sich auf Märkten eine große Zahl von Anbietern und Nachfragern treffen und einen Gleichgewichtspreis finden. Explizit ausgesprochen und analysiert wurde dies von Leon Walras und seinem Konzept des "`Allgemeinen Gleichgewichts"'. Dieses spielt ja bis heute - vor allem bei den Neuen Klassikern - als "`Walrasianischer Auktionator"' eine große Rolle. In der Mikroökonomik analysierten Gerard Debreu und Kenneth Arrow in den 1950er Jahren das "`Allgemeine Gleichgewicht"' unter Einbeziehung der Finanzmärkte mit modernen mathematischen Methoden. 
Natürlich wusste man aber, dass selbst Märkte, auf denen es sowohl viele Anbieter als auch viele Nachfrager gibt, in vielen Fällen keine perfekten Konkurrenzmärkte sind. Auf perfekten Konkurrenzmärkten herrscht unendliche Preiselastizität. Das heißt die kleinste Abweichung vom Gleichgewichtspreis durch einen Anbieter führt dazu, dass dieser Anbieter schlagartig kein einziges Gut mehr verkauft. Das ist mit unserer tagtäglichen Erfahrung nicht vereinbar. Man versetze sich dazu nur in folgendes Beispiel: Sie gehen in einen Supermarkt und wollen dort Güter des täglichen Bedarfs kaufen. Auf einem vollständigen Konkurrenzmarkt müssten Sie zu jeder Zeit den Gleichgewichtspreis von jedem Gut erheben. Das wäre zeitaufwändig und damit auch teuer. Stattdessen akzeptieren Sie mit dem Besuch im Supermarkt implizit, dass der Verkäufer einen Preis festsetzt. Auch wenn Ihnen manchmal vielleicht bewusst ist, dass Sie eine gute Verhandlungsbasis für einen niedrigeren Preis hätten (z.B.: Das gleiche Gut kostet bei einem Konkurrenten weniger). Der Supermarkt wiederum ist sich seiner Position ebenso bewusst. Er weiß, dass er in einem gewissen Rahmen die Preise wählen kann, ohne dass die Kunden sich sofort abwenden und für ein bestimmtes Produkt in den Konkurrenz-Supermarkt wechseln nur um ein paar Cent zu sparen. Wichtig ist hier die Betonung auf "`in einem gewissen Rahmen"'. Der Name "`Monopolistische Konkurrenz"' ist nämlich etwas irreführend. Die Anbieter auf einem derartigen Markt haben nämlich \textit{keine} Monopolstellung! Im Gegenteil, es handelt sich in der Regel um Märkte mit ausgeprägter Konkurrenz. Allerdings sind die einzelnen Anbieter eben "`Preissetzer"' und nicht "`Preisnehmer"'. Das heißt, der Preis der Produkte wird vom Verkäufer festgesetzt und nicht vom Markt in dem Sinne, dass Abweichungen vom Gleichgewichtspreis sofort zu einem totalen Rückgang der abgesetzten Menge führen. Die Miteinbeziehung von "`imperfektem Wettbewerb\footnote{"Imperfekter Wettbewerb"' und "`Monopolistische Konkurrenz"' wird in der VWL häufig gleichgesetzt. So auch hier. In Wirklichkeit aber umfasst "`Imperfekter Wettbewerb"' mehr, nämlich jegliche Abweichung von perfekten Konkurrenzmärkten, also auch Oligopole oder Monopole}"' berücksichtigt ein Phänomen innerhalb der Volkswirtschaftslehre, das man in der Betriebswirtschaftslehre schon lange kennt: Im Marketing spricht man von "`horizontaler Differenzierung"', das Unternehmen erlaubt unterschiedliche Preise zu verlangen, die auf den ersten Blick exakt die gleichen Bedürfnisse befriedigen. Denken Sie nur an die Preisunterschiede am Automarkt. aus volkswirtschaftlicher Sicht spielt das Thema der Marktformen in der Mikroökonomie eine entscheidende Rolle. Da die Makroökonomie seit der Revolution durch die Neuen Klassiker "`mikrofundiert"' ist, spielen Marktformen auch in der Makroökonomie eine Rolle.  
In der Mainstream-Makroökonomie hatte man aber lange Zeit ein Problem damit, vom Konzept des "`Walrasianischen Auktionators"' abzugehen. Bei Keynesianers und Monetaristen spielten Überlegungen zur Marktform, mangels Mikrofundierung ihrer Modelle, keine Rolle. Aber auch die Neuen Klassiker griffen - übrigens vehement bis heute -  ausschließlich auf das Konzept der perfekten Konkurrenzmärkte zurück. Die Ursprünge der "`Monopolistischen Konkurrenz"' liegen dennoch schon recht weit zurück: Fast zeitgleich veröffentlichten \textcite{Chamberlin1933} und \textcite{Robinson1933} ihre Werke "`The Theory of Monopolistic Competition"' und "`The Economics of Imperfect Competition"'. Die Motivation hinter diesen beiden Werken ist grundverschieden von jener im modernen Neu-Keynesianismus. Beide Arbeiten zielen schließlich rein auf mikroökonomische Überlegungen ab. Aber die grundlegende Idee ist identisch: Nämlich, dass auf einem Markt mit vielen Anbietern und Nachfragern, ein einzelner Anbieter seinen Verkaufspreis in engen Grenzen wie ein Monopolist festsetzen kann. Joan Robinson wurde später zu einer zentralen Figur des "`Post-Keynesianismus"'. In dieser Schule, die eben nicht zum Mainstream gehört,  ist die Berücksichtigung "`Imperfekten Wettbewerbs"' eine zentrale Annahme. Nämlich in der Form eines sogenannten "`Mark-up"'. Mehr dazu im Kapitel \ref{Post-Keynes}. Entsprechend den riesigen inhaltlichen Differenzen zwischen "`Neu-Keynesianern"' und "`Post-Keynesianern"' ist es wenig überraschend, dass erstere es ablehnen, dass das Konzept der "`Monopolistischen Konkurrenz"' von den "`Post Keynesianern"' übernommen wurde. "`Post-Keynesianer haben ein breites Spektrum an Modellen mit Imperfektem Wettbewerb, aber im Detail sind sie nicht sehr ähnlich zu den Neu-Keynesianischen Modellen"' \textcite[S. 439]{Snowdon2005}, meinte Greg Mankiw in einem Interview direkt darauf angesprochen, ob nicht die Post-Keynesianer in diesem Bereich Vorreiter waren? Abgesehen von seiner Aussage, muss man aber doch bemerken, dass das Element der "`Monopolistischen Konkurrenz"' jenes innerhalb der heutigen Mainstream-Ökonomie ist, das am weitesten vom Dogma der "`Marktgläubigkeit"' abweicht. 

Die Idee "`Imperfekten Wettbewerb"' in makroökonomischen Modellen umzusetzen ist \textit{das} Alleinstellungsmerkmal der Neu-Keynesianer schlechthin. Wie bereits erwähnt ist dieses Element nämlich sowohl den Keynesianern, als auch den Monetaristen und den Neuen Klassikern fremd. 
Die Existenz von Monopolistischen Märkten steht in engen Zusammenhang mit den im letzten Kapitel behandelten Rigiditäten. Tatsächlich machen Überlegungen zu Rigiditäten nur dann Sinn, wenn man "`Imperfekten Wettbewerb"' berücksichtigt, andernfalls wären die anbietenden Unternehmen nämlich Preisnehmer und Überlegungen zu Rigiditäten würde jegliche Grundlage fehlen!

Was ist die Motivation der Neu-Keynesianer "`Imperfekten Wettbewerb"' in Makro-Modellen zu berücksichtigen? Was ändert sich dadurch in den Modellen? Und was sind die Auswirkungen auf die Ergebnisse der Modelle? Nun die Motivation war wohl primär empirisch gegeben. Der Neu-Keynesianismus wurde ja praktisch aus der Idee heraus geboren, dass Marktunvollkommenheiten auf Märkten eine wichtige Rolle spielen. Eine Tatsache, die relativ unumstritten ist, aber von den Neu-Klassikern nicht berücksichtigt wurde. Greifen wir das kurz angesprochene Beispiel des Automarktes auf. Ökonomisch ausgedrückt besteht der Zweck eines Automobiles jemand von A nach B zu bringen. Das ist aber nicht mit den empirisch leicht zu beobachtenden Preisunterschieden am Automarkt zu vereinbaren\footnote{Man könnte einwenden der Zweck eines Autos besteht eben nicht nur im Transport von A nach B, sondern auch darin Status zu vermitteln, Sport zu betreiben, etc. Allerdings gibt es Preisdifferenzierungen auch bei fast allen anderen Produkten, die nicht als Statussymbol etc. gesehen werden}. Die Annahme, dass zumeist homogene Güter auf perfekten Konkurrenzmärkten gehandelt werden, erscheint also einfach nicht der Realität zu entsprechen.
Auf Modelle hat die Berücksichtigung entscheidende Auswirkungen. Aus technischer Sicht ändert sich die Nachfragefunktion dahingehen, dass diese nicht mehr perfekt elastisch ist, wie auf "`Perfekten Konkurrenzmärkten"'. Das heißt, Änderungen im Preis einer einzelnen Firma führen zwar dazu, dass die Firma weniger Produkte verkauft, aber nicht mehr schlagartig gar keine Produkte mehr. Die Firmen können daher ihren Gewinn individuell maximieren (Grenzertrag = Grenzkosten). Die Firmen können ihren Preis - wenn auch in einem gewissen Rahmen - frei wählen, sie sind Preissetzer. Das heißt sie müssen nicht sofort auf Änderungen des Marktpreises reagieren um überhaupt noch Produkte zu verkaufen\footnote{Dies ermöglicht erst, dass Preise in irgendeiner Form rigide sind, ist also Voraussetzung für die Aspekte, die wir als "`Rigiditäten"' oder "`Menu Costs"' kennengelernt haben.}. Märkte mit "`perfekter Konkurrenz"' sind somit effizienter als Märkte mit monopolistischer Konkurrenz: Die Gewinn-optimale Output-Menge ist niedriger als auf "`Perfekten Konkurrenzmärkten"'. Die Preise sind etwas höher, womit positive Gewinne möglich sind, was wiederum dazu führt, dass auch nicht-effiziente Firmen überleben können. Diese Aspekte sind rein mikroökonomischer Natur. \textcite{Hart1982} analysierte theoretisch, die Auswirkung der Berücksichtigung "`monopolistischer Konkurrenz"' auf die Ergebnisse makroökonomischer Modelle.

Die Anfänge "`Monopolistische Konkurrenz"' in makroökonomischen Mainstream-Modellen zu berücksichtigten waren schwierig. Ein erstes, rein technisches Modell, \textit{wie} monopolistische Konkurrenz berücksichtigt werden kann, lieferten \textcite{Dixit1977}. Darin zu finden ist ein bis heute gängiges Instrumentarium, noch ohne Bezug zu makroökonomischen Modellen. Erst in den 1980er Jahre wurden deren Modell langsam wiederentdeckt und in makroökonomischen Modellen angewendet. In der Realität war es ja schon seit jeher unumstritten, dass Preise vom Verkäufer festgesetzt werden. Also hat man in frühen neu-keynesianischen Makro-Modellen \parencite[S. 97]{Hart1982} zunächst ebenfalls Preise festgesetzt, mit der Konsequenz, dass die Nachfrage und das Angebot entsprechend schwanken sollten. Das war aber nicht in Einklang zu bringen mit der Annahme rationalen Verhaltens. Aus welchen Gründen sollte zum Beispiel ein Anbieter seine Preise nicht anpassen, wenn er mehr verkaufen möchte? Oliver Hart\footnote{Dies ist tatsächlich jener Oliver Hart, der 2016 den Wirtschafts-Nobelpreis für ein ganz anderes Thema, nämlich die Vertragstheorie, erhalten hat.} war der erste, der im Jahr 1982 dieses Problem umging, indem er eben die Annahme des perfekten Wettbewerbs und die damit verbundenen vollkommenen Elastizitäten, fallen ließ \parencite[S. 110]{Hart1982}. In seinem Modell führen Änderungen der aggregierten Nachfrage zu Änderungen im Gesamt-Output, die nicht sofort durch Änderungen im Preisniveau- und Zinsänderungen sofort wieder ausgeglichen werden. 
Geldpolitik ist damit - entgegen den Annahmen der "`Neuen Klassikern"' - ein wirksames Mittel der wirtschaftspolitischen Steuerung. Ein bahnbrechendes Ergebnis: In Neu-klassischen Modellen mit perfektem Wettbewerb ist es nämlich genau umgekehrt: Änderungen der aggregierten Nachfrage führen zu Preisniveau- und Zinsänderungen, die wiederum dafür sorgen, dass der Output gleich bleibt und sofort nur Preisniveau-Änderungen resultieren. 
Das Ergebnis seines mathematisch-theoretischen Modells war erst der Anfang einer langen Reihen von theoretischen und später auch empirischen Arbeiten im Bereich des "`Imperfekten Wettbewerbs"'. Es zeigte außerdem, dass die Ökonomie bei Unterauslastung zu einem Gleichgewicht finden kann. Eine Erkenntnis, die der "`Neuen Klassik"' ebenso klar widerspricht.  Damit ist Arbeitslosigkeit weder notwendigerweise ausschließlich freiwillig, wie bei den Neuen Klassikern, aber auch nicht ein vorübergehendes Phänomen, wie bei Keynes. Hart zeigt im Paper außerdem, dass budget-neutrale Fiskalpolitik (also die Stimulierung der Wirtschaft durch staatliche Ausgaben, die aber durch Steuern gegenfinanziert wird und somit nicht zu Budgetdefiziten führt) wirksam sein kann. Außerdem schließt er seinen Artikel damit, dass durchaus Keynesianische, aber auch Neoklassische und eben auch Post-Keynesianische Elemente darin vorkommen. 
Die Arbeit von \textcite{Hart1982} lieferte wichtige \textit{theoretische} Ergebnisse. Denken wir noch einmal zurück: Was war die ursprüngliche Motivation der "`Neu-Keynesianer"'? Sie fanden sich in einer Welt, in der die "`Neuen Klassiker"' ein theoretisch überlegenes Modell lieferten. Mit rationalen Erwartungen, Mikrofundierung und mathematisch fundierten Ergebnissen. Alleine die Modellannahmen, insbesondere, dass es auch allen Märkten stets zu effizienten Gleichgewichten käme und in der Folge Geldpolitik und Fiskalpolitik keinerlei Wirksamkeit hätten und die Wirtschaft auf einem stabilen langfristigen Wachstumskurs verläuft, waren unrealistisch. Hier sprangen die Neu-Keynesianer auf den Zug auf: Die Wirklichkeit zeigte deutlich, dass Schwankungen der aggregierten Nachfrage zu Änderungen beim Gesamtoutput führten. Also zu Konjunkturschwankungen, die im ausgeprägten Fall Wirtschaftskrisen darstellten. Die Neu-Keynesianer wollten zwar die Mikrofundierung, die rationalen Erwartungen und die eleganten mathematischen Modelle übernehmen, hielten aber die Modellannahmen für unrealistisch. Und hier kommen nun ihre punktuellen Lösungen ins Spiel: Der soeben vorgestellte Artikel von Hart zeigte, dass es Gleichgewichte ohne "`Perfekten Wettbewerb"' geben konnte - quasi aus rein mathematischer Sicht. Aber sind diese Ergebnisse in der Realität auch wirklich relevant? Dies analysierten \textcite[S. 647]{Blanchard1987}: \textit{"'Monopolistic competition provides a convenient conceptual framework in which to think about price decisions, and appears to describe many markets more accurately than perfect competition. But, how important is monopolistic competition for macroeconomics?"'}. Konkret stellen sich zwei Fragen: Erstens, Kann die Berücksichtigung von "`Monopolistischer Konkurrenz"' tatsächlich erklären warum Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Diese Frage hatte \textcite{Hart1982} eben nur auf theoretischer Ebene behandelt. \textcite{Blanchard1987} analysierten ob der Effekt groß genug sei um empirisch eine Rolle zu spielen. Und sie kamen zu dem ernüchternden Ergebnis, dass dies nicht der Fall sei. Aber \textcite{Blanchard1987} erweiterten ihr Modell und untersuchten, zweitens, ob Interaktionen zwischen mehreren Marktunvollkommenheiten - konkret das auftreten "`Monopolistischer Konkurrenz"' und gleichzeitig das Auftreten von "`Rigiditäten"' -  dazu führen konnte, dass Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Dies konnten die beiden mittels Modell bejahen. Obwohl die beiden Autoren am Schluss ihrer Arbeit \parencite[S. 663]{Blanchard1987} die Limitationen ihrer Arbeit anführen, gehen sie richtigerweise davon aus, dass ihr Beitrag im Speziellen und "`Monopolistische Konkurrenz"' im Allgemeinen hilft, die empirisch beobachtbaren Konjunkturschwankungen zu erklären. Die im Paper vollzogene gemeinsame Analyse von "`Menu Costs"' und "`Monopolistischer Konkurrenz"' zeigt auch, dass makroökonomische Realitäten schwer zu modellieren sind. Es gibt nicht den einen Auslöser von Konjunkturschwankungen. Erst die Kombinationen von mehreren Marktunvollkommenheiten führen zu empirisch ansprechenden Ergebnissen. Manche dieser Marktunvollkommenheiten treten zudem nur wechselseitig auf, bedingen sich also gegenseitig. So wie wir gesehen haben, dass Rigiditäten nur dann sinnvollerweise auftreten können, wenn die Voraussetzungen eines "`perfekten Konkurrenzmarktes"' nicht erfüllt sind. Heutige Mainstream-Makro-Modelle berücksichtigen in der Folge üblicherweise sowohl "`Rigiditäten"' als auch "`Monopolistische Konkurrenzmärkte"'.


\section{Koordinationsversagen}
\label{Suchtheorie}
Schon in einer der ersten als neukeynesianisch zu bezeichnenden Arbeiten wird - in überraschend detaillierter Weise - darauf eingegangen, dass sich auf Märkten in der Realität selten ein perfektes Gleichgewicht einstellt \parencite[S. 683]{Phelps1968} und zwar unabhängig von all den genannten Gründen wie Rigiditäten und Abweichungen vom perfekten Wettbewerbsmarkt. Der Grund ist leichter zu verstehen als theoretisch zu modellieren: Die neoklassische Theorie geht davon aus, dass bei perfektem Wettbewerb der Markt durch einen "`Walrasianischen Auktionator"' vollständig geräumt wird und sich in der Folge im Gleichgewicht befindet. Jeder Nachfrager, der mit dem entsprechenden Gleichgewichtspreis einverstanden ist, findet einen Anbieter. Der Hausverstand sagt uns allen: Das klingt schön und gut, aber die Realität funktioniert nicht so perfekt. Angebot und Nachfrage finden aus verschiedensten Gründen häufig \textit{nicht} zusammen. Die Koordination zwischen Angebot und Nachfrage ist in der Realität ein komplizierter Prozess. Denken wir an den Arbeitsmarkt: Da gibt es ständig gleichzeitig offene Stellen und Arbeitssuchende, die vermeintlich perfekt "`zusammenpassen"' sich aber gegenseitig nicht finden. Man spricht in so einem Fall von "`Koordinationsversagen"' ("`Coordination Failure"').
Grundsätzlich stellt dies auch eine Reale Rigidität dar \parencite[S. 11]{RomerDavid1993} und könnte daher auch teil des letzten Unterkapitel sein. Allerdings geht die Theorie des Koordinationsversagens über die übrigen Ansätze bei Realen Rigiditäten hinaus. Tatsächlich gehen die Theorien mit Rigiditäten nämlich davon aus, dass es grundsätzlich ein einheitliches Marktgleichgewicht gibt, von dem allerdings durch Rigiditäten verschiedener Art abgewichen wird. Die Theorien des Koordinationsversagens hingegen gehen davon aus, dass es mehrere stabile Gleichgewichte geben könnte, von denen allerdings nur eines das Markt-räumende, sogenannte Walras-Gleichgewicht, darstellt. 
Die Arbeiten zur Koordinationsversagen wurden vor allem in den 1980er Jahre verfasst. Die entsprechenden Modelle gehen in verschiedene Richtungen. \textcite{Cooper1988} zum Beispiel verwenden spieltheoretische Ansätze um zu zeigen, dass es auf grundsätzlich kompetitiven Märkten mehrere stabile Gleichgewichte geben kann. \textcite{Woodford1990, Woodford1990b} wiederum, dass es auf Märkten, auf denen mehrere Gleichgewichte existieren, dazu kommen kann, dass die (rationalen) Erwartungen der Nachfrager zu Verschiebungen zwischen den Gleichgewichten führen. Die Erwartungen werden hierbei also zu einer "`Selbsterfüllenden Prophezeiung"'. Beide Modelle haben übrigens den Nebeneffekt, dass sie eine aktive Fiskalpolitik unterstützen \parencite[S. 16]{RomerDavid1993}, etwas dass in Neukeynesianischen Modellen ansonsten eher selten vorkommt. Fiskalpolitik im Sinne von keynesianischen, staatlichen Investitionen, könnten einen Markt nämlich von einem ineffizienten Gleichgewicht zum Walras-Gleichgewicht führen. Warum auch immer, diese Ansätze verschwanden nach und nach aus der Mainstream-Ökonomie. 

Ein anderer Zweig der Theorien des Koordinationsversagens fand hingegen Eingang in die modernen Mainstream-Modelle: Zur Erklärung von Arbeitslosigkeit wurden sogenannte Such- und Matchingmodelle herangezogen. Die Modelle berücksichtigen Koordinationsversagen und begründeten damit einen ganz neuen Zugang zur Modellierung von Unterbeschäftigung.  Diese Modelle sind heute, nach wie vor, State-of-the-Art bei deren Erklärung in Form der sogenannten "`Friktionalen Arbeitslosigkeit"'. 
Modelle der "`Suchtheorie"' waren grundsätzlich mikroökonomisch motiviert. Etwas genauer werden sie als "`Theorien der Friktion auf Suchmärkten"' bezeichnet. In ihrer Anwendung in der Makroökonomie spricht man dann meist von der "`Matching Theory"'. Obwohl die Theorien auf Fragestellungen verschiedenster Konzepte anwendbar sind - zum Beispiel auch den Heiratsmarkt - verbindet man meist vor allem eine Arbeitsmarkttheorie damit.
Als Ausgangspunkt kann das "`Diamond-Coconut"'-Modell herangezogen werden. Hierbei gibt es auf einer Insel ausschließlich auf Palmen wachsende Kokosnüsse als Nahrungsquelle und zusätzlich die Vorgabe, dass niemand eine Kokosnuss essen darf, die er selbst geerntet hat. Es ist also die Suche nach einem Handelspartner, der ebenfalls eine Kokosnuss geerntet hat, notwendig, wenn man was essen will. Das ernten einer Kokosnuss erfordert das mühsame Erklettern einer entsprechenden Palme. Damit werden die "`Suchkosten"' dargestellt. Bevor man Handel tätigen kann, muss man also Kosten und Mühen (ökonomisch: irgendeine Form von Nutzenverlust) auf sich nehmen. Die Koordination zwischen den beiden Marktseiten verläuft schließlich auch in der Realität nicht ohne Kosten und Reibung: Die arbeitsökonomischen Grundannahmen sehen wir folgt aus: Sowohl Arbeitnehmer als auch Arbeitsplätze sind sehr heterogen. Das heißt, die Aufgabe eines bestehenden Arbeitsverhältnisses und die gleichzeitige Suche nach einem neuen Dienstnehmer bzw. Arbeitsplatz ist mit Zeit und Kosten verbunden. Alleine diese Tatsache begründet schon die Existenz einer gewissen Arbeitslosigkeit, nämlich für die Zeit zwischen Aufgabe des alten Arbeitsplatzes und Aufnahme des neuen Dienstverhältnisses. Diese Annahmen sind wesentlich komplexer als die Annahme eines Walrasianischen Arbeitsmarktes auf dem ein Arbeitsplatz bzw. ein Arbeitnehmer jederzeit und kostenlos gegen einen anderen, identischen ausgetauscht werden kann. Aber die Annahmen bilden eben auch wesentlich besser die Realität ab. 

Wissenschaftlich ausformuliert wurde ein Modell hierfür erstmals von \textcite{Diamond1982}, der damit die Grundlagen für ein Konzept schuf, das heute als das \textit{Diamond-Mortensen-Pissarides}-Modell (DMP-Modell) bekannt ist\footnote{Die drei soeben genannten Ökonomen erhielten 2010 gemeinsam den Ökonomie-Nobelpreis.}. Ob zwei Marktteilnehmer in diesem Modell zueinander finden, wird durch einen Zufallsprozess bestimmt, der durch eine Poisson-Verteilung abgebildet wird. Diese Art der Modellierung wurde von \textcite{Mortensen1978} erstmals angewendet und erwies sich als ein wichtiger Baustein in der "`Matching Theory"'. Darauf aufbauend wird eine "`Matching Funktion"' erstellt. Durch diese wird mittels Poisson-Prozess simuliert, dass jeder Arbeitnehmer in zufälligen Zeitabständen auf einen grundsätzlich passenden Arbeitgeber trifft. Das heißt, die Parteien finden nicht sofort ohne Zeitverzögerung zueinander, sondern es gibt sogenannte Such-Friktionen, die die Arbeitslosigkeit verursachen. Kommt es zu Vertragsverhandlungen dann können beide Parteien dem jeweiligen Gegenüber zusagen oder absagen und weitersuchen. Grundsätzlich wäre der Abschluss eines Vertrages für beide Seiten immer von Vorteil, andernfalls wäre es gar nicht zu einem "`Match"' gekommen. Das heißt, der Job bietet dem Arbeitnehmer einen Lohn, der höher als das Arbeitslosengeld (oder sein Reservations-Lohn) ist und der Arbeitnehmer liefert eine Leistung, der dem Unternehmen einen höheren zusätzlichen Umsatz liefert, als der Lohn Kosten verursacht \parencite{Pissarides1985}. Dennoch kommt es nicht automatisch zu einem Vertrag, denn die beiden Parteien müssen sich auf einen Lohn einigen, der beide zufriedenstellt. Ist das nicht der Fall, hofft zumindest eine der Parteien offensichtlich einen besseren Vertragspartner zu finden. Zusätzlich wird berücksichtigt, dass auf dem Arbeitsmarkt laufend Jobs vernichtet werden und andere Jobs neu entstehen \parencite{MortensenPissarides1994}. Dies gibt dem System eine zusätzliche dynamische Komponente. 

Das komplizierte mathematische DMP-Modell wurde und wird bis heute erweitert \parencite{Rogerson2005}\footnote{Der Artikel liefert einen tollen Überblick über die gesamte Literatur}. Das Konzept der natürlichen Arbeitslosigkeit wurde damit um einen Erklärungsansatz erweitert. Welchen Anteil an der Arbeitslosenquote die friktionale Arbeitslosigkeit hat, kann kaum festgestellt werden.


\section{Der Neu-Keynesianische Konjunkturzyklus}

Wie spielen die nun dargestellten neukeynesianischen Elemente nun zusammen, wenn ein konkreter Schock auftritt? Nehmen wir als Beispiel an, das Geldangebot wurde sprunghaft zurückgehen. Festgehalten sei nur zum Vergleich: Wir wissen, dass die Neuen Klassiker davon ausgehen, dass dies keine Auswirkung auf das Gleichgewicht von aggregiertem Angebot und aggregierter Nachfrage haben würde, da das aggregierte Angebot - auch in der kurzen Frist - vollkommen unelastisch auf Geldmengenänderungen reagiert. Die einzige Auswirkung wäre ein Sinken des Preisniveaus. Das reale Gleichgewicht bliebe gleich. Die Neu-Keynesianer gehen zwar \textit{langfristig} auch von der Neutralität des Geldes aus, \textit{kurzfristig} reagieren die Märkte hingegen sehr wohl auf Geldmengenschocks. Bei den Neukeynesianiern bliebe dementsprechend das Preisniveau bei einem sprunghaften Rückgang der Geldmenge zunächst rigide, also konstant. Der Grund liegt in der oben beschriebenen Kombination aus "`Menu Costs"' und realen Rigiditäten. Folglich muss - bei konstantem Preisniveau aber niedrigerer verfügbarer Geldmenge - die aggregierte Nachfrage sinken. Das reale gesamtwirtschaftliche Gleichgewicht, also der reale Gesamtoutput, würde nun ebenfalls zurückgehen. In der Folge sinkt die Nachfrage nach Arbeitskräften. Da auch die Real-Löhne als kurzfristig rigide angenommen werden, benötigen die Unternehmen, bei nun geringerem Output, weniger Arbeitskräfte. Das Resultat ist unfreiwillige Arbeitslosigkeit. In der langen Frist steigt der Preisdruck aufgrund des verringerten Geldangebots und die Marktkräfte würden das ursprüngliche Gleichgewicht wieder herstellen. In der langen Frist unterscheidet sich das Ergebnis bei Neu-Keynesianern und Neuen Klassikern also nicht \parencite[S. 398]{Snowdon2005}. In der kurzen First hingegen sind monetäre Schocks - wie gezeigt - eben nicht ohne Auswirkung. Im Umkehrschluss sollte aktive Geldpolitik betrieben werden, um die monetären Schocks auszugleichen.

\section{Wirkung und Bedeutung des Neu-Keynesianismus}

Der Neu-Keynesianismus ist als Antwort auf den Aufstieg der Neuen-Klassik entstanden. Man könnte es so formulieren, dass die Neu-Keynesianer dem Keynesianismus nahe stehen, insbesondere dessen zentrale Aussage, dass die Wirtschaft zumindest in der kurzen Frist von ihrem natürlichen Gleichgewicht abweicht. Das impliziert, dass Geld- und Fiskalpolitik wirksam eingesetzt werden können. Vom methodischen Ansatz her waren die Neu-Keynesianer aber von den Ideen der Neuen-Klassiker überzeugt. Die Mikrofundierung, die Rationalen Erwartungen und die streng formalen Modelle der Neuen Klassiker wurden von den Neu-Keynesianern gerne angenommen. Das geradezu perfekte Funktionieren der Märkte, welches die Neuen Klassiker in ihren Theorien voraussetzten, war vielen Ökonomen allerdings ein Dorn im Auge. Insbesondere, weil die Empirie zeigt, dass Märkte alles andere als perfekt funktionieren. Dementsprechend ist den Neu-Keynesianismus auch gewachsen und alles andere als eine in sich geschlossene Schule. Punktuell wurden aus verschiedenen Richtungen Unzulänglichkeiten der neu-klassischen, aber auch der keynesianischen Theorien, aufgenommen und erklärt.
Zwei unterschiedliche Wege erwiesen sich hierbei als besonders interessant.
\begin{itemize}
	\item Eine Gruppe beschäftigte sich mit Rigiditäten, also "`Starrheiten"' in irgendeiner Form, die verhinderten, dass sich das natürliche Gleichgewicht rasch einstellt. Zunächst wurden diese auf den verschiedenen Märkten - Güter-, Finanz- und Arbeitsmarkt - recht unabhängig analysiert. Erst \textcite{RomerDavid1990} machten die wichtige Unterscheidung zwischen \textit{realen} und \textit{nominalen} Rigiditäten. Reale Rigiditäten können zum Beispiel von Gewerkschaften durchgesetzte Löhne sein, die über dem Markträumungsniveau liegen und so Arbeitslosigkeit verursachen. Dies würde aber nicht erklären warum der reale und der nominale Geldwert auseinanderlaufen. Das können nur nominale Rigiditäten erklären. Zum Beispiel "`Menu Costs"' als Ausprägung starrer Preise. Diese sind aber normalerweise eher gering und können große Schwankungen im Konjunkturzyklus nicht erklären. \textcite{RomerDavid1990} zeigten aber gleichzeitig wie sich beide gegenseitig bedingen um doch für Konjunkturschwankungen verantwortlich gemacht werden zu können. Der große Forschungszweig bezüglich Rigiditäten erklärte also warum monetäre Schocks sehr wohl eine Rolle spielen und im Umkehrschluss aktive Geldpolitik zur Konjunktursteuerung eingesetzt werden soll. Ökonomen bezeichnen diesen Zustand als "`Fehlende Klassische Dichotomie (zwischen Realmärkten und Finanzmärkten)"', die Folge daraus bezeichnen Ökonomen gerne als "`Nicht-Neutralität des Geldes"'.
	\item Die zweite typisch neu-keynesianische Forschungsrichtung behandelt die nicht perfekten Märkte. Also Märkte, die nicht zu einem einheitlichen Gleichgewicht im Sinne von Walras finden. Dies passiert durch Koordinationsversagen, oder - häufiger - durch fehlende perfekte Konkurrenzmärkte ("`Imperfect Competition"'). Die meisten Anbieter hätten also eine gewisse Marktmacht und agierten demnach auf "`Monopolistischen Konkurrenzmärkten"'.
\end{itemize}
Diese beiden Punkte zeichnen den Neu-Keynesianismus aus und grenzen ihn sowohl gegenüber dem Keynesianismus und dem Monetarismus ab, als auch gegenüber der Neu-Klassik ab. Dennoch ist der "`Neu-Keynesianimus"' keine einheitliche Schule. So gibt es zum Beispiel innerhalb der Neu-Keynesianer kritische Stimmen gegenüber dem Konzept der rationalen Erwartungen. Außerdem sind so inhaltlich unterschiedliche Ökonomen wie Joseph Stiglitz und Paul Krugman, aber auch Gregory Mankiw und John Taylor Vertreter des Neu-Keynesianismus. Die letztgenannten gehören zur jene Gruppe von Ökonomen, die den Neu-Keynesianismus schließlich ab Anfang der 1990er Jahr zur sogenannten "`Neuen neoklassischen Synthese"' weiterentwickelten (vgl. Kapitel \ref{Neue Neoklassische Synthese}).
Abschließend sei noch erwähnt, dass Fiskalpolitik interessanterweise im Neu-Keynesianismus praktisch keine Rolle spielt. Dies ist insofern bemerkenswert, als im Keynesianismus Fiskalpolitik \textit{das} zentral Steuerungselement der Wirtschaftspolitik ist. Dafür gibt es verschiedene Gründe. Erstens, akzeptieren die Neu-Keynesianer die Annahme zufälliger Konjunkturschwankungen und halten das keynesianische "`Fine-Tuning"' daher ab, stattdessen sollte Fiskalpolitik nur in absoluten Ausnahmefällen, wie ausgeprägten Wirtschaftskrisen, eingesetzt werden. Zweitens, reagiert Politik in der Regel langsamer bei der Umsetzung von Maßnahmen als die Zentralbank, die entstehenden Lags können dazu führen, dass Fiskalpolitik unerwünscht wirkt. Drittens, Grundsätzlich akzeptieren die Neu-Keynesianer - im Gegensatz zur Neuen Neoklassischen Synthese, wo das schon umstritten ist - dass Staatsausgaben kurzfristig positiv wirken. Allerdings verursachen sie langfristig Kosten und sind dann in Form von Staatsverschuldung schädlich für Wachstum \parencite[S. 447]{Snowdon2005}, eine Überzeugung, die Anfang der 1990er-Jahre durch den Zeitgeist verstärkt wurde.