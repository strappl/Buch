%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neu-Keynesianismus} \label{cha: Neu Keynes}

Der Neu-Keynesianismus ist leider (noch) wesentlich schwieriger von anderen Schulen abzugrenzen als etwa der Keynesianismus oder der Monetarismus. Dies gilt sowohl in inhaltlicher Sicht, also auch in zeitlicher Sicht. Inhaltlich lässt sich der Neu-Keynesianismus am ehesten negativ abgrenzen. Einige Ökonomen erkannten, dass die Theorien der Keynesianer nicht mehr zureichend waren. Sie akzeptierten aber auch nicht die starren Annahmen der "`Neuen Klassiker"', diese Ökonomen könnte man "`Neu-Keynesianer"' nennen. Die "`Neu-Keynesianer"' sind dementsprechend keine geschlossene Gruppierung von Ökonomen, sondern behandelten eher zerstreut einzelne brennende Fragen der Ökonomie. Dazu gehörten zum Beispiel Fragen der Inflation (Phillips-Kurve), der Arbeitslosigkeit (Suchproblem, Natürliche Arbeitslosigkeit) und des Marktversagens (Informationsasymmetrien, Natürliche Monopole). 
Auch was die Personen betrifft ist die Abgrenzung schwieriger. So muss der Erzliberale \textsc{John Taylor} - Präsidenten der Mont Pelerin Society von 2018 - 2020 - inhaltlich zweifelsohne als ein früher Vertreter des Neu-Keynesianismus gesehen werden. 

Positive Definition:
- Mikrofundierung (Abgrenzung zu Keynes)
- Rationale Erwartungen (Abgrenzung zu Keynes)
- Rigiditäten auf Märkten --> Wirksamkeit der Geldpolitik (Abgrenzung zur Neu Klassik)
- Monopolistische Konkurrenz, Imperfekte Märkte (Abgrenzung zur Neu Klassik)


Die schwierigste Abgrenzung erfolgt aber in zeitlicher Hinsicht. Schließlich wird die heutige Mainstream-Ökonomie häufig als "`Neu-Keynesianismus"' bezeichnet. In dieser Logik müsste man den "`Neu-Keynesianismus"' zumindest in zwei Generationen teilen. Zweifelsohne beginnt der "`Neu-Keynesianismus"' nämlich als Antwort auf die "`Neue Klassische Makroökonomie"' ab den frühen 1980er Jahren zu existieren. In Wahrheit sogar schon deutlich früher, nämlich mit der Kritik an der Phillips-Kurve ab Mitte der 1960er Jahre. Dieser "`frühe"' Neu-Keynesianismus wird an dieser Stelle beschrieben und dauerte bis etwa Ende der 1980er Jahren. Die Hauptproponenten sind hier \textit{Edmund Phelps, Peter Diamond, Joseph Stiglitz, George Akerlof, Michael Spence, John Taylor und Olivier Blanchard}. Diese Schule war der Gegenpol zur aufstrebenden "`Neuen Klassischen Makroökonomie"' um Lucas, Sargeant und Barro. Die Vertreter dieses frühen Neu-Keynesianismus liefern mit ihren Arbeiten vor allem "`Aufweichungen"' der zu starren Annahmen der "`Neuen Klassiker"'. Sie lehnen in diesem Sinn die Arbeiten der "`Neuen Klassiker"' ab, akzeptieren aber auch, dass der Keynesianismus veraltet ist. Von der Zuordnung der Personen her entwickelte sich der "`Neu-Keynesianismus"' eher aus den Salzwasser-Universitäten (vgl. die entsprechende Einteilung in Kapitel \ref{Neue Makro}), die die Neuen Klassiker ja strikt - und nicht nur auf inhaltlicher Ebene - ablehnten. Es wurden aber nicht alle Keynesianer zu "`Neu-Keynesianern"': James Tobin zum Beispiel bestand darauf ein "`Alt-Keynesianer"' zu sein \parencite[S. 45ff]{Tobin1993}. Edmund Phelps drückte dies so aus: "`I [had] warm personal relations with Jim [James] Tobin and Bob [Robert] Solow as well as with Bob [Robert] Lucas and Tom [Thomas] Sargent – relations that have survived our differences. But I belonged to neither school." \parencite{Phelps2006}.

Ab Anfang der 1990er Jahre kam es zunehmend zu einer Verschmelzung von "`Neu-Keynesianismus"' und "`Neuer Klassischer Makroökonomie"'. Diese wird in diesem Buch als "`Neue Neoklassische Synthese"' im nächsten Kapitel beschrieben. Da es eher eine Verdrängung der "`starren"' Neuen Klassischen Makroökonomie durch junge Vertreter des "`Neu-Keynesianismus"' ist, wird sie aber häufig auch einfach "`Neu-Keynesianismus"' genannt\footnote{Man könnte sie auch Zweite Generation des "`Neu-Keynesianismus"' nennen}. Die Hauptvertreter sind hier \textsc{John Taylor}\footnote{der aber auch zur ersten Generation der Neu-Keynesianer gezählt werden muss} \textsc{David Romer, Greg Mankiw, Paul Krugman und Jordi Gal\i}. Der Unterschied zwischen der ersten Generation der Neu-Keynesianer und der zweiten Generation ("`Neue Neoklassische Synthese"') ist, dass die Letztgenannte vor allem die Methoden der "`Neuen Klassiker"', insbesondere "`Dynamische Stochastische General Equilibrium"'-Modelle aus der "`Real Business Cycle"'-Theorie übernommen hat und um ursprünglich keynesianische Elemente, nämlich "`Monopolistische Konkurrenz"', "`Rigide Löhne und Preise"' und "`Nicht-Neutralität der Geldpolitik"' (und Fiskalpolitik) in der kurzen Frist, übernommen hat. Mehr dazu aber im nächsten Kapitel

Allgemein aber täuscht der Name "`Neu-Keynesianismus"' auf jeden Fall: Er ist nicht etwa eine Weiterentwicklung des Keynesianismus. Schon die hier beschriebene "`Erste Generation der Neu-Keynesianer"' akzeptierte inhaltlich und methodologisch die Fortschritte durch die "`Neuen Klassiker"', bestand aber auf der Bedeutung von Fiskal- und vor allem Geldpolitik, sowie der Existenz von Marktversagen. 

 
 Unterschiede Neu-Keynes vs. Neu-Klassik: (Romer-Paper, Snowdon S. 363)
 
 Klassische Dichotomie (Kein Unterschied zwischen Real und Nominal, weil Anpassung)
 Monopolistische Konkurrenz vs. Walrasianischer Auktionator
 
 
 
 
 
\section{Mikroökonomische Vorläufer: Marktversagen} \label{cha: Marktversagen}

\subsection{Informationsasymmetrie}
Akerlof: Market for Lemons
Stiglitz: Adverse Selektion auf Versicherungsmärkten (Screening)
Moral Hazard - Signalling (Spence)
Eventuell: Shiller Finanzmarktinstabilitäten

\subsection{Natürliche Monopole}
Natürliche Monopole oder Baumol's angreifbare Märkte



 
 
 

\section{Phelps: Mikrofoundation der Makroökonomie}
\label{micmac}

Man findet wohl kaum einen Namen, der den Übergang von "`Keynesianismus"' zu "`Neu-Keynesianismus"' besser repräsentiert als \textsc{Edmund Phelps}. Ökonomisch geprägt wurde er in einem eindeutig keynesianischen Umfeld: Er verfasste bei James Tobin seine Dissertation und arbeitete Mitte der 1960er Jahre mit Robert Solow, Paul Samuelson und Franco Modigliani zusammen. Also alles eindeutig keynesianische Ökonomen, die wir aus Kapitel \ref{Synthese} kennen. Laut seines autobiografischen Artikels \textcite[S. 93]{Heertje1995} war diese Zeit, inklusive Gastprofessur am Massachusetts Institute of Technologie (MIT), die prägendste seiner Karriere. Er selbst war innerhalb weniger Jahre ein international anerkannter Ökonom. Schon 1961 veröffentlichte er sein erstes bedeutendes Werk: \textit{The Golden Rule of Accumulation} \parencite{Phelps1961}. Ein bemerkenswerter Artikel, den der gerade mal 28-jährige Phelps im American Economic Review veröffentlichte. Gerade einmal sieben Seiten lang, beginnt dieser - so wie im Englischen normalerweise Märchen  - mit "`Once upon a time"'. In weiterer Folge wechseln sich mathematische Formeln mit Dialogen zwischen dem König und dem Volk der Solovians ab \parencite[S. 640]{Phelps1961}. So witzig und amüsant die Geschichte des Artikels, so bahnbrechend ist auch deren Inhalt. Diese Arbeit kann als direkter Anschluss an die Wachstumstheorie Solow's gesehen werden und im Zentrum steht folgende hypothetische Überlegung: Wenn die gesamte aktuelle Wirtschaftsleistung für die Investition (Investition = Sparen!) in neue Produktionsgüter verwendet wird, dann wird nichts für den aktuellen Konsum ausgegeben. Wird hingegen die gesamte aktuelle Wirtschaftsleistung für Konsum verwendet, werden im Umkehrschluss keinerlei neuen Investitionen getätigt. Beide Extrembetrachtungen führen also zu keinem sinnvollen Gleichgewicht. Das heißt aber auch, dass dazwischen irgendein optimales Verhältnis zwischen Sparen/Investieren auf der einen Seite und Konsumieren auf der anderen Seite bestehen muss. Dieses erreicht man eben durch \textit{The Golden Rule of Accumulation}. Diese wird erreicht - solange man einige vereinfachenden Annahmen zulässt - wenn die Wachstumsrate des BIPs dem Zinssatz entspricht. Bereits Phelps nannte diese natürliche Wachstumsrate "`nachhaltig"' \parencite[S. 638]{Phelps1961}. Weiters zeigt Phelps formal, dass diese Wachstumsrate erzielt wird, wenn die Summe der Investitionen der Summe der Profite entspricht, also alle Profite investiert werden. Umgekehrt werden im Optimum alle Löhne konsumiert. Zusammengefasst: Wenn alle Löhne konsumiert werden und alle Profite investiert werden, befindet sich die Ökonomie auf einem nachhaltigen Wachstumspfad. Die Wachstumsrate entspricht dann dem Zinssatz. Insgesamt erinnert das Ergebnis an die Arbeiten von Wicksell und Hayek. Die formale Herleitung durch Phelps war aber zu diesem Zeitpunkt - im Jahre 1961 - eine bahnbrechende Erweiterung des Solow-Wachstumsmodells.

Das bisher in diesem Unterkapitel dargestellte, entspricht noch vollständig dem keynesianischem Denken aus Kapitel \ref{Synthese}. Im Jahr 1966 wechselte Phelps von Yale an die University of Pennsylvania (Penn). Mit dem Umzug konzentrierte er sich auf neue Themen, nämlich auf die theoretische Fundierung der Phillipskurve. Seine Arbeiten dazu sollten später die ersten Zweifel am dominierenden, keynesianschen Framework begründen. Im Nachhinein kann man getrost sagen, dass damit die Grundlagen für den "`Neu-Keynesianismus"' geschaffen wurden.

Wie in Kapitel \ref{sec: Phillips} dargestellt, war der vermeintliche, negative Zusammenhang zwischen Inflation und Arbeitslosigkeit zwar nicht Bestandteil der ursprünglichen keynesianischen Theorie. Aber in weiterer Folge vor allem in der keynesianischen Wirtschaftspolitik ein fixer Bestandteil. Unabhängig voneinander waren Milton Friedman und eben Edmund Phelps bereits ab Mitte der 1960er Jahre die ersten Ökonomen, die den Zusammenhang zwischen Inflation und Arbeitslosigkeit in Frage stellten. Wohlgemerkt zu einer Zeit, in der der Zusammenhang empirisch noch recht gut beobachtet werden konnte. Das in den 1970er Jahren diese Korrelation weitgehend verschwand gab den Kritikern Friedman und Phelps natürlich gehörig Auftrieb. Phelps hatte seine Kritik dabei - im Gegensatz zu Friedman - mathematisch-formal unterlegt. 

Der Artikel mit dem unscheinbaren Titel "`Money-Wage Dynamics and Labor-Market Equilibrium"' \parencite{Phelps1968} stellte die bis dahin unbestrittene Phillipskurve nicht nur infrage, sondern legte die Grundlage für eine ganz neue Sicht auf die Wirtschaftswissenschaften. Interessant ist, dass gleich mehrere Punkte, die natürlich ineinandergriffen, in diesem Artikel revolutionäre waren:
\begin{enumerate}
\item Die Mikrofundierung der Makroökonomie
\item Die formale Einführung der Erwartungen (als adaptive Erwartungen) als Notwendigkeit bei Unvollständiger Information
\item Die formale Einführung der Natürlichen Arbeitslosigkeit und "`Effizienzlöhne"'
\end{enumerate}
Bemerkenswert ist insbesondere, dass alle drei genannten Punkte bis heute fixer Bestandteil der Mainstream-Modelle sind. Die heutigen DSGE-Modelle sind mikrofundiert, beinhalten das Konzept der Erwartungen (wenn auch der rationalen statt der adaptiven) und akzeptieren einen gewissen Prozentsatz an Arbeitslosigkeit als Gleichgewichtszustand. Natürlich wurden alle drei Konzepte seit 1968 wesentlich erweitert, aber im Gegensatz zu den Arbeiten anderer großen Ökonomen, fällt auf, dass Phelps' Arbeiten bis heute, 50 Jahre später, kaum an Gültigkeit verloren. Keynes' Multiplikator ist heute höchst umstritten, Friedman's Geldmengensteuerung betreibt keine Zentralbank der Welt mehr und selbst die späteren Arbeiten von Robert Lucas wurden größtenteils von der Realität überholt. Phelps' bahnbrechende Erkenntnisse sind hingegen bis heute die Grundlage ökonomischer Modelle und kann daher als Geburtsstunde des "`Neu-Keynesianismus"' gesehen werden.

In seiner Nobelpreis-Biographie schreibt Phelps, dass es seit seiner College-Zeit das Gefühl hatte die wichtigste aktuelle Herausforderung der Wirtschaftswissenschaften sei die Integration der Mikroökonomie in die Makroökonomie \parencite{Phelps2006}. Heute nennen wir dies die Mikrofundierung der Makroökonomie.
Der inhaltliche Ausgangspunkt des oben genannten Artikels \parencite{Phelps1968} ist die Phillipskurve. Phelps beschreibt sie als Naivität der Keynesianer. Wobei er Keynes selbst ausdrücklich in Schutz nimmt: Keynes' Nachfragesteuerung wäre niemals soweit gegangen einen dauerhaft stabilen Zusammenhang zwischen Inflation und Arbeitslosigkeit anzunehmen \parencite{Phelps2006}. Phelps stellt stattdessen einen Zusammenhang zwischen der \textit{erwarteteten} Inflation und Arbeitslosigkeit her. Dieser Zusammenhang sei aber nur in der kurzen Frist stabil. Angenommen die erwartete Inflation läge bei 4\%. Arbeitgeber und Arbeitnehmer würden bei ihren Vertragsverhandlungen diese Inflationserwartung einfließen lassen und die Lohnhöhe entsprechend festlegen. Will die Zentralbank nun die Arbeitslosigkeit senken, kann sie Maßnahmen setzen, die die Inflation auf zum Beispiel 6\% erhöhen. Solange die erwartete Inflation unter der tatsächlichen Inflation liegt, wird die Arbeitslosigkeit sinken und sich somit wie von der Phillipskurve postuliert verhalten. Es ist aber klar, dass die Diskrepanz zwischen tatsächlicher und erwarteter Inflation nur kurzfristig aufrechterhalten werden kann, bevor sich die Erwartung dem tatsächlichen Wert anpasst. Die keynesianische, langfristige Phillipskurve wurde durch die neu-keynesianische, kurzfristige erwartungsgestützte Phillipskurve ersetzt. Als solche findet sie bis heute Eingang in die makroökonomischen Lehrbücher. Nebenbei etablierte Phelps dabei das Konzept der adaptiven Erwartungen, das aber später vom neuklassischen Konzept der rationalen Erwartungen abgelöst werden sollte.
Die zentrale Aussage in \textcite{Phelps1968} lautet, dass durch Geldpolitik die Arbeitslosigkeit nicht dauerhaft beeinflusst werden kann, sehr wohl aber unter Umständen in der kurzen Frist. Geldpolitik funktioniere außerdem über Inflations\textit{erwartungen} und diese passen sich recht schnell an die aktuelle Inflation an. Eine niedrige Inflation wird daher auch nicht langfristig zu höherer Arbeitslosigkeit führen\parencite{Phelps1967}. Daraus könnte man ableiten, dass die zentrale Aufgabe der Zentralbanken die Inflationssteuerung ist. Heute orientieren sich fast alle führenden Zentralbanken tatsächlich primär an den Inflationszielen, dies aber direkt auf Phelps' frühe Arbeiten zurückzuführen ginge aber zu weit, folgten doch noch weitere Arbeiten dazu von anderen Neu-Keynesianern und Neuen Klassikern.
Sehr wohl direkte Folge aus \textcite{Phelps1968} ist hingegen die Idee der "`natürlichen Arbeitslosenrate"', später häufig als NAIRU\footnote{non-accelerating inflation rate of unemployment} bezeichnet. Während die Klassiker davon ausgingen, dass es im Gleichgewicht keine Arbeitslosigkeit gäbe und die Neuen Klassiker meinten im Gleichgewicht gäbe es ausschließlich freiwillige Arbeitslosigkeit, verfolgten die Keynesianer den Ansatz Arbeitslosigkeit sei stets mit nachfrageorientierter Wirtschaftspolitik zu minimieren. Die Neu-Keynesianer gehen davon aus, dass es im Gleichgewicht ein gewisses Maß an unfreiwilliger Arbeitslosigkeit gäbe. Diese "`natürliche Arbeitslosenrate"' wird häufig Milton Friedman zugeschrieben, der einen sehr ähnlichen Ansatz ebenfalls 1968 veröffentlichte \parencite{Friedman1968}. Tatsächlich hatten Friedman und Phelps unterschiedliche Wege gewählt, die sie zu den gleichen Schlussfolgerungen führten. Der Begriff "`natürliche Arbeitslosenrate"' ist wohl Friedman zuzuschreiben, größeren Einfluss in der akademischen Welt hatte aber der Ansatz von Phelps \textcite[S. 9f]{Nobelpreis-Komitee2006}. Die theoretische Bearbeitung der Arbeitslosigkeit wurde später zu einem eigenen Forschungsgebiet innerhalb des "`Neu-Keynesianismus"' und ist in Kapitel \ref{Suchtheorie} dargestellt.
Die zentralen Aussagen der damals neuen Theorie wurden in einer Konferenz aufgearbeitet und schließlich gesammelt als Buch veröffentlicht \textcite{Phelps1970}. Dieses war in weiterer Folge einflussreich und erreichte unter Ökonomen einen hohen Bekanntheitsgrad unter dem Titel "`The Phelps volume"'.

Die Mikrofundierung der Makroökonomie wird ebenfalls häufig als wesentliche Neuerung der "`Neuen Klassischen Makroökonomie"' gesehen. Es ist aus methodischer Sicht \textit{der} große Bruch mit den Theorien der Keynesianer und auch Monetaristen. Tatsächlich ist nicht von der Hand zu weisen, dass die Neuen Klassiker diesen Ansatz als Standard in ökonomischen Modellen etablierten (vgl. Kapitel \ref{Neue Makro}). Aber auch hier gilt, dass die erstmalige Anwendung auf Phelps zurückgeht. In der nun schon häufig zitierten Arbeit \textcite{Phelps1968} verwendet Phelps ein mikroökonomisches Modell um den klar makroökonomischen Zusammenhang zwischen Inflation und Arbeitslosigkeit zu modellieren. Die dort notwendige intertemporale Betrachtung (im Kapitel \ref{Neue Makro} haben wir das schlicht "`dynamische Betrachtung"' genannt) der Interaktion der Kennzahlen war eine weitere Neuerung durch Phelps, die bis heute State-of-the-art in den Wirtschaftswissenschaften ist \parencite[S. 9]{Nobelpreis-Komitee2006}.

Die soeben dargestellten Ergebnisse wurden nicht unmittelbar begeistert aufgenommen, wie dies zum Beispiel mit Keynes' General Theory, oder der Theorie der Rationalen Erwartungen von Lucas passierte. Bei ihrem erscheinen Ende der 1960er-Jahre konkurrierten die Ideen von Phelps mit zahlreichen alternativen Ideen. Erst etwas später wurde durch die Stagflation sichtbar, dass die Phillipskurve in ihrer alten Form untragbar wurde und Phelps Erwartungsgestützte Phillipskurve als Alternative zielführender sei.

Für die Ende der 1060er Jahre noch unumschränkt dominierende keynesianische Theorie waren Phelps' Ergebnisse gewissermaßen schockierend: Die Phillipskurve war zwar wenig theoretisch begründet, spielte aber in der keynesianisch geprägten Wirtschaftspolitik eine wichtige Rolle. Das Ergebnis, dass Geldpolitik in der langen Frist als nachfrageorientierte Wirtschaftspolitik wirkungslos sei, beschränkte das keynesianische Framework. Die Mikrofundierung der Makroökonomie beschritt methodisch gänzlich neue Wege. 

Kurz zusammengefasst kann Phelps' frühes wirken so beschrieben werden: Er entwickelte noch in der Tradition der neoklassischen Synthese die "`Goldene Regel der Akkumumlation"'. Mit seinem Umzug an die University of Pennsylvania emanzipierte er sich aber vom Keynesianismus und es folgten Arbeiten die zur Revolution der Phillipskurve führen sollten. Diese Arbeiten umfassten Konzepte, die bis heute in der Mainstream-Ökonomie State-of-the-Art sind. Erstens, war er ein Vorreiter bei der Mikrofundierung der Makroökonomie und zweitens, etablierte er (adaptive) "`Erwartungen"' in den Modellen der Ökonomie. Beides spielte später bei den "`Neuen Klassikern"' eine wesentliche Rolle, wenn auch in der Form der \textit{rationalen} statt der \textit{adaptiven} Erwartungen. Er nahm also die Kritikpunkte der Neuen Klassiker am Keynesianismus vorweg. Man beachte, dass das bahnbrechende Werk \textcite{Phelps1968} noch vor der Neu-Klassischen Revolution erschien \parencite{Lucas1972, Lucas1976}. Er wurde aber kein Vertreter dieser "`Neuen Klassiker"', sondern war sich immer der Unvollständigkeit der Märkte bewusst, die sich in unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten ausdrückte. Er nahm der Wirtschaftspolitik damit die Illusion einer funktionierenden Feinsteuerung der Wirtschaft, ohne dabei aber einer vollkommenen Marktgläubigkeit zu verfallen. Ich denke man kann ihn daher getrost als eigentlichen Begründer des "`Neu-Keynesianismus"' bezeichnen.

Die eben genannten Arbeiten zur Unvollständigkeit der Märkte, unfreiwilliger Arbeitslosigkeit, monopolistischer Konkurrenz und Rigiditäten auf Märkten wurden später als Antwort auf die empirischen Defizite der "`Neuen Klassiker"' ausgearbeitet. Sie bilden den Kern der Ersten Generation des Neu-Keynesianismus.


\section{"'Kern"' des Neukeynesianismus}
\label{Kern}

Diese Arbeiten stellen den Kern der Neu-Keynesianer 1. Generation dar, weil hier erstmals die zwei wesentlichen Punkte des Neu-Keynesianismus zusammengefügt werden: Erstens, die Monopolistische Konkurrenz im Zusammenhang mit den Nominalen Rigiditäten und den Menu Costs und zweitens, die Nicht-Neutralität der Geldpolitik aus dem Spannungsverhältnis Nominale vs. Reale Rigiditäten und das daraus ableitbare Nicht-Vorhandensein der Klassischen Dichotomie. \parencite{RomerDavid1993}

\subsection{Nominale Rigiditäten}
\label{Nominale Rigiditäten}

Die Arbeiten von Phelps waren, wie gerade erwähnt, der Ursprung des Neu-Keynesianismus und wurden zeitlich vor der neu-klassischen Revolution formuliert. Die meisten neu-keynesianischen Arbeiten der 1. Generation entstanden allerdings als direkte Antworten auf die aufkommenden aber mit starren Annahmen unterlegten Arbeiten der "`Neuen Klassiker"'.

\textcite{Lucas1976} gab den Anstoß zur "`Neuen klassischen Makroökonomie"' und damit zur Theorie der rationalen Erwartungen. \textcite{Sargent1975} steuerten ihre berühmte "`policy-ineffectiveness proposition"' bei, also die Annahme, dass jegliche Wirtschaftspolitik ohne Effekt verpufft. Bereits 1977 folgten - als direkte Antwort - zwei Arbeiten \textcite{Taylor1977, Fischer1977} von Ökonomen, die eben nicht Teil der "`Neuen Klassik"' sein wollten, aber die Überlegenheit einzelner Elemente daraus akzeptierten. Das realisiert man bereits wenn man nur das Abstract der beiden Artikel liest. Sinngemäß steht da, dass aktive Geldpolitik sehr wohl eine Wirkung haben kann, da Löhne in der kurzen Frist rigide sind. Dies sei unabhängig von der Annahme rationaler Erwartungen. Zwei typisch neu-keynesianische Elemente kommen hier vor. Erstens, das Vorhandensein von (nominalen) Rigiditäten und damit die Wirksamkeit von aktiver Wirtschaftspolitik und somit die Ablehnung der "`policy-ineffectiveness proposition"' und zweitens, die implizite Akzeptanz der Annahme rationaler Erwartungen.
Die Annahme "`Adaptiver Erwartungen"' im Sinne von \textcite{Phelps1968} bedeutet, dass Entscheidungsträger, also zum Beispiel die Zentralbank, für einen Unterschied zwischen tatsächlicher Inflationsrate und erwarteter Inflationsrate sorgen kann. Die "`Rationalen Erwartungen"' nach \textcite{Lucas1976} gehen hier weiter und behaupten, dass es keine Differenz zwischen tatsächlicher und erwarteter Inflationsrate geben kann. Wenn nämlich die Zielinflation (oder zu dieser Zeit noch die Geldmengenziel - "`money supply rule"') bekannt ist, dann wissen die Haushalte ebensogut wie die Entscheidungsträger in welcher Form auf überschießende oder zu niedrige Inflation reagiert wird.  Sowohl \textcite{Fischer1977} als auch \textcite{Taylor1977} akzeptieren die Existenz von Rationalen Erwartungen. Die von den Neuen Klassikern daraus abgeleitete Wirkungslosigkeit von Geldpolitik hingegen lehnen sie hingegen strikt ab. Das Argument dafür ist, dass es langfristige Verträge gibt die Löhne (in \textcite{Fischer1977}), bzw. Preise (in \textcite{Taylor1977}) festsetzen. Geldpolitik hingegen kann laufend vorgenommen werden. Das Ergebnis ist, das diese nominale Lohn- und Preisrigidität, das Geldpolitik in der kurzen Frist sehr wohl wirksam ist. Die beiden genannten Werke gelten heute noch als ein Eckpfeiler der Neu-Keynesianischen Theorie. Das Ergebnis ist in gewisser Weise paradox, denn waren es nicht die "`alten"' Keynesianer, die behaupteten, dass Geldpolitik wirksam ist ("`money matters"'), wenn Preise und Löhne rigide sind? \textcite[S. 166]{Taylor1977} sind sich dessen bewusst. Aber Sie schränken ein, dass die postulierten Zusammenhänge ganz andere waren und nur ihre Theorie mit der Annahme Rationaler Erwartungen vereinbar sei. Oder wie es \textcite[S. 166]{Taylor1977} ausdrücken: \textit{"'By adopting the framework of rational expectations, we hope to have produced not a new wine but an old wine in a new and more secure bottle."'}
Die Akzeptanz der Gültigkeit der Annahme Rationaler Erwartungen ist übrigens bis heute ein Streitpunkt zwischen den "`alten"' Keynesianern (also den Vertretern der Neoklassischen Synthese) und den Neu-Keynesianern, aber auch innerhalb der Gruppe der Neu-Keynesianer. Für die Vertreter der Neoklassischen Synthesen ist diese Annahme schlicht unrealistisch. Innerhalb der Gruppe der Neu-Keynesianer gibt es durchaus auch Zweifler am Konzept der Rationalen Erwartungen.

\textcite{Taylor1979, Taylor1980} verallgemeinerte die Aussagen dahingehend, dass Rigiditäten auch außerhalb der strengen Annahmen fixer Laufzeiten bei Arbeitsverträgen auftreten. In seinem Modell geht er davon aus, dass Verträge gestaffelt neu für eine gewisse Zeitperiode ausverhandelt werden. Das Modell wird dementsprechend als "`Staggered contracts"', oder "`Taylor contracts"' bezeichnet. Im einfachsten Fall kann man davon ausgehen, dass Löhne in Arbeitsverträgen nur alle zwei Jahre angepasst werden. Das heißt jedes Kalenderjahr wird eine Hälfte der Arbeitsverträge an die beobachtete Inflation angepasst. Taylor konnte so zeigen, dass durch diese künstlich modellierte Lohnrigidität eine Abweichung vom langfristigen Gleichgewicht entsteht. \textcite{Blanchard1983} wendete dieses Modell auf die Preissetzung von Waren an und stellte fest, dass bei längeren Herstellungsketten der Effekt der Rigidität größer ist. Diese frühen Modelle der nominalen Rigiditäten waren noch nicht mikroökonomisch fundiert \parencite[S. 194]{Fischer1977}, was auch von Seiten der Neuen Klassiker recht rasch zu entsprechender Kritik führte. Dies wurde später durch die Arbeit von \textcite{Rotemberg1987} behoben. Die Annahme nominaler Lohnrigiditäten (nicht nominaler Preisrigiditäten) wurde später als unzureichend kritisiert \parencite{Mankiw1990}, weil die Reallöhne während Rezessionen steigen würden, was einerseits empirischen Beobachtungen widerspricht und andererseits dazu führt, dass Wirtschaftskrisen bei Personen mit sicheren Jobs sehr populär wären \parencite[S. 371]{Snowdon2005}. Erst in Verbindung mit der Annahme "`Monopolistischer Märkte"' (Imperfekte Märkte), "`Nicht-kostenloser Preisanpassung"' (Menu Costs), und "`Friktionen auf den Arbeitsmärkten"', die allesamt ebenfalls als Neu-Keynesianische Markenzeichen in diesem Kapitel noch besprochen werden, lassen sich rigide Löhne rechtfertigen.
Die nominale Preisrigidität überlebte aber und ist bis heute Teil der aktuellen Mainstream-Modelle! In den  Neu-Keynesianischen DSGE-Gesamtmodellen, die in Kapitel \ref{Neue Neoklassische Synthese} beschrieben werden, wird nominale Preisrigidität mittels "`staggered price setting"'-Modell von \textcite{Calvo1983} modelliert. Er bezieht sich dabei direkt auf die Arbeiten von \textcite{Taylor1979, Taylor1980}, macht daraus aber ein stochastisches Modell. Das heißt, Unternehmen können die Preise und Löhne nicht mehr nach Ablauf einer gewissen Zeitspannen anpassen, sondern erst jeweils nach einem zufällig langem Zeitraum. Das heißt die Preisanpassungen nach einem exogenen Schock finden noch unregelmäßiger statt. Dies bildet empirische Beobachtungen noch besser ab. 

Überzeugende Empirische Evidenz für die Existenz von nominalen Preisrigiditäten konnte man erst mit dem Aufkommen von Mikro-Datensätzen um die Jahrtausendwende erstellen. \textcite{Nakamura2008} fanden heraus, dass nominale Preise etwa neun bis elf Monate im Durchschnitt Bestand haben, dass es also nominale Rigiditäten tatsächlich gibt. Die beiden kritisieren aber auch, dass in den am häufigsten zitierten Modellen von \textcite{Taylor1980} und \textcite{Calvo1983} bei Preisänderungen stets von Preis\textit{erhöhungen} ausgeht. In ihrer empirischen Studie fanden \textcite[S. 1442]{Nakamura2008} hingegen heraus, dass mehr als ein Drittel aller Preisänderungen im Beobachtungszeitraum aber Preissenkungen waren.


Die modelltheoretischen Grundlagen wie man nominale Rigiditäten \textit{berücksichtigen} kann, waren also schon früh durch \textcite{Taylor1977, Fischer1977}, bzw. \textcite{Calvo1983} geschaffen worden, wie soeben dargestellt. Unbeantwortet hingegen blieb bislang die Frage, wie es zu diesen nominalen Rigiditäten überhaupt \textit{kommen kann}. Die Antwort darauf lieferten \textcite{Mankiw1985b, Akerlof1985, Parkin1986, RomerDavid1990} und \textcite{Ball1988}. 

Das erste, berühmt gewordene, Anwendungsbeispiel sind die sogenannten "`Menu Costs"', also "`Speisekarten-Kosten"'. Die Ausgangsannahme ist, dass die Anpassung von Preisen selbst Kosten verursacht. Daraus leitet sich der von \textcite{Mankiw1985b} geprägte Begriff der "`Menu Costs"' ab. Dahinter steht folgende exemplarische Idee: Das Drucken neuer Speisekarten kostet Geld. Gastronomen müssen also abschätzen, ab welchem Ausmaß von Preiserhöhungen Mehrerlöse entstehen, die den Druckkostenaufwand wieder ausgleichen\footnote{Die Idee entstand in den 1980er Jahren, also vor der großen digitalen Revolution. Heute wären diese "`Menu Costs"' im Wortsinn vermutlich wesentlich geringer als 1985.}. Eine sehr einfache Idee, die für die meisten individuellen Unternehmen kaum von Belang ist. \textcite{Akerlof1985, Mankiw1985b, Parkin1986} zeigten aber jeweils, dass diese kleinen individuellen Effekte zu großen makroökonomischen Effekten führen können. \textcite{Rotemberg1987} nannte diese Erkenntnis "`PAYM insights"', angelehnt an die Anfangsbuchstaben der vier Autoren. Konkret führt auf Märkten mit Monopolistischer Konkurrenz (siehe nächstes Kapitel) das individuell Nutzen-maximierende Verhalten der einzelnen Unternehmer\footnote{\textcite[S. 823]{Akerlof1985} nennen es \textit{"'Insignifikant"' suboptimales Verhalten}, bzw. \textit{Nahe-Rationales Verhalten}} dazu, dass Preisanpassungen an den Gleichgewichtspreis erst bei größeren Preissprüngen vorgenommen werden.  Kommt es also zu einem (geringen) Rückgang der aggregierten Nachfrage, werden Unternehmen bei monopolistischer Konkurrenz ihre Preise aufgrund der "`Menu Costs"' zunächst nicht anpassen, sondern stattdessen, trotz niedriger Nachfrage, den ehemaligen Gleichgewichtspreis verlangen. Gesamtwirtschaftlich optimal wäre es, wenn die Unternehmen ihren Preis senken würden. Das würde man auch in der neoklassischen Analyse von Monopolmärkten erwarten. Da Unternehmen aber "`Menu Costs"' ausgesetzt sind, werden die höheren Preis beibehalten. Für Unternehmen ist dieses Verhalten Nutzen-maximierend, weil die Preis-Anpassungskosten höher wären als der zusätzliche Gewinn, den Unternehmen bei geringerem Preis aber höherer abgesetzter Menge, erhalten würden \parencite[S. 372]{Snowdon2005}. Gesamtwirtschaftlich ist das Ergebnis aber suboptimal, weil die abgesetzte Menge beim rigiden Preis viel geringer ist als die theoretische Gleichgewichtsmenge.\footnote{Dieser Ansatz mit Monopolistischer Konkurrenz und de-facto Preissetzung ist nicht unähnlich frühen Post-Keynesianischen Ansätzen (vgl. \ref{Post-Keynes})! Auch wenn Neu-Keynesianer dies nur ungern zugeben würden.} 

Bleibt man auf Ebene eines einzelnen Unternehmens ist die Erkenntnis zwar durchaus interessant, sie scheint aber weitgehend folgenlos für die Gesamtwirtschaft. Das ist aber ein Trugschluss. Die wichtigste Erkenntnis der "`PAYM-insights"' ist, dass kleine - auf natürliche Schwankungen zurückzuführende - Rückgänge bei der aggregierten Nachfrage zu deutlichen Schwankungen beim gesamtwirtschaftlichen Output führen\parencite[S. 375]{Snowdon2005}. Da solche Schwankungen unerwünscht sind, argumentieren \textcite{Akerlof1985}, dass aktive Wirtschaftspolitik\footnote{\textcite[S.837]{Akerlof1985} schreiben konkret nur von Geldpolitik} sehr wohl einen stabilisierenden und damit wünschenswerten Effekt hat.


\subsection{Reale Rigiditäten}
\label{Reale Rigiditäten}

Anfang der 1990er Jahre wurde das Konzept der Rigiditäten verfeinert. Denn zwar konnte durch die "`PAYM-insights"' gezeigt werden, dass es theoretisch möglich ist, dass nominale Rigiditäten zu großen Schwankungen im Gesamtoutput führen, aber eben auch, dass dies in der Realität sehr unwahrscheinliche sei. Wie \textcite[S. 183]{RomerDavid1990} herausarbeiten mussten dazu ganz bestimmte Bedingungen erfüllt sind. So würde es, zum Beispiel, beim Auftreten von nominalen Preisrigiditäten nur dann zu großen Effekten auf den Gesamtoutput kommen, wenn der Arbeitsmarkt gleichzeitig extrem elastisch wäre. Gastronomen würden demnach zwar bei Nachfragerückgängen ihre Preise auf den Speisekarten unverändert lassen, aber gleichzeitig sofort Köche und Kellner kündigen?! Ein eher unrealistisches Szenario. Stattdessen führten \textcite{RomerDavid1990, Ball1988, Ball1989} \textit{reale} Rigiditäten als notwendige Ergänzung zu \textit{nominalen} Rigiditäten ein.

Zunächst muss einmal abgegrenzt werden, wie sich reale Rigiditäten von nominalen Rigiditäten unterscheiden. Nominale Rigiditäten haben wir schon als "`Menu Costs"' kennengelernt. Allgemein könnte man diese definieren als die Geschwindigkeit mit der sich Löhne und Preise anpassen, wenn vorgelagerte Preise sich ändern \parencite[S. 270]{Blanchard2003}. Bei nominalen Rigiditäten kommt der Effekt der Geldpolitik ins Spiel. Dazu ein einfaches Beispiel: Stellen Sie sich vor Sie möchten einen Apfel kaufen. Dieser sei mit 1EUR/Stück angeschrieben. Was glauben Sie was passiert, wenn in diesem Moment die Geldmenge verdoppelt wird (und alles andere gleich bleibt)? Intuitive Antwort: "`Dann kostet der Apfel 2EUR/Stück"'. Das ist auch intuitiv richtig. Nur was, wenn es eben nicht augenblicklich nach Ausweitungen der Geldmenge zu entsprechenden Preisanpassungen kommt? Dann ist die Geldpolitik eben "`Nicht-Neutral"'. Diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise kann sich die Wirtschaftspolitik zunutze machen und eben "`Geldpolitik"' betreiben. 
Glaubt man, wie die Neuen Klassiker, an die "`Klassische Dichotomie"' zwischen Geldmarkt und Gütermarkt auch in der kurzen Frist, ist diese zeitliche Differenz zwischen Geldmengenerhöhung und Anpassung aller Preise nicht vorhanden. Dann gibt es keine wirksame Geldpolitik. Die Neu-Keynesianer hingegen lehnen diese "`Klassische Dichotomie"' zumindest für die kurze Frist ab. Damit akzeptieren sie das temporäre Auseinanderlaufen von realen und nominalen Werten und eben auch die Wirksamkeit von Geldpolitik!

Reale Rigiditäten sind schon seit Keynes (vgl. Kapitel \ref{Keynes}) bekannt: Nominale Löhne sind rigide, weil sie bei Deflation in Regel nicht nach unten angepasst werden. Die \textit{realen} Löhne steigen also durch Deflation. Man spricht von \textit{realen} Rigiditäten. Steigende Real-Löhne führen im oben genannten Fall zu steigender Arbeitslosigkeit. 
Wie sind reale Rigiditäten aber in Abwesenheit von Deflation zu erklären? Damit beschäftigte sich ein ganzes Forschungsfeld (vgl. Kapitel \ref{drei Grunde}).  

\textcite{RomerDavid1990} verbanden in ihrer Arbeit das Forschungsfeld der Rigiditäten mit den Forschungsfeldern, die in Kapitel \ref{drei Grunde} dargestellt sind und identifizierten diese als reale Rigiditäten. Mit ihren Überlegungen grenzten \textcite{RomerDavid1990} den Neu-Keynesianismus ein weiteres Mal entscheidend als eigene ökonomische Denkrichtung ab und trugen mit der Verbindung der einzelnen Elemente dazu bei, dass der Neu-Keynesianismus als einheitliches Gesamtmodell gesehen werden kann. Davor waren Beiträge stets als ablehnende Antwort gegenüber den Neuen Klassikern entstanden, die aber eher unabhängig voneinander gesehen werden mussten. Betrachten wir den Inhalt dieses, für den Neu-Keynesianismus, wichtigen Beitrags: \textcite[S. 183]{RomerDavid1990} heben gleich zu Beginn hervor, "`dass \textit{reale} Rigiditäten nicht das gleiche sind wie \textit{nominale} Rigiditäten"'. Bis Ende der 1980er Jahre entstanden zwar viele Forschungsarbeiten zu Rigiditäten, diese unterschieden aber nicht zwischen realen und nominalen Effekten. Im nächsten Schritt erstellen die beiden ein interessantes aber komplexes Modell. Dessen Grundaussage lautet wie folgt: Erstens, nominale Rigiditäten ("`Menu Costs"') können realistischerweise nur zu kleinen gesamtwirtschaftlichen Schwankungen führen. Zweitens, reale Rigiditäten können alleinstehend kaum existieren: Auf Märkten ohne jegliche nominale Rigidität, kommt es immer zur Anpassung an das Marktgleichgewicht. Würde man hier aufhören, wäre die Essenz: Rigiditäten spielen keine Rolle. Aber jetzt kommt der Clou aus \textcite{RomerDavid1990}: Treten nominale Rigiditäten auf, so können auch reale Rigiditäten existieren. In diesem Fall verstärken die realen Rigiditäten die nominalen Rigiditäten und es kann zu großen Schwankungen im Gesamtoutput kommen. Diese wiederum rechtfertigen - wie schon im Paper von \textcite{Akerlof1985} - den Einsatz aktiver Wirtschaftspolitik.
Die soeben beschriebenen Ergebnisse sind natürlich nicht als plausibles Narrativ formuliert, sondern aus einem formal-theoretischen Modellrahmen abgeleitet. Die dort dargestellten Beispiele zeigen, dass bei realen Rigiditäten vor allem Arbeitsmarkt-Effekte - in geringerem Ausmaß Gütermarkt-Effekte - große Wohlfahrtsverluste (Schwankungen im Gesamtoutput) verursachen. Das zentrale Beispiel in \textcite{RomerDavid1990} betrachtet ein Modell, in dem der Arbeits- als auch der Gütermarkt berücksichtigt werden, und in dem Rigidität bei Gütermarkt-Preisen durch rigide Reallöhne verursacht werden. Das Modell bedient sich zwei Annahmen. 

Erstens, die Autoren nehmen an, dass Unternehmen "`Effizienz-Löhne"' bezahlen. Das sind Löhne, die etwas höher sind als der Gleichgewichtslohn um die Arbeitnehmer zu besserer Leistung zu motivieren\footnote{Details dazu im Unterkapitel \ref{RR_AM}}. Zusätzlich geht man davon aus, dass Arbeitsmärkte recht unelastisch sind. Das würde implizieren, dass sich Löhne pro-zyklisch verhalten: Bei guter Konjunktur sind freie Arbeitskräfte gefragt aber rar. Durch die Unelastizität der Arbeitsmärkte, müssen die Löhne überproportional angehoben werden, um zusätzliches Personal zu finden. Akzeptiert man die Existenz von Effizienz-Löhnen, kann man erklären wie es Gleichzeitig zur Unelastizität von Löhnen und zur Azyklizität der Real-Löhne kommen kann. Beides entsprach in den 1980er Jahren nämlich empirischen Beobachtungen \parencite{RomerDavid1990}. 
Zweitens, funktioniert das Modell nur wenn man annimmt, dass die Rigidität der Reallöhne, Rigidität bei Realen Preisen verursacht (und nicht etwa umgekehrt). Dann, und nur dann, tritt nämlich die folgende Wirkungskette ein: Ein Nachfrage-Schock führt nur zu einem geringen Anstieg der Realen Löhne und somit zu einem geringen Anstieg der Grenzkosten der Unternehmen. Das heißt, die Unternehmen haben wenig Motivation ihre Preise anzupassen. Der Gesamtoutput fällt im Modell als Folge aber dennoch erheblich. 

Man merkt schon, das das ganze Modell etwas konstruiert wirkt, wenn auch mit plausiblen Werten und Annahmen. Insgesamt ist der Ansatz der Versuch die zu einfache Sichtweise der "`Neuen Klassiker"' zu durchbrechen. Indem man einzelne, zu beobachtende Marktvorgänge, die nicht dem perfekten Wettbewerb abbilden, modelliert, kommt man zu großen Abweichungen beim Gesamtergebnis. Der Artikel von \textcite{RomerDavid1990} ist auch deshalb so entscheidend für den Neu-Keynesianismus, weil es die drei wesentlichen Elemente, die im Neu-Keynesianismus zuvor jeweils einzeln analysiert wurden, zusammenführt:
\begin{itemize}
	\item Rigiditäten: "`Menu Costs"', aber auch "`Sticky Wages"' verhindern die sofortige Anpassung an den Gleichgewichtspreis.
	\item Nicht Neutralität des Geldes: Abgeleitet durch die akzeptierte Existenz der Rigiditäten, muss es in der kurzen Frist einen Unterschied zwischen nominalen und realen Werten geben. Diese Differenz kann man durch Geldpolitik künstlich steuern. Die "`Klassische Dichotomie"' zwischen Geldmarkt und Realmarkt ist zumindest in der kurzen Frist Illusion
	\item Monopolistische Konkurrenzmärkte: \textit{Die} Neuerung in der \textit{Modellierung} war die Berücksichtigung nicht perfekter Märkte. Die Annahme, dass sich Preise nicht ausschließlich durch Angebot und Nachfrage und einen Walrasianischen Auktionator ergeben, führt überhaupt erst zur Möglichkeit von Rigiditäten. Ist aber im Hinblick auf die meisten Gütermärkte und den Arbeitsmarkt realistischer.
\end{itemize}
Die Einbettung dieses Rahmenwerks in die Modellwelt der Neuen Klassiker, die DSGE-Modelle der "`Real Business Cycle"'-Theorie führte schließlich zum "`Neu-Keynesianismus der 2. Generation"' (Neue Neoklassische Synthese, vgl. Kapitel \ref{Neue Neoklassische Synthese}), der heute noch weitgehend den Mainstream in der Ökonomie darstellt.

Vielleicht haben Sie sich beim Lesen schon gefragt: Okay, es gibt einen Unterschied zwischen den Nominalen Rigiditäten - die verzögerte Anpassung der Preise - und Realen Rigiditäten. Was sind aber jetzt Reale Rgiditäten in der Praxis? Das Ausgangsbeispiel mit den Löhnen, die während Zeiten der Deflation nicht sinken, ist nämlich höchst theoretisch. Deflation trat selbst während der "`Great Recession"' nach 2008 nur sehr vereinzelt auf.
Mit den verschiedenen Quellen Realer Rigiditäten beschäftigen sich die nächsten Unterkapitel. Es ist an dieser Stelle aber darauf hingewiesen, dass die dort vorgestellten Arbeiten nicht ursprünglich als Quellen Realer Rigiditäten identifiziert wurden. Der Entstehungsweg war anders herum: Man versuchte empirisch beobachtbare Abweichungen vom ökonomischen Gleichgewicht zu erklären. Erst \textcite{RomerDavid1990} vereinte dieses Sammelsurium an Erklärungen unter dem Begriff Reale Rigiditäten \textcite[S. 4]{Mankiw1991})


\subsubsection{Unvollkommenheiten am Arbeitsmarkt}
\label{RR_AM}

Der Arbeitsmarkt spielt bei den Neu-Keynesianern eine zentrale Rolle. Erstens, bei der Frage warum Real-Löhne scheinbar deutlich weniger prozyklisch agieren, als in der klassischen Ökonomie angenommen. Diese Frage ist allgemein zentral für die Existenz von Rigiditäten. Der zweiten Frage widmen wir uns hier: Wie kann es zu unfreiwilliger Arbeitslosigkeit kommen?\footnote{Der Arbeitsmarkt spielt, drittens, eine wesentliche Rolle in der Suchtheorie die gesondert in Kapitel \ref{Suchtheorie} behandelt wird.}

Der Arbeitsmarkt wurde in der VWL sehr lange als ein "`gewöhnlicher"' Markt, analog zum Gütermarkt betrachtet. Löhne ergeben sich hier wie Preise aus Angebot und Nachfrage und Treffen sich im Gleichgewicht. Aus heutiger Sicht erscheint es überraschend wie spät in der Makroökonomie erstmals darüber nachgedacht wurde, ob die Produktivität eines Arbeitnehmers mit höheren Löhnen steigen könnte und in der Folge Arbeitgeber einen Anreiz haben, höhere Löhne (als den Gleichgewichtslohn) zu bezahlen? In der Psychologie/Betriebswirtschaftslehre spielten solche Überlegungen früher eine Rolle wie die berühmte Literatur von \textcite{Maslow1943, Herzberg1966, McClelland1961, McGregor1960} zeigt. In der Volkswirtschaftslehre wurde dies erstmals in den 1970er Jahren diskutiert. Wie schon mehrmals beschrieben, zerbrach zu der Zeit der keynesianische Konsens der Makroökonomie. Neben dem Anstieg der Inflation und dem damit verbundenen Ende der Theorie der Phillips-Kurve, war auch ein Anstieg der Arbeitslosenraten zu beobachten. Mit der keynesianischen Theorie war dies nicht in Einklang zu bringen, dort wurde Arbeitslosigkeit schließlich durch eine Unterauslastung der Wirtschaft bei gleichzeitig rigiden Löhnen verursacht. Während einer Deflation steigt dann der reale Wert rigider Nominallöhne. Die realen Löhne wären dann höher als der Gleichgewichtslohn, was in Arbeitslosigkeit resultiert.
In den 1970er Jahren war man aber weit entfernt von Deflation, womit dieser Erklärungsansatz scheiterte. Die Neuen Klassiker machten es sich einfach und behaupteten es gäbe keine unfreiwillige Arbeitslosigkeit. Ein Ansatz, der von Neu-Keynesianern - wie schon in Kapitel \ref{Neue Makro} beschrieben - geradezu lächerlich gemacht wurde. Allerdings hatten die Neu-Keynesianer zunächst keine eigene Erklärungen parat. Erste Ansätze gingen von empirischen Beobachtungen aus: Man konnte sehen, dass selbst in Zeiten hohen Arbeitskräfteangebots kaum ein Unternehmen versuchte die Real-Löhne seiner Mitarbeiter zu kürzen. Dies wäre in der klassischen Theorie eigentlich zu erwarten, denn ein Überangebot an Arbeitskräften sollte gleichzeitig zu einem sinkenden Gleichgewichtslohn führen. Erste Erklärungsansätze gingen folglich in die Richtung, dass die Arbeitnehmer in Gewerkschaften stark organisiert wären und daher wie ein Monopolist auftraten. Die resultierenden zu hohen Löhne verursachten dann Arbeitslosigkeit (QUELLE VVV). Dies war aber mit der Empirie nicht vereinbar sobald man Arbeitsmärkte im Detail betrachtete. So waren in den USA verhältnismäßig wenig Arbeitnehmer Mitglieder in Gewerkschaften.
Weit erfolgreicher war die Effizienzlohn-Hypothese, die zunächst von \textcite{Stiglitz1976} vorgeschlagen wurde. Diese hat eine Vorbedingung: Es \textit{muss} einen positiven Zusammenhang zwischen der Höhe des Lohnes und der Arbeitsproduktivität des Arbeitnehmers geben. Für diesen Fall zeigt \textcite{Stiglitz1976}, dass es unter der Prämisse der Gewinnmaximierung rational ist, dass ein Lohn bezahlt wird, der üblicherweise höher ist als der eigentliche Gleichgewichtslohn. Warum? Ein Gewinn-maximierendes Unternehmen wird jenen Lohn bezahlen, bei dem das Verhältnis aus Arbeitsproduktivität und Lohn maximal ist. Steigt die Arbeitsproduktivität zumindest in einem gewissen Bereich überproportional zur Lohnhöhe wird das optimale Verhältnis aus Arbeitsproduktivität und Lohn nicht beim ökonomischen Gleichgewichtslohn, sondern bei einem höheren Lohn erreicht. Dieser Lohn wird "`Effizienzlohn"' genannt.
Ein höherer Lohn als der Gleichgewichtslohn erklärt warum es zu unfreiwilliger Arbeitslosigkeit kommen kann. Der eben beschriebene "`Effizienzlohn"' erklärt zusätzlich warum Arbeitssuchende sich nicht einfach in einen Job hinein-reklamieren können, indem sie ihre Arbeitskraft zu einem niedrigeren Lohn anbieten\footnote{Das "`Insider-Outsider-Modell"' argumentiert hier allerdings, dass die Lohnkosten für neue, unternehmens-externe, Mitarbeiter aufgrund von Kosten der Personalsuche, Einschulungskosten und auch sozialen Kosten, im Sinne von Demotivation durch hohe Fluktuation, in Wahrheit wesentlich höher sind als der Gleichgewichtslohn am Markt. In der Folge ist es nicht so einfach möglich sich in ein Unternehmen hinein zu reklamieren indem man einen niedrigeren Lohn anbietet wie hier beschrieben.} (und somit den eigentlichen Gleichgewichtslohn wieder herstellen würden). Die Arbeitgeber gehen davon aus, dass höhere Löhne gleichzeitig höhere Produktivität bedeuten. Sie wollen daher gar keine Arbeitnehmer die zum niedrigeren Gleichgewichtslohn arbeiten, sondern sie wollen Arbeitnehmer bei denen das Verhältnis zwischen Lohn und Produktivität optimal ist. 
 
So weit so gut. Bleibt die Frage der mikroökonomischen Fundierung: Warum sollte es überhaupt einen Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität geben? Aus praktischer Sicht scheint die Antwort klar und intuitiv und man würde eher die Gegenfrage stellen? Warum soll es \textit{keinen} derartigen Zusammenhang geben? Die rein ökonomische Antwort auf diese Gegenfrage lautet: Weil auf einem perfekten Markt unendlich viele Arbeitgeber unendlich vielen Arbeitnehmern gegenüberstehen. Und wenn ein Arbeitnehmer die im Arbeitsvertrag vereinbarten Pflichten nicht erfüllt, wird er solange gegen einen anderen ausgetauscht, bis die Anforderungen erfüllt werden. Die Realität ist weder so hart, noch so einfach. Daher wurden in der Folge verschiedene Modelle \parencite{Yellen1984} entwickelt, die die Annahme zum Zusammenhang von Lohnhöhe und Arbeitsproduktivität formal plausibilisierten. Fünf davon haben sich als etabliert: 

Der erste Ansatz ist das sogenannte "`Shirking-Modell"'\footnote{"'Shirking"' (engl.) entspricht in etwa dem deutschen "`sich drücken"'. Dementsprechend lautete der nicht geläufige, deutsche Name: "`Drückeberger-Modell"'}. Es basiert, ebenso wie das zweite Modell, auf das in Kürze eingegangen wird, auf dem Problem der Informationsasymmetrie, die wir ja schon im Kapitel \ref{cha: Marktversagen} kennen gelernt haben. Das bekannteste Shirking-Modell ist wohl die Shapiro-Stiglitz-Hypothese \parencite{ShapiroStiglitz1984}. Demnach können Arbeitgeber ihre Arbeitnehmer nicht zu hundert Prozent monitoren. Dementsprechend entsteht ein Moral Hazard Problem. Die Arbeitnehmer können sich dazu entschließen weniger zu arbeiten als im Vertrag vorgesehen. Die Arbeitgeber werden nicht alle Drückeberger identifizieren können, aber jene, die sie beim "`shirken"' erwischen, werden sie kündigen. Auf einem perfekten Wettbewerbsmarkt wäre das aber für den ertappten Drückeberger wenig problematisch. Schließlich gibt auf so einem Markt keine unfreiwillige Arbeitslosigkeit und es wird der Gleichgewichtslohn bezahlt. Das heißt, jeder Drückeberger findet sofort einen neuen Job zu gleichem Lohn.  Bei \textcite{ShapiroStiglitz1984} könnten Arbeitgeber folglich beschließen einen höheren Lohn - den "`Effizienz-Lohn"' - zu bezahlen. Dann hätten Arbeitnehmer nämlich den Anreiz fleißig zu arbeiten. Der sonstige drohende Jobverlust wäre jetzt nämlich problematisch für die Dienstnehmer, da alternativ nur der niedrigere Gleichgewichtslohn bleibt. Wenn aber alle Arbeitgeber so vorgehen und als Anreiz höhere Löhne bezahlen, gibt es ausschließlich den "`Effizienz-Lohn"' am Markt. Da dieser höher ist als der Gleichgewichtslohn wird der Markt nicht vollständig geräumt. Mit anderen Worten: Es kommt zu unfreiwilliger Arbeitslosigkeit. Die Arbeitnehmer haben übrigens in diesem Modell auch dann keinen Anreiz zu "`shirken"', wenn der "`Effizienz-Lohn"' den Gleichgewichtslohn vollständig verdrängt hat. Schließlich droht nun als Konsequenz des erwischt werdens nach wie vor die Kündigung. Da es nun aber unfreiwillige Arbeitslosigkeit gibt, können sich Arbeitnehmer nicht darauf verlassen sofort wieder einen Job zu finden. Zusammengefasst: Das "`Shirking-Modell"' geht also davon aus, dass höhere Löhne bezahlt werden, weil sich die Unternehmen dadurch höhere Arbeitsproduktivität - in der Form von weniger "`shirking"' - versprechen. Folglich bekommen alle Arbeitnehmer einen "`Effizienz-Lohn"' angeboten, der höher als der Gleichgewichtslohn ist. Dies resultiert darin, dass es stabil einen bestimmten Prozentsatz unfreiwillige Arbeitslosigkeit gibt.

Als zweite Erklärung dient das Adverse-Selektions-Modell. Auch hier wird der Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe dadurch erklärt, dass es eine Informationsasymmetrie zwischen Arbeitgeber und Arbeitnehmer gibt. Im Gegensatz zum "`Shirking-Modell"' bezieht es sich aber auf einen Informationsmangel seitens der Arbeitgeber \textit{vor} Vertragsabschluss. Im diesbezüglich am häufigsten zitierte Paper von \textcite{Weiss1980} argumentiert, dass Arbeitgeber mit hohen ausgeschriebenen Löhnen ein Signal an den Markt senden: Wer diesen Job annimmt, muss eine Produktivität in entsprechender Höhe leisten. Arbeitnehmer würden sich demnach ausschließlich auf Stellen bewerben, bei denen das angebotene Gehalt ähnlich hoch ist wie ihr persönlicher Mindestlohn. Unternehmen werden in diesem Modell ihre existierende Lohnstruktur in so einem Fall selbst dann nicht nach unten anpassen, wenn der Gleichgewichtslohn am Markt fällt. Dann nämlich würden sofort die besten Mitarbeiter freiwillig kündigen, da diese auf dem Arbeitsmarkt die besten Jobaussichten haben. Diese Rigidität der Löhne führt insgesamt zu höheren Löhnen als dem Gleichgewichtslohn, was wiederum in unfreiwilliger Arbeitslosigkeit resultiert.

Der dritte Ansatz ist das "`Labor-Turnover-Model"' und findet bereits bei \textcite{Phelps1968} Erwähnung. Also in jenem Journal-Artikel, der einen der ersten Anstöße zum Neu-Keynesianismus darstellt (vgl. Kapitel \ref{micmac}). Ausgangspunkt ist, dass die Suche nach Arbeitnehmern sowie deren Einschulung ein kostenintensiver Prozess ist. Eingeschulte Arbeitnehmer liefern also eine höhere Produktivität als neue Arbeitnehmer. Um Arbeitnehmer nun davon abzuhalten von sich aus zu kündigen, wird ein höherer Lohn als der Gleichgewichtslohn bezahlt. Da alle Unternehmen so agieren, ist der Marktlohn ein "`Effizienz-Lohn"'. Da dieser höher ist als der marktäumende Gleichgewichtslohn, führt dies zu einem gewissen Niveau an unfreiwilliger Arbeitslosigkeit.
Der Wirkungszusammenhang ist damit ähnlich wie beim "`Shirking-Modell, hier zahlen die Unternehmer aber einen höheren Lohn um freiwillige Abgänge zu verhindern, anstatt, wie beim "`Shirking"', Untätigkeit zu vermeiden.

Außerdem gibt es, viertens, das nicht-rationale, Soziologische- oder Fairness-Modell. Gott-sei-Dank wird sich so mancher Leser denken. Schließlich waren die bisher genannten Modelle strikt am neoklassischen Nutzenmaximierer ausgerichtet, mit der Folge, dass das darin gezeichnete Menschenbild ein nicht gerade sehr positives ist. Stichwort "`Shirking"': Der nutzenmaximierende Arbeitnehmer versucht möglichst wenig zu arbeiten. \textcite{Solow1980} hatte als erster argumentiert, dass die Gründe für Lohn-Rigiditäten vielleicht viel einfacher in "`sozialen Konventionen"' oder "`empathischem Verhalten"' zu finden seinen, als in nutzenmaximierenden Verhalten \parencite[S. 204]{Yellen1984}.  \textcite{Akerlof1982} war der erste, der dies systematisch untersuchte. Er startet damit eine Reihe von Publikationen, die er gemeinsam mit seiner Ehefrau Janet Yellen fortsetzte \parencite{Akerlof1984, AkerlofYellen1987, AkerlofYellen1988, AkerlofYellen1990}. Die Inhalte analysieren psychologische und soziologische Gründe dafür, dass Unternehmen höhere Löhne als den Gleichgewichtslohn bezahlen. Akerlof und Yellen heben Faktoren wie Moral, soziale Verantwortung und faire Löhne. Außerdem kritisieren sie, dass menschliche Arbeitskraft nicht wie nicht-menschliche Inputs modelliert werden sollte, da die oben genannten Faktoren eben dazu führen, dass es signifikante Unterschiede in der Bewertung von Menschen wie Maschinen gibt \parencite[S. 392]{Snowdon2005}. Die Arbeit von \textcite{Akerlof1982} war hierfür die erste, die solche Prozesse formalisierte. Es sollte dabei erwähnt werden, dass es Akerlof und Yellen (und anderen Ökonomen) nicht darum geht menschliches Verhalten nicht formal zu modellieren. Sondern im Gegenteil, ihre Modelle sind ebenso formal-mathematisch aufgebaut. Aber es geht darum grundsätzlich anzuerkennen, dass Arbeitnehmer als Menschen negative Gefühle entwickeln, wenn sie sich unfair behandelt fühlen und in der Folge als Konsequenz - also durchaus rational erklärbar - eine geringere Arbeitsproduktivität zeigen. Arbeitgeber wissen dies und bezahlen deshalb einen "`fairen"' Lohn, eben um die Arbeitnehmer bei Laune zu halten. Als Resultat entwickeln \textcite{AkerlofYellen1990} ihre "`fair wage-effort hypothesis"'. Darin optimieren Arbeitnehmer ihre individuelle Nutzenfunktion, indem sie ihre Arbeitsproduktivität an das Verhältnis zwischen Reallohn und als fair empfundenen Lohn anpassen. Dies macht es für Arbeitgeber sinnvoll einen höheren Lohn als den Gleichgewichtslohn zu bezahlen, weil dann eben die Arbeitsproduktivität höher ist. Das Resultat ist das gleiche wie bei den drei zuvor genannten Ansätzen zum Zusammenhang zwischen Lohnhöhe und Arbeitsproduktivität: Die über dem Gleichgewichtslohn liegenden "`Effizienz-Löhne"' führen dazu, dass der Arbeitsmarkt nicht vollständig geräumt wird. Mit anderen Worten: Unfreiwillige Arbeitslosigkeit entsteht. 

Eigentlich handelt es sich beim hier dargestellten Teilkapitel nur um ein kleines Rädchen im  Neu-keynesianischen Rahmenwerk. Dennoch ist der Input von Akerlof und Yellen bei Betrachtung des gesamten Neu-Keynesianismus interessant. Kann er doch als der Versuch gesehen werden, im Neukeynesianismus eine verhaltensökonomische Facette zu integrieren und ihn wieder stärker Richtung Keynesianismus auszurichten. Ihren Artikel \textit{Rational Models of Irrational Behavior} schließen \textcite{AkerlofYellen1987} wie folgt: "`The bad press that Keynesian theory has recently received from maximizing, super-rational theory is simply undeserved."' Und die beiden argumentieren weiter, dass die Annahmen der keynesianischen Theorie mit den Ergebnissen der modernen Psychologie und Soziologe übereinstimmen. Auch bei seiner Nobelpreisrede im Jahr 2001 stellte Akerlof \textcite{Nobelpreis-Komitee2001} das Thema "`Verhaltensorientierung in der Makroökonomie"' in den Vordergrund. An dieser Stelle zeigt sich meines Erachtens auch sehr schön einer der Angriffspunkte auf die moderne Makroökonomie: Wir haben nun fünf verschiedene Ansätze gesehen zu denen jeweils dutzende Journal-Artikel geschrieben wurden, die jeweils mit verschiedenen modell-theoretischen Annahmen und formalen mathematischen Methoden versuchen zu erklären, warum es einen Zusammenhang zwischen Arbeitsproduktivität und Lohnhöhe gibt!? Nur einer davon, nämlich jener von Akerlof und Yellen, beruft sich auf Argumente, die man aus menschlichem Eigenschaften -  wie Fairness, Neid oder Rache - ableiten muss. Dieses menschliche Verhalten an sich kann man als solches aber nicht modellieren. Keynes hätte dieses Verhalten als "`animal spirits"' zusammengefasst. Alle anderen genannten Ansätze bemühen sich die Vorgänge als Ergebnis individuell-rationalen Verhaltens zu modellieren. Sie wirken etwas konstruiert und weniger natürlich, passen aber dafür perfekt in das formal-mathematische und streng rationale Konzept der modernen Makro-Ökonomie. Letzteres hat sich schlussendlich als Mainstream-Makroökonomie durchgesetzt. Tatsächlich gibt es aber Vertreter des Neu-Keynesianismus, die zwar in dessen Frühphase wichtige Beiträge zu dessen Entwicklung beigesteuert haben, den Übergang zur "`Neuen neoklassischen Synthese"' aber nicht mitgegangen sind. Zu nennen sind hier vor allem Joseph Stiglitz, George Akerlof, Janet Yellen und Paul Krugman. Auf der anderen Seite gibt es Vertreter des frühen Neukeynesianismus, die diesen - gemeinsam mit jungen Wirtschaftswissenschaftlern - zur "`Neuen neoklassischen Synthese"' weiterentwickelt haben. Hier sind vor allem John Taylor, David Romer und Greg Mankiw zu zählen. Gerade Anfang der 1990er Jahre etablierte sich die "`Neue neoklassische Synthese"' (vgl. Kapitel \ref{Neue Neoklassische Synthese}) und die Ansätze von \textcite{AkerlofYellen1990} gerieten eher in Vergessenheit.

\subsubsection{Unvollkommenheiten am Finanzmarkt}
HIER WEITER
\textcite[S. 12]{Mankiw1991b}
Stiglitz/Weiss

\subsubsection{Unvollkommenheiten am Gütermarkt}
\textcite[S. 14]{Mankiw1991b}




\subsection{Monopolistische Konkurrenz}
\label{Monopol}
Die Neuen Klassiker etablierten die Mikrofundierung der Makroökonomie, auf die die Neu-Keynesianer aufbauen. So konnte das grundsätzlich mikroökonomische Thema der Marktformen in der Makroökonomie berücksichtigt werden. Die Annahme, dass auf den Märkten generell "`Vollständige Konkurrenz"' ("`Perfekter Wettbewerb"') herrscht, ist grundsätzlich so alt wie die Wirtschaftswissenschaft selbst. Implizit ging bereits Adam Smith davon aus, dass sich auf Märkten eine große Zahl von Anbietern und Nachfragern treffen und einen Gleichgewichtspreis finden. Explizit ausgesprochen und analysiert wurde dies von Leon Walras und seinem Konzept des "`Allgemeinen Gleichgewichts"'. Dieses spielt ja bis heute - vor allem bei den Neuen Klassikern - als "`Walrasianischer Auktionator"' eine große Rolle. In der Mikroökonomik analysierten Gerard Debreu und Kenneth Arrow in den 1950er Jahren das "`Allgemeine Gleichgewicht"' unter Einbeziehung der Finanzmärkte mit modernen mathematischen Methoden. 
Natürlich wusste man aber, dass selbst Märkte, auf denen es sowohl viele Anbieter als auch viele Nachfrager gibt, in vielen Fällen keine perfekten Konkurrenzmärkte sind. Auf perfekten Konkurrenzmärkten herrscht unendliche Preiselastizität. Das heißt die kleinste Abweichung vom Gleichgewichtspreis durch einen Anbieter führt dazu, dass dieser Anbieter schlagartig kein einziges Gut mehr verkauft. Das ist mit unserer tagtäglichen Erfahrung nicht vereinbar. Man versetze sich dazu nur in folgendes Beispiel: Sie gehen in einen Supermarkt und wollen dort Güter des täglichen Bedarfs kaufen. Auf einem vollständigen Konkurrenzmarkt müssten Sie zu jeder Zeit den Gleichgewichtspreis von jedem Gut erheben. Das wäre zeitaufwändig und damit auch teuer. Stattdessen akzeptieren Sie mit dem Besuch im Supermarkt implizit, dass der Verkäufer einen Preis festsetzt. Auch wenn Ihnen manchmal vielleicht bewusst ist, dass Sie eine gute Verhandlungsbasis für einen niedrigeren Preis hätten (z.B.: Das gleiche Gut kostet bei einem Konkurrenten weniger). Der Supermarkt wiederum ist sich seiner Position ebenso bewusst. Er weiß, dass er in einem gewissen Rahmen die Preise wählen kann, ohne dass die Kunden sich sofort abwenden und für ein bestimmtes Produkt in den Konkurrenz-Supermarkt wechseln nur um ein paar Cent zu sparen. Wichtig ist hier die Betonung auf "`in einem gewissen Rahmen"'. Der Name "`Monopolistische Konkurrenz"' ist nämlich etwas irreführend. Die Anbieter auf einem derartigen Markt haben nämlich \textit{keine} Monopolstellung! Im Gegenteil, es handelt sich in der Regel um Märkte mit ausgeprägter Konkurrenz. Allerdings sind die einzelnen Anbieter eben "`Preissetzer"' und nicht "`Preisnehmer"'. Das heißt, der Preis der Produkte wird vom Verkäufer festgesetzt und nicht vom Markt in dem Sinne, dass Abweichungen vom Gleichgewichtspreis sofort zu einem totalen Rückgang der abgesetzten Menge führen. Die Miteinbeziehung von "`imperfektem Wettbewerb\footnote{"Imperfekter Wettbewerb"' und "`Monopolistische Konkurrenz"' wird in der VWL häufig gleichgesetzt. So auch hier. In Wirklichkeit aber umfasst "`Imperfekter Wettbewerb"' mehr, nämlich jegliche Abweichung von perfekten Konkurrenzmärkten, also auch Oligopole oder Monopole}"' berücksichtigt ein Phänomen innerhalb der Volkswirtschaftslehre, das man in der Betriebswirtschaftslehre schon lange kennt: Im Marketing spricht man von "`horizontaler Differenzierung"', das Unternehmen erlaubt unterschiedliche Preise zu verlangen, die auf den ersten Blick exakt die gleichen Bedürfnisse befriedigen. Denken Sie nur an die Preisunterschiede am Automarkt. aus volkswirtschaftlicher Sicht spielt das Thema der Marktformen in der Mikroökonomie eine entscheidende Rolle. Da die Makroökonomie seit der Revolution durch die Neuen Klassiker "`mikrofundiert"' ist, spielen Marktformen auch in der Makroökonomie eine Rolle.  
In der Mainstream-Makroökonomie hatte man aber lange Zeit ein Problem damit, vom Konzept des "`Walrasianischen Auktionators"' abzugehen. Bei Keynesianers und Monetaristen spielten Überlegungen zur Marktform, mangels Mikrofundierung ihrer Modelle, keine Rolle. Aber auch die Neuen Klassiker griffen - übrigens vehement bis heute -  ausschließlich auf das Konzept der perfekten Konkurrenzmärkte zurück. Die Ursprünge der "`Monopolistischen Konkurrenz"' liegen dennoch schon recht weit zurück: Fast zeitgleich veröffentlichten \textcite{Chamberlin1933} und \textcite{Robinson1933} ihre Werke "`The Theory of Monopolistic Competition"' und "`The Economics of Imperfect Competition"'. Die Motivation hinter diesen beiden Werken ist grundverschieden von jener im modernen Neu-Keynesianismus. Beide Arbeiten zielen schließlich rein auf mikroökonomische Überlegungen ab. Aber die grundlegende Idee ist identisch: Nämlich, dass auf einem Markt mit vielen Anbietern und Nachfragern, ein einzelner Anbieter seinen Verkaufspreis in engen Grenzen wie ein Monopolist festsetzen kann. Joan Robinson wurde später zu einer zentralen Figur des "`Post-Keynesianismus"'. In dieser Schule, die eben nicht zum Mainstream gehört,  ist die Berücksichtigung "`Imperfekten Wettbewerbs"' eine zentrale Annahme. Nämlich in der Form eines sogenannten "`Mark-up"'. Mehr dazu im Kapitel \ref{Post-Keynes}. Entsprechend den riesigen inhaltlichen Differenzen zwischen "`Neu-Keynesianern"' und "`Post-Keynesianern"' ist es wenig überraschend, dass erstere es ablehnen, dass das Konzept der "`Monopolistischen Konkurrenz"' von den "`Post Keynesianern"' übernommen wurde. "`Post-Keynesianer haben ein breites Spektrum an Modellen mit Imperfektem Wettbewerb, aber im Detail sind sie nicht sehr ähnlich zu den Neu-Keynesianischen Modellen"' \textcite[S. 439]{Snowdon2005}, meinte Greg Mankiw in einem Interview direkt darauf angesprochen, ob nicht die Post-Keynesianer in diesem Bereich Vorreiter waren? Abgesehen von seiner Aussage, muss man aber doch bemerken, dass das Element der "`Monopolistischen Konkurrenz"' jenes innerhalb der heutigen Mainstream-Ökonomie ist, das am weitesten vom Dogma der "`Marktgläubigkeit"' abweicht. 

Die Idee "`Imperfekten Wettbewerb"' in makroökonomischen Modellen umzusetzen ist \textit{das} Alleinstellungsmerkmal der Neu-Keynesianer schlechthin. Wie bereits erwähnt ist dieses Element nämlich sowohl den Keynesianern, als auch den Monetaristen und den Neuen Klassikern fremd. 
Die Existenz von Monopolistischen Märkten steht in engen Zusammenhang mit den im letzten Kapitel behandelten Rigiditäten. Tatsächlich machen Überlegungen zu Rigiditäten nur dann Sinn, wenn man "`Imperfekten Wettbewerb"' berücksichtigt, andernfalls wären die anbietenden Unternehmen nämlich Preisnehmer und Überlegungen zu Rigiditäten würde jegliche Grundlage fehlen!

Was ist die Motivation der Neu-Keynesianer "`Imperfekten Wettbewerb"' in Makro-Modellen zu berücksichtigen? Was ändert sich dadurch in den Modellen? Und was sind die Auswirkungen auf die Ergebnisse der Modelle? Nun die Motivation war wohl primär empirisch gegeben. Der Neu-Keynesianismus wurde ja praktisch aus der Idee heraus geboren, dass Marktunvollkommenheiten auf Märkten eine wichtige Rolle spielen. Eine Tatsache, die relativ unumstritten ist, aber von den Neu-Klassikern nicht berücksichtigt wurde. Greifen wir das kurz angesprochene Beispiel des Automarktes auf. Ökonomisch ausgedrückt besteht der Zweck eines Automobiles jemand von A nach B zu bringen. Das ist aber nicht mit den empirisch leicht zu beobachtenden Preisunterschieden am Automarkt zu vereinbaren\footnote{Man könnte einwenden der Zweck eines Autos besteht eben nicht nur im Transport von A nach B, sondern auch darin Status zu vermitteln, Sport zu betreiben, etc. Allerdings gibt es Preisdifferenzierungen auch bei fast allen anderen Produkten, die nicht als Statussymbol etc. gesehen werden}. Die Annahme, dass zumeist homogene Güter auf perfekten Konkurrenzmärkten gehandelt werden, erscheint also einfach nicht der Realität zu entsprechen.
Auf Modelle hat die Berücksichtigung entscheidende Auswirkungen. Aus technischer Sicht ändert sich die Nachfragefunktion dahingehen, dass diese nicht mehr perfekt elastisch ist, wie auf "`Perfekten Konkurrenzmärkten"'. Das heißt, Änderungen im Preis einer einzelnen Firma führen zwar dazu, dass die Firma weniger Produkte verkauft, aber nicht mehr schlagartig gar keine Produkte mehr. Die Firmen können daher ihren Gewinn individuell maximieren (Grenzertrag = Grenzkosten). Die Firmen können ihren Preis - wenn auch in einem gewissen Rahmen - frei wählen, sie sind Preissetzer. Das heißt sie müssen nicht sofort auf Änderungen des Marktpreises reagieren um überhaupt noch Produkte zu verkaufen\footnote{Dies ermöglicht erst, dass Preise in irgendeiner Form rigide sind, ist also Voraussetzung für die Aspekte, die wir als "`Rigiditäten"' oder "`Menu Costs"' kennengelernt haben.}. Märkte mit "`perfekter Konkurrenz"' sind somit effizienter als Märkte mit monopolistischer Konkurrenz: Die Gewinn-optimale Output-Menge ist niedriger als auf "`Perfekten Konkurrenzmärkten"'. Die Preise sind etwas höher, womit positive Gewinne möglich sind, was wiederum dazu führt, dass auch nicht-effiziente Firmen überleben können. Diese Aspekte sind rein mikroökonomischer Natur. \textcite{Hart1982} analysierte theoretisch, die Auswirkung der Berücksichtigung "`monopolistischer Konkurrenz"' auf die Ergebnisse makroökonomischer Modelle.

Die Anfänge "`Monopolistische Konkurrenz"' in makroökonomischen Mainstream-Modellen zu berücksichtigten waren schwierig. Ein erstes, rein technisches Modell, \textit{wie} monopolistische Konkurrenz berücksichtigt werden kann, lieferten \textcite{Dixit1977}. Darin zu finden ist ein bis heute gängiges Instrumentarium, noch ohne Bezug zu makroökonomischen Modellen. Erst in den 1980er Jahre wurden deren Modell langsam wiederentdeckt und in makroökonomischen Modellen angewendet. In der Realität war es ja schon seit jeher unumstritten, dass Preise vom Verkäufer festgesetzt werden. Also hat man in frühen neu-keynesianischen Makro-Modellen \parencite[S. 97]{Hart1982} zunächst ebenfalls Preise festgesetzt, mit der Konsequenz, dass die Nachfrage und das Angebot entsprechend schwanken sollten. Das war aber nicht in Einklang zu bringen mit der Annahme rationalen Verhaltens. Aus welchen Gründen sollte zum Beispiel ein Anbieter seine Preise nicht anpassen, wenn er mehr verkaufen möchte? Oliver Hart\footnote{Dies ist tatsächlich jener Oliver Hart, der 2016 den Wirtschafts-Nobelpreis für ein ganz anderes Thema, nämlich die Vertragstheorie, erhalten hat.} war der erste, der im Jahr 1982 dieses Problem umging, indem er eben die Annahme des perfekten Wettbewerbs und die damit verbundenen vollkommenen Elastizitäten, fallen ließ \parencite[S. 110]{Hart1982}. In seinem Modell führen Änderungen der aggregierten Nachfrage zu Änderungen im Gesamt-Output, die nicht sofort durch Änderungen im Preisniveau- und Zinsänderungen sofort wieder ausgeglichen werden. 
Geldpolitik ist damit - entgegen den Annahmen der "`Neuen Klassikern"' - ein wirksames Mittel der wirtschaftspolitischen Steuerung. Ein bahnbrechendes Ergebnis: In Neu-klassischen Modellen mit perfektem Wettbewerb ist es nämlich genau umgekehrt: Änderungen der aggregierten Nachfrage führen zu Preisniveau- und Zinsänderungen, die wiederum dafür sorgen, dass der Output gleich bleibt und sofort nur Preisniveau-Änderungen resultieren. 
Das Ergebnis seines mathematisch-theoretischen Modells war erst der Anfang einer langen Reihen von theoretischen und später auch empirischen Arbeiten im Bereich des "`Imperfekten Wettbewerbs"'. Es zeigte außerdem, dass die Ökonomie bei Unterauslastung zu einem Gleichgewicht finden kann. Eine Erkenntnis, die der "`Neuen Klassik"' ebenso klar widerspricht.  Damit ist Arbeitslosigkeit weder notwendigerweise ausschließlich freiwillig, wie bei den Neuen Klassikern, aber auch nicht ein vorübergehendes Phänomen, wie bei Keynes. Hart zeigt im Paper außerdem, dass budget-neutrale Fiskalpolitik (also die Stimulierung der Wirtschaft durch staatliche Ausgaben, die aber durch Steuern gegenfinanziert wird und somit nicht zu Budgetdefiziten führt) wirksam sein kann. Außerdem schließt er seinen Artikel damit, dass durchaus Keynesianische, aber auch Neoklassische und eben auch Post-Keynesianische Elemente darin vorkommen. 
Die Arbeit von \textcite{Hart1982} lieferte wichtige \textit{theoretische} Ergebnisse. Denken wir noch einmal zurück: Was war die ursprüngliche Motivation der "`Neu-Keynesianer"'? Sie fanden sich in einer Welt, in der die "`Neuen Klassiker"' ein theoretisch überlegenes Modell lieferten. Mit rationalen Erwartungen, Mikrofundierung und mathematisch fundierten Ergebnissen. Alleine die Modellannahmen, insbesondere, dass es auch allen Märkten stets zu effizienten Gleichgewichten käme und in der Folge Geldpolitik und Fiskalpolitik keinerlei Wirksamkeit hätten und die Wirtschaft auf einem stabilen langfristigen Wachstumskurs verläuft, waren unrealistisch. Hier sprangen die Neu-Keynesianer auf den Zug auf: Die Wirklichkeit zeigte deutlich, dass Schwankungen der aggregierten Nachfrage zu Änderungen beim Gesamtoutput führten. Also zu Konjunkturschwankungen, die im ausgeprägten Fall Wirtschaftskrisen darstellten. Die Neu-Keynesianer wollten zwar die Mikrofundierung, die rationalen Erwartungen und die eleganten mathematischen Modelle übernehmen, hielten aber die Modellannahmen für unrealistisch. Und hier kommen nun ihre punktuellen Lösungen ins Spiel: Der soeben vorgestellte Artikel von Hart zeigte, dass es Gleichgewichte ohne "`Perfekten Wettbewerb"' geben konnte - quasi aus rein mathematischer Sicht. Aber sind diese Ergebnisse in der Realität auch wirklich relevant? Dies analysierten \textcite[S. 647]{Blanchard1987}: \textit{"'Monopolistic competition provides a convenient conceptual framework in which to think about price decisions, and appears to describe many markets more accurately than perfect competition. But, how important is monopolistic competition for macroeconomics?"'}. Konkret stellen sich zwei Fragen: Erstens, Kann die Berücksichtigung von "`Monopolistischer Konkurrenz"' tatsächlich erklären warum Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Diese Frage hatte \textcite{Hart1982} eben nur auf theoretischer Ebene behandelt. \textcite{Blanchard1987} analysierten ob der Effekt groß genug sei um empirisch eine Rolle zu spielen. Und sie kamen zu dem ernüchternden Ergebnis, dass dies nicht der Fall sei. Aber \textcite{Blanchard1987} erweiterten ihr Modell und untersuchten, zweitens, ob Interaktionen zwischen mehreren Marktunvollkommenheiten - konkret das auftreten "`Monopolistischer Konkurrenz"' und gleichzeitig das Auftreten von "`Rigiditäten"' -  dazu führen konnte, dass Änderungen der aggregierten Nachfrage zu Änderungen des Gesamtoutputs führen? Dies konnten die beiden mittels Modell bejahen. Obwohl die beiden Autoren am Schluss ihrer Arbeit \parencite[S. 663]{Blanchard1987} die Limitationen ihrer Arbeit anführen, gehen sie richtigerweise davon aus, dass ihr Beitrag im Speziellen und "`Monopolistische Konkurrenz"' im Allgemeinen hilft, die empirisch beobachtbaren Konjunkturschwankungen zu erklären. Die im Paper vollzogene gemeinsame Analyse von "`Menu Costs"' und "`Monopolistischer Konkurrenz"' zeigt auch, dass makroökonomische Realitäten schwer zu modellieren sind. Es gibt nicht den einen Auslöser von Konjunkturschwankungen. Erst die Kombinationen von mehreren Marktunvollkommenheiten führen zu empirisch ansprechenden Ergebnissen. Manche dieser Marktunvollkommenheiten treten zudem nur wechselseitig auf, bedingen sich also gegenseitig. So wie wir gesehen haben, dass Rigiditäten nur dann sinnvollerweise auftreten können, wenn die Voraussetzungen eines "`perfekten Konkurrenzmarktes"' nicht erfüllt sind. Heutige Mainstream-Makro-Modelle berücksichtigen in der Folge üblicherweise sowohl "`Rigiditäten"' als auch "`Monopolistische Konkurrenzmärkte"'.





\section{Arbeitslosigkeit als Suchproblem}
\label{Suchtheorie}

Ursprünglich mikroökonomisch als "`Friktion auf Suchmärkten"'. In der Makro dann "`Matching Theory"'

Coordination Failures
Diamond 1982: Coconut Modell

Nobelpreisrede Diamond zur Verbindung zu Mortensen (Power of Poisson-distribution)
Und Verbindung zur "`Phelps-Volume"'



Direkt angestoßen von Phelps. Aber erst wirklich interessant als Gegenposition zur "`Neuen Klassik"'.
\textcite[S. 683]{Phelps1968}

Microeconomic foundations for wage and price setting
Phelps (1968a, 1970b) derived aggregate wage-setting behavior from detailed modeling of the 
behavior of individual agents. Jobs and workers are heterogeneous and there is imperfect 
information on both sides of the market. Markets are assumed to be almost atomistic, but there is 
no “Walrasian auctioneer” that instantaneously finds the wages (and prices) that clear all markets 
(as went the metaphor used in earlier general equilibrium analysis). Instead, wages are set by 
firms that are able to exercise temporary monopsony power. Workers and firms meet randomly 
at a rate determined by the number of unemployed workers searching for a job and the number of 
vacancies, according to a function that would today be recognized as a matching function. 
Phelps’s work here is a precursor of the search and matching theory of unemployment, where 
Peter Diamond, Dale Mortensen, and Christopher Pissarides have made especially important 
contributions. \parencite{Nobelpreis-Komitee2006}  See, for example, Diamond (1984), Mortensen (1982a, b), Mortensen and Pissarides (1994), and Pissarides 
(2000).


\subsection{Diamond, Mortensen und Pisaridis}

Widersprach rasch der "`Neuen Klassik"' indem sie die natürliche Arbeitslosigkeit erweiterte.

Poisson-Verteilung als Instrument entdeckt (Nobelpreisrede Diamond)



\section{Der Neu-Keynesianische Konjunkturzyklus}

Nach:
\textcite{Yellen1984}
\textcite[S. 396]{Snowdon2005}



\section{Wirkung und Bedeutung des Neu-Keynesianismus}
Eventuell "`New Keynesian Business Cycle Theorie"' Seite 396 in Snowdon/Vane

Interessant, dass Fiskalpolitik praktisch keine Rolle spielt. Aussage dazu von Mankiw in: \parencite[S. 446]{Snowdon2005} Zwar Akzeptiert man, dass Staatsausgaben kurzfristig prozyklisch wirken \parencite[S. 408]{Snowdon2005}, allerdings verursachen sie langfristig Kosten und sind dann schädlich für Wachstum. Geldpolitik hingegen verursacht nur Kosten, wenn daraus Inflation entsteht. Daraus leitete sich in weiterer Folge \parencite[S. 413]{Snowdon2005} die Politik der Inflationssteuerung her, die heute durch praktisch alle führenden Zentralbanken praktiziert wird. (Allerdings auch durch Monetaristen und Neue Klassiker)

Gewachsen: Anfang Nominale Rigiditäten --> Monopolisitsiche Konkurrenz (Imperfect Competition) --> Menu Costs --> Real Rigidtäten --> Fehlende Klassische Dichotomie --> Nicht-Neutralität des Geldes.

Dazu Speziall-Forschungsgebiete: 

1. Reale Rigiditäten: Arbeitsmarkt, Kreditmarkt, Gütermarkt
2. Monopolistische Konkurrenz: Coordination Failures (Überbegriff)
