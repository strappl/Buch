%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neue Klassische Makroökonomie}
\label{Neue Makro}

\section{Lucas' Kritik und Sargent's Beitrag}
In Bezug auf die dogmengeschichtliche Einordnung könnte man argumentieren, die "`Neue Klassische Makroökonomie"' wäre eine Weiterentwicklung des Monetarismus. Dafür sprechen aber eigentlich nur ideologische und geografische Gründe. Die Vertreter beider Schulen, also des Monetarismus und der Neuen Klassischen Makroökonomie, sind zumindest wirtschaftspolitisch dem Liberalismus zuzuordnen. Außerdem war Robert E. Lucas Student und später Professor an der University of Chicago, gehörte also auch dem großen und erfolgreichen Zirkel von Ökonomen an, die nach Frank Knight und Milton Friedman in Chicago lehrten. Allerdings kritisierte Robert Lucas den Monetarismus zu vehement \parencite[S. 121]{Lucas1972}, als dass man bei der Neuen Klassischen Makroökonomie von einer Erweiterung des Monetarismus sprechen könnte.  

Entscheidend ist aber ohnehin die inhaltliche Sichtweise und hier unterscheiden sich die beiden Schulen doch entscheidend. Vor allem in der verwendeten Methodik brachte die "`Neue Klassik"' eine Revolution und sie brach gleich an mehreren Stellen mit den bisherigen Usancen der Ökonomie und ging ökonomische Fragestellungen grundlegend anders an, als sowohl Keynesianer als auch Monetaristen. Am bekanntesten ist das Beispiel der Phillips-Kurve: Der negative Zusammenhang zwischen Inflation und Arbeitslosigkeit. Keynesianische Wirtschaftspolitik akzeptierte eine hohe Inflation, weil damit eine niedrige Arbeitslosigkeit verbunden sei. Tatsächlich ließ sich dieser Zusammenhang überraschend eindeutig bis Ende der 1960er Jahre feststellen. Danach aber folgten der Ölpreisschock und Jahre der "`Stagflation"'. Also Jahre in denen es zwar kaum Wirtschaftswachstum, aber sowohl hohe Inflation als auch hohe Arbeitslosigkeit gab. Die Monetaristen um Milton Friedman (aber auch der Neu-Keynesianer Edmund Phelps) griffen die Keynesianer in Bezug auf die Phillips-Kurve an und argumentierten es mache auch schon theoretisch keinen Sinn einen langfristigen Zusammenhang zwischen der nominalen Größe Inflation und der realen Größe Arbeitslosigkeit anzunehmen. Inflation kann daher nicht kausal für niedrige Arbeitslosigkeit sein kann. Die Stagflation der 1970er-Jahre deckte die Schwächen der damaligen Makroökonomie auf. Aber auch die Monetaristen konnten die empirischen Vorgänge nicht befriedigend erklären.

Stattdessen war die Geburtsstunde der "`Neuen Klassischen Makroökonomie"' gekommen. In seiner wahrlich bahnbrechenden "`Lucas-Kritik"' \parencite[S. 19ff]{Lucas1976} zeigte er, dass es geradezu naiv sei zu glauben Arbeitnehmer würden Verträge abschließen in denen Löhne festgeschrieben werden, die aus heutiger Sicht zwar "`fair"' sind, aber durch Inflation in einem Jahr deutlich weniger Kaufkraft hätten. Ebenso naiv sei es zu glauben, dass Arbeitgeber Verträge abschließen nur weil sie wissen, dass die darin festgelegten Löhne in einem Jahr ohnehin real viel geringer seien. Nein! Beide Parteien, sowohl Arbeitnehmer als auch Arbeitgeber, wissen um den Einfluss der Inflation auf die Kaufkraft Bescheid und lassen ihre entsprechenden \textit{Erwartungen} -- selbstverständlich -- auch in die Lohnverhandlungen einfließen. Aus diesem Beispiel lassen sich auch die, für die Neue Klassische Makroökonomie so charakteristischen, Grundannahmen ableiten: Erstens, die "`Rationalen Erwartungen"', zweitens, die Betonung des "`natürlichen Gleichgewichts"' und der daraus folgenden Wirkungslosigkeit von Fiskal- und Geldpolitik und drittens der Mikrofundierung der Makroökonomie.

\begin{enumerate}
	\item Rationale Erwartungen: Der Begriff der "`Rationalen Erwartungen"' wurde eigentlich schon durch \textcite{Muth1961} begründet und von \textcite{Lucas1972} bereits erstmals als für die gesamte Makroökonomie gültiges Theorem vorgeschlagen. Ab Mitte der 1970er Jahre, vor alle durch die Arbeiten von \textcite{Lucas1976} und \textcite{Sargent1975}, wurde das Konzept der Rationalen Erwartungen etabliert. Seit damals gelten sie als \textit{die} wesentliche Neuerung durch die Neue Klassische Makroökonomie und zählen - obwohl nicht unumstrittenen - zum Kern der modernen Mainstream-Ökonomie. So leicht sind die "`Rationalen Erwartungen"' gar nicht abzugrenzen. Den (mikroökonomischen) \textit{Homo Oeconomicus} -- also den rational \textit{entscheidenden} Mensch -- gab es in der Ökonomie schließlich schon lange. Auch die Bedeutung von Erwartungen war nicht neu. Bei Keynes zum Beispiel wurden Änderungen der Zukunftserwartungen als "`Animal Spirits"' bezeichnet. Diese waren bei Keynes ein wichtiges Konzept, dass aber als nicht modellierbar akzeptiert wurde. Die \textit{rationalen Erwartungen} umfassen eben mehr. Während der Homo Oeconomicus nutzen-maximierend vergangenheitsorientierte Information auswertet\footnote{Häufig wird zwischen "`Adaptiven Erwartungen"' und eben "`Rationalen Erwartungen"' unterschieden. Bei ersteren Konzept ziehen die Leute ausschließlich vergangene Daten heran und schreiben diese in die Zukunft fort. Beim zweiten Konzept leiten die Leute hingegen wahrscheinliche zukünftige Handlungen ab um ihre Zukunftserwartungen herzuleiten.}, umfassen die Rationalen Erwartungen die Tatsache, dass sich Menschen auch rational verhalten was die Informationsgewinnung betrifft und entsprechend rationale "`Vorhersagen"' zur zukünftigen Entwicklung wirtschaftlicher Aspekte treffen. Das heißt, eine bestimmte Wirtschaftspolitik bestimmt auch die Erwartungen der Menschen. Änderungen der Wirtschaftspolitik führen dementsprechend auch zu Änderungen der Erwartungshaltungen. Zusammengefasst: Erstens, Menschen machen vorhersagen, ohne dabei systematische Fehler zu begehen (\textcite{Lucas2013}: Interview mit Lucas: "`Die Leute sind nicht verrückt"'). Das heißt, der Staat kann seine Bewohner nicht systematisch "`austricksen"'. Zweitens, Menschen bauen alle verfügbaren Informationen und Wirtschaftstheorien in ihre Entscheidungen ein. Diese Annahme ist umstritten. Auf Beispiele herunter gebrochen bedeuten Rationale Erwartungen folgendes:
	Hohe Budgetdefizite durch expansive Fiskalpolitik führen dazu, dass die Menschen steigende Steuerbelastung erwarten und ihre Sparquoten erhöhen. Anstatt der von der Politik erhofften Konjunktur-belebenden Wirkung führt die Fiskalpolitik zu einer Verdrängung privater Ausgaben durch staatliche Ausgaben. Politiker, die eine hohe Inflation bewusst nutzen wollen um die Arbeitslosenraten zu senken, werden enttäuscht: Erwarten die Menschen höhere Inflationsraten, fordern sie in den Lohnverhandlungen eine entsprechende Abgeltung dafür, womit das bewusste Ausnutzen der hohen Inflation zugunsten niedriger Arbeitslosenzahlen scheitert. Die neue klassische Makroökonomie konnte damit elegant das konkrete Phänomen der "`Stagflation"' der 1970er Jahre erklären. 	 
	Diese beiden Beispiele zeigen den enormen Nebeneffekt der Rationalen Erwartungen: Nämlich, dass weder Fiskalpolitik noch Geldpolitik\footnote{Nur wenn die politischen Handlungen absolut unvorhergesehen erfolgen, können damit kurzfristig die erwünschten Effekt eintreten.}, einen stabilisierenden Einfluss auf die gesamtwirtschaftliche Entwicklung haben \parencite{Sargent1975, Barro1976}. Wirtschaftspolitik \textit{steuert} nicht länger die ökonomische Entwicklung, sondern ist nur ein \textit{Player} in einem Spiel zwischen Politik und den Markteilnehmern \parencite{Kydland1977}. Dies brachte die Spieltheorie in die Makroökonomie, was ebenfalls dem Zeitgeist entsprach und die Neue Klassische Makroökonomie noch "`sexier"' machte.	
	Abgesehen davon ist dies aber natürlich auch der absolute Bruch mit den Lehren des Keynesianismus.
	Die Wirkungslosigkeit jeglicher damals bekannter wirtschaftspolitischer Elemente, führt uns direkt zum zweiten wesentlichen Punkt der Neuen Klassischen Makroökonomie.
	
	\item Dynamisches natürliches Gleichgewicht: Sowohl keynesianische als auch monetaristische Modelle akzeptierten, dass Lohn- und Preisanpassungen auf den Märkten mit einer gewissen \textit{zeitlichen} Verzögerung eintraten. Eine Erhöhung der Geldmenge zum Beispiel führe demnach zunächst zu einer Erhöhung der Produktion, gleichzeitig zu einer Verringerung der Arbeitslosigkeit und erst in weiterer Folge zu höheren Nominallöhnen und höheren Preisen. Erst nach mehreren Runden dieser Anpassungsprozesse sind die Löhne und Preise wieder im Gleichgewicht.
	In der "`Neuen Klassischen Makroökonomie"' gibt es aufgrund der rationalen Erwartungen diese langfristigen Anpassungsprozesse nicht. Dementsprechend sind alle Märkte auch in der kurzen Frist im Gleichgewicht und vollständige Konkurrenzmärkte\footnote{Auf diese Annahmen ist auch der Name "`Neue \textit{Klassische} Makroökonomie"' zurückzuführen.}. 
	Dies wiederum impliziert, dass Konjunkturschwankungen ausschließlich durch exogene Schocks verursacht werden. Diese Implikation ist notwendig, da Konjunkturschwankungen ständig empirisch zu beobachten sind, bei vollständiger Konkurrenz mit flexiblen Löhnen und Preisen müssten schließlich die Lohn- und Preisanpassungen laufend zu Markträumung und Glättung der Konjunkturschwankungen führen. Die "`Real Business Cycle"'-Theorie von Edward Prescott formalisierte diese Annahmen (siehe Kapitel \ref{RBC}).
	
	Dieser zweite Kernpunkt der "`Neuen Klassischen Makroökonomie"' wurde allerdings zum "`Sargnagel"' dieser ökonomischen Schule. Anfang der 1980er Jahre sah es so aus, als würde die Neue Klassische Makroökonomie zur alleinigen Mainstream-Ökonomie aufsteigen. Aber dafür stellten sich die vorausgesetzten Annahmen als zu realitätsfern heraus. Dass alle Märkte vollkommene Konkurrenzmärkte sind und sich ständig im Gleichgewicht befinden, ist einfach zu weit weg von täglichen Beobachtungen: Erstens war diese Annahme nicht vereinbar mit der empirischen Beobachtung von Arbeitslosenraten von fast 10\% in den USA der frühen 1980er Jahre. Zweitens, zeigten der Neukeynesianer \textcite{Fischer1977} mittels formaler Modelle, dass sich Löhne und Preise auch bei Berücksichtigung der Theorie der Rationalen Erwartungen nur langsam an veränderte Arbeitslosenraten anpassen.
	Und drittens, erwiesen sich die "`Real Business Cycle"' Modelle bald als wenig treffsicher. Sie implementierten die oben beschriebenen Annahmen, dass es ständig zu Markträumung kommt und es keine natürliche Arbeitslosigkeit gibt. Die Konjunkturzyklen würden dann primär durch Schwankungen im technischen Fortschritt verursacht. Die Modelle konnten zwar vor allem methodisch überzeugen, aber die empirisch beobachtete Konjunkturschwankungen nicht erklären.
	Selbst innerhalb der Neuen Klassischen Makroökonomie akzeptierte man bald, dass die Annahmen zu starr sind und verwarf einige davon\footnote{Dies "`nutzten"' die Neu-Keynesianer, die pragmatisch die bahnbrechenden Erkenntnisse der Neuen Klassik übernahmen, aber realistischere Marktannahmen zu Märkten, Rigiditäten, Arbeitslosigkeit und Wirtschaftspolitik verwendeten}. Was exogene Wachstumsmodelle betrifft, überließen die Neuen Klassiker bald den Neu-Keynesianern das Feld. Innerhalb der Neuen Klassik wendete man sich den "`Endogenen Wachstumstheorien"' zu.
	
	\item Mikrofundierung der Makroökonomie. Die keynesianischen und monetaristischen Modelle verwendeten typische makroökonomische Kennzahlen, wie zum Beispiel Bruttoinlandsprodukt, Konsum, Investitionen und Sparen. Das Zusammenspiel dieser Kennzahlen wurde häufig mit sogenannten "`Ad-hoc-Annahmen"' modelliert. Das heißt, es wurden Zusammenhänge herangezogen, die nicht weiter begründet wurden. Ein Beispiel ist die Einkommenshypothese nach Keynes, wonach Der Konsum eine - nicht weiter spezifizierte - Funktion des derzeitigen Einkommens sei. Zum Teil wurden einzelne dieser Kennzahlen im Laufe der Zeit abgeändert dargestellt. So erweiterten die "`permanente Einkommenshypothese"' von Milton Friedman und die "`Lebenszyklushypothese"' von Franco Modigliani die eben genannte keynesianische Konsumtheorie. Aber es blieb immer bei der Heranziehung statischer makroökonomischer Faktoren. Natürlich wusste man auch vor Lucas' Kritik, dass jede Entscheidung nicht ausschließlich auf statischen, gegenwärtigen Fakten basierten und Vermutungen über die Zukunft blieben nicht völlig ausgeklammert. Aber doch war Lucas' Kritik ein entscheidender Anstoß für ein Umdenken innerhalb der Ökonomie von statischen Überlegungen zur Implementierung dynamischer Erwartungen. Dementsprechend wurden auch ökonometrische Modelle völlig neu gedacht. Lange Zeit dominierten zuvor in der Makroökonomie statische keynesiansche Totalmodelle, beziehungsweise die neoklassischen, mikroökonomischen Walrasianischen Gleichgewichtsmodelle. 
	
	Die Neue Klassische Makroökonomie brachte auch hier eine Revolution:\footnote{In den Ökonomie-Lehrbüchern sah man das daran, dass das Standardmodell der neoklassischen Synthese, das IS-LM-Modell, zunehmend durch das einfachste mikrofundierte Modell, das AS-AD-Modell ersetzt wurde.} Die Mikrofundierung der Makroökonomie. Heute sind praktisch alle Makroökonomischen Modelle mikroökonomisch fundiert. Konkret bedeutet dies, dass man die Konzepte aus der Mikroökonomie, also die Nutzenmaximierung aus der Haushaltstheorie und die Gewinnmaximierung aus der Unternehmenstheorie heranzieht. Man kann aber nicht alle Individuen auf allen Märkten beobachten und deren Verhalten zu einem "`Gesamtverhalten"' aggregieren, also aufsummieren. Stattdessen behilft man sich eines \textit{repräsentativen Agenten}, also ein "`Haushalt"' oder "`Unternehmen"', der/das typisches Verhalten zeigen würde. Mit den in der Mikroökonomie üblichen Techniken wird dann \textit{optimales} Verhalten des Agenten modelliert. Das heißt, man nimmt an, dass alle Modellgleichungen auf \textit{konsistenten Annahmen} beruhen. In diesem Umfeld optimiert der Agent sein Verhalten, wobei er \textit{rationale Erwartungen} über zukünftige Entwicklungen hat. Der Term rational ist hier im Sinne von "`stochastisch berechenbar"' zu verstehen. Die Modelle sind dynamisch, der repräsentative Agent passt also sein Optimierungsverhalten schlagartig auf Veränderungen in seinen Erwartungen oder den konsistenten Annahmen an und die einzelnen Gleichungen beeinflussen sich gegenseitig. Damit ist das Verhalten des repräsentativen Agenten konsistent mit den Vorhersagen des Modells. 
	
	Der Leser mag sich aufgrund der komplizierten Formulierungen und Vielzahl an Annahmen denken, dass die Ökonomie mit diesen Modellen den Bezug zur Realität vollkommen verloren hat. Höheren Mathematik war endgültig in ökonomischen Modellen angekommen. Dies entsprach und entspricht dem Zeitgeist. Die Ökonomie wurde in der Folge zunehmend als Naturwissenschaft oder gar Formalwissenschaft betrieben, immer weniger als Sozialwissenschaft. Ein Umstand der seither mit wechselnder Vehemenz kritisiert wird, ob zu Recht oder zu Unrecht muss jeder Leser für sich entscheiden. Im Mainstream haben sich diese Modelle auf jeden Fall fest etabliert. Neben der Neuen Klassischen Makroökonmie, setzten auch die Neu-Keynesianer auf diese Art von ökonomischen Modellen. Die heute so häufig herangezogenen neukeynesianischen "`Dynamischen stochastischen allgemeinen Gleichgewichtsmodelle"' unterscheiden sich zwar deutlich von den dynamischen Gleichgewichtsmodellen der "`Neuen Klassischen Makroökonomie"', basieren aber im wesentlichen auf deren Ideen.  
	Man könnte sogar so weit gehen, dass diese Art der ökonomischen Modelle den Mainstream von den heterodoxen Schulen trennt. Sowohl die österreichische Schule als auch die Post-Keynesianer und natürlich die Verhaltensökonomen, lehnen diesen stark formalisierten Zugang jedenfalls strikt ab.


\end{enumerate}	


Die Neuen Klassiker erlebten rasch einen enormen Aufschwung und enorme Beachtung. Rasch war klar, dass ihre formell tatsächlich sehr schön dargestellten und abgehandelten Modellen, den keynesianischen und monetaristischen Modellen formal überlegen waren.
Vor allem die Mikrofundierung der Makroökonomischen Modelle stellte einen deutlichen und nachhaltigen Fortschritt dar. Schließlich akzeptierten auch die Keynesianer die modelltheoretische Überlegenheit\footnote{Die Implementierung keynesianischer Ideen in die Modellannahmen der Neu Klassiker machte die Keynesianer schließlich zu Neu-Keynesianern}
Damit konnte sich die Annahme der Rationalen Erwartungen rasch etablieren. Schließlich war dieses Konzept nicht neu, aber eben technisch schwer umsetzbar, da bei rationalen Erwartungen eine Wechselwirkung zwischen erwarteten zukünftigen Entwicklungen und heutigem Verhalten besteht. Genau das Problem wurde mit den Modellen der "`Neuen Klassiker"' gelöst. Und so sah es Anfang der 1980er Jahre danach aus, als würde sich die "`Neue Klassische Makroökonomie"' als neue, alleinige Mainstream-Ökonomie etablieren. Aber auch das erwies sich rasch als Trugschluss. Die Annahmen der Modelle waren einfach zu stark und zu starr als dass damit die Realität beschrieben werden konnte. 
		
Im Gegensatz zu den Neukeynesianern, die sich zu dieser Zeit ebenfalls formierten und deren Hauptthemen die verschiedenen Formen von Marktversagen waren, meinten die neuen Klassiker, dass alle Märkte selbstständig ein Gleichgewicht finden. Und das auch in der kurzen Frist! Rückbesinnung auf die Klassik eben. Dieser Punkt erwies sich rasch als nicht haltbar. Dies impliziert, dass es keine unfreiwillige Arbeitslosigkeit gäbe, was wohl unrealistisch ist. Genau das war auch der zentrale Angriffspunkt auf die Neue Klassik. "`Wenn der Arbeitsmarkt immer ins Gleichgewicht findet, dann heißt das, dass die Neuen Klassiker davon ausgehen, dass sich mitten in der "`Great Depression"' Millionen Amerikaner dafür entschieden haben jetzt mehr Freizeit zu konsumieren"' (cf \cite{Stiglitz1987}, p. 119), lautete ein hämischer Kommentar der Neu-Keynesianer. 
Warum aber war die Neue Klassik zu deren Beginn so erfolgreich? Meines Erachtens liegt der Grund hierfür in ihrer methodischen Überlegenheit gegenüber Keynesianern und Monetaristen. Wenn alle Modellannahmen eingehalten sind, dann führen die Modelle unwiderlegbar und sehr elegant zu eindeutigen Ergebnissen. Allerdings stellte man rasch fest, dass viele der Modellannahmen empirisch nicht zu halten sind. Robert Solow in einem Interview, dass in \textcite[S. 146]{Klamer1984} veröffentlicht wurde, brachte dies wohl am besten auf den Punkt:

\textit{"'Angenommen, jemand [...] sagt zu ihnen, er sei Napoleon Bonaparte. Das Letzte, was ich möchte, ist, mich mit ihm auf eine technische Diskussion über die Kavallerietaktik in der Schlacht von Austerlitz einzulassen. Wenn ich das tue, werde ich stillschweigend anerkennen, dass er Napoleon ist. Nun, Bob Lucas und Tom Sargent mögen nichts lieber, als technische Diskussionen vorzunehmen, denn dann haben Sie sich stillschweigend auf ihre Grundannahmen eingelassen. Ihre Aufmerksamkeit wird von der grundlegenden Schwäche der ganzen Geschichte abgelenkt."'}

Gerade als sich Anfang der 1980er-Jahre die Neue Klassik als neue Mainstream Ökonomie durchzusetzen schien, stiegen die Arbeitslosenquoten in den USA auf 10\%. Das war nicht vereinbar mit der angeblich ausschließlich freiwilligen Arbeitslosigkeit. Mittlerweile rücken auch die meisten Vertreter der neuen Klassik davon ab, dass sich alle Märkte auch in der kurzen First im Gleichgewicht befinden (Zitat).
Man darf daraus jetzt aber nicht schließen, die Neue Klassische Makroökonomie sei widerlegt und verschwunden. Ganz im Gegenteil! Sie hat viele wichtige und richtige Erweiterungen der Mainstream-Ökonomie gebracht. Erstens, die Methodik wurde revolutioniert. Mikrofundierte, dynamische Gleichgewichtsmodelle wurden von den Neu-Keynesianern rasch aufgenommen, erweitert und für sich beansprucht. Die Anerkennung für die Überwindung der veralteten Makromodelle der Keynesianer und Monetaristen steht aber den Neuen Klassikern zu. Zweitens, die Theorie der Rationalen Erwartungen war - trotz aller Kritik - ein Meilenstein in der Ökonomie, der bis heute State-of-the-Art ist.

Die Veröffentlichung der Lukas-Kritik gilt als eine Revolution in der Ökonomie. Warum aber blieben deren Vertreter, allen voran Robert Lucas, in der öffentlichen Wahrnehmung eher blass? Ein Aspekt ist sicherlich, dass sowohl Keynes als auch Friedman und Hayek auch außerhalb der wissenschaftlichen Ökonomie auftraten, vor allem als Politikberater. Ein weiterer Aspekt ist aber auch die Art der Kommunikation der Vertreter der "`Neuen Klassischen Makroökonomie"'. Diese war ungewöhnlich scharf: \textit{That [the Keynesian] predictions were wildly incorrect and that the doctrine on which they were based is fundamentally flawed are now simple matters of fact}, schrieben Lucas und Sargent in ihrem Artikel \textit{After Keynesian Macroeconomics} \parencite[S. 1]{Lucas1979}. Die Ideen der Neuen Klassiker wurden schon nach wenigen Jahren in die Modelle des bisherigen Mainstreams integriert, nicht jedoch die Leute, die Stimmung innerhalb der wirtschaftswissenschaftlichen Community war in den 1970er und 1980er Jahren vergiftet, beschreibt \cite{Blanchard2003} in seinem Standardlehrbuch. Ähnlich, wenn auch etwas diplomatischer drückte sich \cite{Samuelson1998} aus. Lucas und Co kümmerten die etablierten Ökonomen wenig, es scheint als hielten sie so wirklich gar nichts von ihnen. Umgekehrt erkannten vor allem die Vertreter der neoklassischen Synthese die inhaltliche Sinnhaftigkeit der Ideen der Neuen Klassiker. Deren "`Schüler"' implementierten diese Ideen in ihre eigenen Modelle, deckten die vorhandenen Schwachpunkte der "`Neu Klassiker"' auf und wurden zu "`Neu-Keynesianern"'. Die Modelle näherten sich also an - vor allem weil die Mainstream-Ökonomen die Ideen der Neuen Klassiker aufnahmen - die dahinterstehenden Personen allerdings in keinster Weise. 

Um sich als Mainstream durchzusetzen, waren die Ideen der Neuen Klassiker zu radikal. Die gänzliche Ablehnung der Synthese aus Neoklassik und Keynesianismus erwies sich als vorschnell. Überhaupt zeigte sich der Hauptvertreter der Neuen Klassischen Makroökonomie, Robert Lucas, als wenig pragmatisch was seine ökonomische Sichtweise angeht. Im Jahre 2003 zum Beispiel veröffentlichte er einen seiner Artikel im \textit{American Economic Review} mit der These, dass \textit{macroeconomics in [the] original sense has succeeded: Its central problem of depression-prevention has been solved, for all practical purposes, and has in fact been solved for many decades} \parencite[S. 1]{Lucas2003}. 
Dass nur vier Jahre später mit der "`Great Recession"' die größte Krise seit den 1930er Jahren ausbrechen sollte, zeigte das Gegenteil. Bereits 1987 meinte er: \textit{The most poisonous [tendencies in economics], is to focus on questions of distribution} \parencite{Lucas1987}. Fragen der Einkommensverteilung sind aber seit damals tatsächlich gesellschaftlich wie ökonomisch immer bedeutender geworden. Auf die Frage, ob Ökonomie-Studierende heute noch Keynes lesen sollten, antwortete er 1998 mit einem schlichten "`No"' \parencite{Lucas2013}. Die "`Great Recession"' war somit so etwas wie die "`Widerlegung"' der reinen neuen klassischen Makroökonomie. Lucas sah die Krise, so wie zugegebenermaßen die meisten andern Ökonomen nicht nur nicht kommen, sondern, glaubte auch nicht, dass eine derart schwere Krise kommen könnte. Und während der Krise wendeten die Politiker schließlich gnadenlos keynesianisches "`Deficit Spending"' sowie eine extreme Geldpolitik - "`Quantitative Easing"' - an. 

Insgesamt darf man aus heutiger Sicht darf man aber nicht vergessen, dass diese ökonomische Schule die Wirtschaftswissenschaften tatsächlich revolutioniert hat. Viele der von ihr erstmals vorgebrachten Elemente wurden rasch vom Großteil der Ökonomen aller Richtungen übernommen und sind heute aus der Mainstream-Ökonomie nicht mehr wegzudenken. Robert Lucas zählt daher meines Erachtens zu den größten Ökonomen des 20. Jahrhunderts. Seine Arbeiten sind nicht so einprägend wie die "`General Theory"' von Keynes. Sein Auftritt ist nicht so überzeugend wie jener von Milton Friedman, der durch seine politischen Tätigkeiten auch weit außerhalb der wirtschaftswissenschaftlichen Community bekannt wurde. Aber Robert Lucas stand den beiden in nichts nach. Seine Ideen revolutionierten die Ökonomie des 20. Jahrhunderts und machten daraus eine andere Wissenschaft. Keynes wird oft als genialer "`Lebemann"' dargestellt. Er starb schon zehn Jahre nach der Veröffentlichung seine bahnbrechenden Werkes und musste es selbst nicht mehr gegen Angriffe verteidigen. Vielleicht ist er auch deshalb so populär. Friedman wird sehr kontrovers gesehen. Von den Liberalen noch im hohen Alter als Ikone gefeiert, durch seine Beratertätigkeit oft jedoch auch verhasst. Vor allem aber war er ein brillanter Redner mit charismatischen Auftritt. All das ist nicht die Stärke von Robert Lucas. Seine oben zitierten Aussagen scheinen eher unglücklich formuliert. In seinen Auftritten erscheint er sympathisch, aber nicht als der große Vortragende. Robert Lucas war dafür ein brillanter Wissenschaftler. Seine messerscharfen formalen Abwandlungen prägten Generationen von Studierenden und waren bei seinen Gegnern gefürchtet. Dafür gebührt ihm bleibende Anerkennung und Wertschätzung in der Ökonomie.


\section{Real Business Cycle Theorie}
\label{RBC}
Dass die Makroökonomie auf der Basis von Mikrofundierung ein Fortschritt gegenüber den alten keynesianischen und auch monetaristischen Modellen sei, geht bereits auf Edmund Phelps und eben Robert Lucas zurück. Aber \textit{entwickelt}, und somit in die praktische Anwendung umgesetzt, wurden diese Modelle von \textsc{Edward Prescott} und \textsc{Finn Kydland}. Diese veröffentlichten zusammen zwei bahnbrechende Artikel \parencite{Kydland1977, Kydland1982}. Die erstmalige Entwicklung dynamischer, mikroökonomisch basierter, ökonometrischer Modelle ist die wahre Errungenschaft, die die beiden getätigt haben. Diese Modelle waren die Vorläufer der "`Dynamischen, stochastischen, allgemeinen Gleichgewichtsmodelle (DSGE)"'. Diese gelten bis heute als der Goldstandard der Konjunkturprognose-Modelle, wenn auch die Parameter wesentlich erweitert wurden. (Siehe Kapitel \ref{Neue Neoklassische Synthese})

Kydland und Prescott lieferten ein Modellfundament für zwei wesentliche Bausteine der "`Neuen Klassischen Makroökonomie"': In \textcite{Kydland1977} formalisierten die beiden was ursprünglich Robert Lucas postuliert hatte: Nämlich, dass aktive Wirtschaftspolitik (also sowohl Geldpolitik als auch Fiskalpolitik) nicht den erwünschten stabilisierenden Effekt hat. Im Gegenteil durch rationale Erwartungen und eine Zeitverschiebung (lag) zwischen Beschluss, Umsetzung und Wirksamkeit wirtschaftspolitischer Maßnahmen käme es laut Kydland und Prescott sogar dazu, dass aktive Wirtschaftspolitik im Endeffekt destabilisierend wirke \parencite[S. 486]{Kydland1977}. Dieses "`Zeitinkonsistenz-Problem"' fand es rasch in die Lehrbücher zur Wirtschaftspolitik als "`Inside Lag"' - also die Zeit, die vergeht, bis sich eine Regierung oder eine Zentralbank durchringen kann wirtschaftspolitische Entscheidungen zu treffen - und "`Outside Lag"' - also der Zeitraum der notwendig ist, bis die fiskalpolitischen oder geldpolitischen Maßnahmen tatsächlich wirken. Als Antwort auf dieses Problem kommen die beiden im Artikel - im Einklang mit \textcite{Lucas1976} - zu dem Schluss, dass eben "`Regeln statt [wirtschaftspolitischer] Entscheidungen"' die wirtschaftliche Entwicklung stabilisiere.

In ihrer zweiten großen Arbeit \parencite{Kydland1982} etablierten sich die beiden schließlich als Hauptvertreter der "`Real Business Cycle"'-Theory. Eine ganz ähnliche Arbeit lieferten \textcite{Plosser1983}. Nachdem die Neuklassiker und eben die beiden selbst \parencite{Kydland1977} vorgeschlagen hatten eine langfristige, auf Regeln aufgebaute, Wirtschaftspolitik durchzuführen, benötigte man ein Modell, das prognostizierte wie sich die Ökonomie unter diesen Umständen entwickeln würde. In den Jahrzehnten davor, also die 1960er und die 1970er Jahre, herrschte auf der einen Seite die keynesianische Ansicht vor, dass es Aufgabe der Wirtschaftspolitik war die Konjunkturzyklen zu glätten. Auf der anderen Seite gab es seit 1956 relativ unumstritten das "`Solow-Wachstumsmodell"', mit dem langfristiges Wachstum fast ausschließlich durch technologischen Fortschritt generiert würde.
Kydland und Prescott argumentierten nun, entgegen dem vorherrschen keynesianischen Mainstream, dass Konjunktureinbrüche nicht aufgrund fehlender Nachfrage verursacht würden, sondern vor allem durch angebotsseitige Schocks, wie zum Beispiel plötzlich steigende Rohstoffpreise, oder exogene Faktoren wie Naturkatastrophen. Die Konjunkturzyklen wären demnach keine Folge, erstens keine Folge eine nicht-funktionierende Wirtschaft und zweitens, diese exogenen Schocks würden rein zufällig auftreten. Schließlich haben Naturkatastrophen zwar Auswirkung auf die Wirtschaft, aber umgekehrt kann man diese nicht mit ökonomischen Modellen vorhersagen oder diese in ökonomische Modelle implementieren. Die "`Real Business Cycles"' sind demnach Folgen von Faktoren, die außerhalb der Ökonomie liegen, haben aber einen Effekt auf diese Ökonomie. Das eben genannten impliziert auch, dass die Abschwünge im Konjunkturzyklus rein zufällig auftreten. Da die exogenen Faktoren, zum Beispiel Naturkatastrophen nicht von der Ökonomie abhängen und vorhergesagt werden, treffen sie eine Ökonomie gezwungenermaßen vollkommen unerwartet, also wie zufällig. Genau wie in der neoklassischen Finance (siehe Kapitel XXX) sich Aktienkurse nach einem "`Random Walk"' bewegen, bewegt sich der gesamte Konjunkturzyklus ebenso nach einem reinen Zufallspfad.

Dies waren die zusätzlichen, theoretischen Implikationen, die Kydland und Prescott im Artikel von 1982. Fassen wir zusammen, welche Bedingungen Kydland und Prescott festlegten:

Das Modell stand in der Tradition der "`Neuen Klassischen Makroökonomie"', dementsprechend gelten die drei bereits genannten Ausgangspunkte, die hiermit nur kurz angeführt werden:
\begin{enumerate}
\item Dynamisches Modell
\item Mikrofundiertes Modell
\item Rationale Erwartungen werden berücksichtigt
\end{enumerate}
Des weiteren wurden aber noch folgende Punkte zusätzlich festgelegt.
\begin{enumerate}
\item[4.] Es wird davon ausgegangen, dass die Märkte effizient sind. Das ist in Verbindung mit Rationalen Erwartungen zu sehen: Wenn keine systematischen Fehler bei den Marktbewertungen passieren, dann sind die Märkte eben effizient in der Markträumung. Dies führt uns zu den nächsten beiden Punkten.
\item[5.] Effiziente Märkte befinden sich im Gleichgewicht. Ähnlich wie Walrasianische Gleichgewichtsmodelle, befinden sich auch die mikrofundierten makroökonomischen Modelle im Gleichgewicht.
\item[6.] Prescott und Kydland nehmen dabei an, dass dabei keinerlei Rigiditäten gibt und die eben angesprochenen Gleichgewichte praktisch immer vorhanden sind, also auch in der kurzen Frist.
\item[7.] Alle Märkte in diesem Modell sind außerdem vollkommene Konkurrenzmärkte. Das heißt es gibt auf allen Märkten jeweils auf beiden Marktseiten eine große Anzahl von Teilnehmern. Dementsprechend verfügt kein einzelner Teilnehmer über eine bemerkenswerte Marktmacht.
\item[8.] Die Märkte werden, wie oben dargestellt, wenn dann durch exogene Schocks, angebotsseitig aus dem Gleichgewicht gebracht. Diese Schocks umfassen beispielsweise die schon angesprochenen Naturkatastrophen, explizit angesprochen sind aber technologische Fortschritte. \parencite[S. 1345]{Kydland1982}. Diese rücken in den Vordergrund bei der Analyse von Konjunkturzyklen.
\end{enumerate}

Um diese Annahmen herum bauen die beiden ein ökonometrisches Modell auf. Und dieses \textit{Modell} ist der bahnbrechende Beitrag von Kydland und Prescott. Es gilt heute als erstes "`Dynamischen Stochastisches General Equilibrium"'-Modell. Diese - heute meist abgekürzt genannten - DSGE-Modelle waren den bisherigen keynesianischen Modellen und vor allem den monetaristischen Ansätzen, modelltheoretisch haushoch überlegen. Sie ließen die Ökonomie eine ganz neue Richtung einschlagen. Ab diesem Zeitpunkt beherrschten formalisierte, quantitative Modelle die ökonomische Forschung, zumindest im Mainstream. Rein methodisch war dieses erste Modell der Startschuss für eine Unmenge an Forschungsarbeiten unter anderem im Rahmen der Zeitreihenmodelle. Durch die Vergabe des Ökonomie-Nobelpreises 2001 wurde in diesem Zusammenhang Christopher Sims\footnote{Er erhielt den Preis zusammen mit dem bereits genannten Thomas Sargent} und seine Entwicklung der "`Vektor-Autoregressive"'-Modelle bekannt.

Die modelltheoretische Überlegenheit wurde weitgehend akzeptiert. Damit sah es Mitte der 1980er Jahre so aus, als würde die "`Neue Klassische Makroökonmie"' mit der damit eng verwandten "`Real Business-Cycle"'-Theorie, als deren Umsetzung,  die neoklassische Synthese als Mainstream-Ökonomie ablösen.

Allerdings hatte die "`Real Business Cycle"'-Theorie Schwächen, die bald unübersehbar wurden. Die getätigten, oben beschriebenen Annahmen erwiesen sich rasch als teilweise empirisch nicht haltbar. 
Vor allem das nur halbherzig behandelte Problem der Arbeitslosigkeit wurde später immer wieder von Kritikern aufgegriffen. So sind sich Kydland und Prescott natürlich bewusst, das Arbeitslosigkeit ein zentrales Thema der Makroökonomie ist. Aber sie handeln das in ihrem Artikel extrem kurz ab. Sinngemäß schreiben sie, dass die Menschen nicht nur "`dem Konsum, sondern auch der Freizeit einen Wert zuweisen"' \parencite[S. 1345]{Kydland1982}. Das wurde dem Modell später zum Verhängnis, wenn man so will. Der Angriffspunkt war natürlich, dass das Modell damit unterstellt, dass Arbeitslosigkeit dadurch entsteht, dass Menschen nicht arbeiten, weil sie Freizeit höher einschätzten als Konsum.
Das ist aber nicht mit der empirischen Beobachtung, vor allem während Wirtschaftskrisen, vereinbar. Man könnte diese Ansicht gar als Zynismus auslegen.

HIER DIE KRITIKPUNKTE aus Romer, Advanced-Macro-Buch von Seite 227ff einbauen.

Außerdem wurde bald die Forscherkonkurrenz tätig: Die Vertreter der Neoklassischen Synthese akzeptierten rasch die methodische Überlegenheit, der "`Neuen Klassiker"' und übernahmen deren Modelle bald. Natürlich nicht, ohne Anpassungen vorzunehmen: So wurden die Märkte nicht mehr als vollkommene Konkurrenzmärkte gesehen und es wurden ein Arbeitsmarktmodelle entwickelt, welche die Berücksichtigung von Rigiditäten und somit unfreiwilliger Arbeitslosigkeit innerhalb der Theorie der rationalen Erwartungen ermöglichte. Zudem wurde zunehmend die Effizienz von verschiedenen Märkten in Frage gestellt. Diese drei Punkte können als Geburtsstunde der "`Neu-Keynesianer"' gesehen werden. Daher in Kapitel \ref{cha: Neu Keynes} mehr dazu.

Was aber blieb von der "`Real Business Cycle"'-Theorie? Kydland und Prescott können als Urväter mikrofundierter, dynamischer Modelle und damit auch heute noch weitverbreiteter DSGE-Modelle gesehen werden. Die beiden Autoren wurden daher zurecht mit dem Ökonomie-Nobelpreis 2004 ausgezeichnet.
Auch wenn der Inhalt der RBC rasch stark infrage gestellt wurde. In den beiden ökonomischen Lehrbüchern von Mankiw und Blanchard, wird die "`Real Business-Cycle"'-Theorie überraschend scharf als einfach falsch dargestellt. Tatsächlich überwogen bald die Kritiker an den Modellen und was blieb waren die formal eleganten und fortschrittlichen Modelle auf methodischer Seite. Aber inhaltlich setzten sich in den 1990er Jahren die "`Neu-Keynesianer"' als Mainstream-Ökonomie durch. Wohlgemerkt unter Berücksichtigung wesentlicher Elemente, die die "`Neue Klassik"' hervorgebracht hat. Man kann daher auch argumentieren, die neue Mainstream-Ökonomie der 1990er Jahre war eine Kombination aus "`Neu-Keynesianismus"' und "`Neuer Klassischer Makroökonomie"'. Dagegen spricht allerdings, dass die beiden Schulen in den 1990er Jahren noch stark nebeneinander statt miteinander aktiv waren.
Von einer Verschmelzung der "`Neuen Klassik"' und der "`Neuen Keynesianer"' zur neuen Mainstream-Ökonomie würde ich daher erst später sprechen.


\section{Barro: Ricardianische Äquivalenz}
Neben Robert Lucas und Thomas Sargent gilt auch \textsc{Robert Barro} als einer der Väter der Neuen Klassischen Makroökonomie. Sein Werdegang ist dahingehend interessant, dass er in seinen 20ern durchaus erfolgreich keynesianische Journalartikel verfasste \parencite{Barro1971}. Aber bereits mit 30 Jahren, also 1974, veröffentlichte er den einflussreichen Artikel \textins{Are Government Bonds Net Wealth?} \parencite{Barro1974}. Das Werk schlägt in dieselbe Kerbe wie die Arbeit von \textcite{Sargent1975} und allgemein der Neuen Klassiker, nämlich dass Wirtschaftspolitik keinen positiven Effekt auf das Bruttoinlandsprodukt (Net wealth) hat. Diese Arbeit war damit - ebenso wie die bereits genannten von Arbeiten von Robert Lucas und Thomas Sargent - ein Bruch mit den damals vorherrschenden keynesianischen Ideen. Schließlich war es bis dahin praktisch unumstritten, dass "`expansive Fiskalpolitik"', im Barro-Artikel als Government Debt bezeichnet, über den Multiplikatoreffekt zu einer Steigerung der aggregierten Nachfrage und somit zu einem Wohlfahrtsgewinn im Form eines steigenden BIPs, führt. \textcite[S. 336]{Blinder1973} hatten im Jahr davor gegen argumentiert, dass die keynesianische Fiskalpolitik sehr wohl ökonomisch sinnvoll ist und daher "`[will] survive[s] the monetarist challenge"'. Barro wiederum führt "`neue klassische"' Argumente ins Feld wenn man so will: Fiskalpolitik sei deshalb wirkungslos, weil Haushalte generationsübergreifend agieren würden und sich damit bewusst wären, dass Staatsschulden, die heute aufgenommen werden, in Zukunft nur durch entsprechend höhere Steuereinnahmen zurückbezahlt werden können \textcite[S. 1116]{Barro1974}. Oder mit anderen Worten: Die Menschen sind sich bewusst, dass heutige Budgetdefizite in Zukunft durch höhere Steuereinnahmen kompensiert werden müssen und agieren dementsprechend mit höheren Sparquoten, was dazu führt, dass Fiskalpolitik zu keiner höheren aggregierten Nachfrage führt. 
Das Paper argumentiert also es komme bei Fiskalpolitik immer zu einem vollständigen Crowding-Out-Effect: Der positive Effekt auf das BIP-Wachstum durch die zusätzlichen Staatsausgaben wird durch den negativen Effekt ausgeglichen, der dadurch entsteht, dass die Menschen den privaten Konsum einschränken, weil sie für die zukünftige Steuerbelastung sparen. Der keynesianische Multiplikator wäre demnach nicht größer als Eins. Der Ansatz von Barro wurde später als \textit{Ricardianische Äquivalenz} (oder Ricardo-Barro-Äquivalenz) bekannt. David Ricardo hatte sich nämlich schon 1820 Gedanken darüber gemacht, dass es keinen Unterschied mache, ob ein Staat einen Krieg durch einen Kredit mit den entsprechenden Zinszahlungen oder eine Steuererhöhung in Höhe der Zinszahlungen finanziere. Das Prinzip wurde auf jeden Fall kontrovers aufgenommen und bis heute dementsprechend diskutiert. In einem Interview mit der \textit{Minneapolis Fed} beschreibt Barro, dass sein Theorem zwar "`bis heute nicht Teil der Mainstream-Ökonomie ist, weil die meisten Ökonomen das Konzept nicht vollständig als richtig akzeptieren. Das Konzept aber dennoch einen enormen Einfluss darauf hatte wie in der Ökonomie über Fiskalpolitik gedacht wird."' Beides ist zweifelsfrei richtig: Die Ricardianische Äquivalenz erlitt dasselbe Schicksal wie so viele Konzepte der Neuen Klassik. Formal einwandfrei dargestellt, scheitert das Konzept an empirischen Beobachtungen und an den zu starren theoretischen Voraussetzungen.
Dennoch ist das Konzept nach wie vor nicht von der Bildfläche verschwunden. Zwar gilt die Ricardianische Äquivalenz in seiner reinen Form als widerlegt, aber der keynesianische Optimismus gegenüber der positiven Effekte von Fiskalpolitik wurde nicht zuletzt durch die Arbeit von Barro in Zweifel gezogen. Als State-of-the-Art gilt heute, dass Fiskalpolitik in der kurzen Frist durchaus positive Wirkungen auf das BIP-Wachstum hat, in der langen Frist hingegen sogar negative Effekte.

Robert Barro wird meist mit der umstrittenen "`Ricardianischen Äquivalenz"' in Verbindung gebracht, manchmal auch mit der endogenen Wachstumstheorie (die später in diesem Kapitel beschrieben wird). Relativ selten aber mit jenem Thema, das er ebenfalls stark mitgeprägt hat und das große praktische Auswirkungen hatte: Das "`Inflation Targeting"' der Zentralbanken. Wir erinnern uns, dass die 1970er Jahre in den USA von hohen Inflationsraten geprägt wurden. Die Monetaristen hatten zu deren Bekämpfung zunächst eine Geldmengensteuerung und später eine konstante Wachstumsrate der Geldmenge vorgeschlagen. Beides erwies sich aber als nicht besonders zielführend. Danach folgte - wie soeben beschrieben - die große Zeit der "`Neuen Klassischen Makroökonomie"'. In vielen Punkten wurde diese ökonomische Schule recht umstritten aufgenommen und erwies sich in ihrer Reinform oft für die Wirtschaftspolitik als zu theoretisch. Man denke nur an die angeblich völlige Wirkungslosigkeit der Fiskalpolitik. Was hingegen die Geldpolitik angeht, wenden die meisten Zentralbanken heute jene Ideen an, deren theoretische Grundlagen erstmals von den Neuen Klassikern beschrieben wurden! 
Konkret wandelte sich die primäre Rolle der Zentralbank von der Bekämpfung der Arbeitslosigkeit im Keynesianismus zur Bekämpfung der Inflation im Monetarismus. Die Ausführungen von Milton Friedman und dessen Monetaristen waren aber wenig theoretisch hinterlegt. Erst die Neuen Klassiker entwarfen fundierte wissenschaftliche Arbeiten, die zumindest die Grundlage der heutigen Politik der Zentralbanken schuf, nämlich dass Geldwertstabilität deren primär anzustrebendes Ziel ist. Das hat sich seit Anfang der 1990er Jahre bis heute unbestritten tatsächlich als das primäre Ziel der Zentralbanken in den Industriestaaten etabliert. Zwar spielt die "`Neue Philipskurve"' - also die Bekämpfung von Arbeitslosigkeit durch Inflationserwartungen -  in der kurzen Frist eine Rolle, aber langfristig steht die Geldwertstabilität im Fokus der Zentralbanken. Bei manchen - wie der EZB - ausdrücklich festgeschrieben, bei anderen - wie der Fed - de facto ebenso wichtig, aber formal weniger stark festgelegt\footnote{"'I think over the last two decades the Fed has come close to an inflation targeting regime even though it's not explicit"', Robert Barro 2005 in einem Interview mit der Minneapolis Fed: https://www.minneapolisfed.org/article/2005/interview-with-robert-barro}.

Die Grundlagen dafür lieferten zunächst die bereits besprochenen \textcite{Kydland1977}. Aber es war vor allem Robert Barro (gemeinsam mit David Gordon), der Anfang der 1980er-Jahre \parencite{Barro1976, Barro1983a, Barro1983b}, folgendes Dilemma der Zentralbanken theoretisch löste: In der langen Frist kann die Geldpolitik das BIP und auch die Arbeitslosigkeit nicht entscheidend beeinflussen. Im Gegenteil, führen Bestrebungen dahingehend langfristig zu Kosten in Form von Inflation und damit Vertrauensverlust in die Währung. Langfristig ist Preisstabilität das Beste, dass eine Zentralbank anstreben kann. In der kurzen Frist jedoch können sehr wohl Wachstums- und Beschäftigungsakzente gesetzt werden, vor allem wenn die Zentralbank überraschend von ihrer angekündigten Politik abweicht. Nachdem die Inflationserwartungen der privaten Akteure gebildet sind, ändert sich also das optimale Verhalten der Bank. Wenig überraschend leidet aber die Glaubwürdigkeit ("`reputation"') einer Zentralbank, wenn sie immer wieder kurzfristig von ihren langfristig gesteckten Zielen abweicht. Mittels spieltheoretischen Ansatzes analysierten \textcite{Barro1983a} im sogenannten \textsc{Barro-Gordon-Modell} unter welchen Umständen der langfristige Nachteil (der durch das Abweichen von der angekündigten Politik entsteht) durch den kurzfristigen Vorteil der stimulierenden Wirkung von "`Überraschungsinflation"', überwiegt.

Es ist heute recht unumstritten, dass politisch unabhängige Notenbanken dieses langfristige Ziel der Geldwertstabilität am besten umsetzen können. Politiker, die sich alle paar Jahre einer Wiederwahl stellen müssen, wären wohl eher versucht die kurzfristig sinnvollen Abweichungen vom Inflationsziel vorzunehmen. Diese Fragen wurden ungefähr zehn Jahre später aus institutioneller Sicht \parencite{Tabellini1993} diskutiert. Die ersten formalen Inflationsziele wurden übrigens ebenfalls Anfang der 1990er-Jahre festgelegt. Unter anderem in Kanada und Neuseeland (1991), Großbritannien (1992), Schweden und Finnland (1993) \parencite{Fischer1994}. 

Die erfolgreiche Bekämpfung der Inflation kann auf jeden Fall als Erfolgsgeschichte gesehen werden. Der jüngeren Generation in den westlichen Staaten ist das Problem hoher Inflationsraten gar kein Begriff mehr. Aber zumindest bis in die 1980er-Jahre war dieses Problem ein allgegenwärtiges. Es war für die Menschen ärgerlich, dass das heute verdiente Geld in einem Jahr vier-oder-mehr-Prozent an Wert verlor. Natürlich waren es verschiedene Aspekte, die dazu führten, dass die Zentralbanken das Problem heute weit besser im Griff haben. Zum Einen die rigorose Anti-Inflationspolitik der Federal Reserve in den USA unter Paul Volcker. Weiters die Aufgabe der fixierten Wechselkurse. Damit müssen Staaten nicht länger ihre Geldpolitik nicht mehr länger von der Entwicklung der Referenzwährung abhängig machen. In welchem Ausmaß die soeben dargestellten wissenschaftlichen Erkenntnisse hierbei eine direkte Rolle spielten oder zumindest die Akteure beeinflusste, kann man wohl nicht quantifizieren. Meines Erachtens können diese aber als unumstrittener Erfolg der Neuen Klassiker gesehen werden.

Ab Mitte der 1990er-Jahre setzte sich auch in Bezug auf die Geldpolitik der "`Neu-Keynesianismus"' weitgehend durch, der im nächsten Kapitel beschrieben wird. Vor allem die Arbeiten von \textsc{John Taylor}\footnote{John Taylor ist zwar ein sehr konservativer Ökonom (unter anderem ist er im Jahr 2020 Präsident der wirtschaftsliberalen Mont-Pèlerin-Gesellschaft), seine bedeutendsten Arbeiten sind aber dem Neu-Keynesianismus zuzuordnen und stehen teilweise im Widerspruch zu den Arbeiten der Neu-Klassiker (\textcite{Taylor1977} als Antwort auf \textcite{Sargent1975} } erweiterten die Arbeiten von Barro und Gordon.

Vielleicht geht man zu weit, wenn man die heute in Notenbanken weitverbreitete Praxis des "`Inflation-Targetings"' alleine auf Arbeiten der "`Neuen Klassiker"' zurückführt. Selbst Robert Barro blieb diesbezüglich in einem Interview \parencite{Barro2005} eher zurückhaltend. Fix ist allerdings, dass die Erkenntnisse der "`Neuen Klassische Makroökonomie"' in Bezug auf die Geldpolitik weit weniger umstritten sind, als in den meisten anderen Bereichen. 


\section{Lucas und Romer: Endogenes Wachstumsmodell} \label{sec: endogene}
Die "`Endogene Wachstumstheorie"' ist eines jener Themen in diesem Buch, dass an der "`falschen"' Stelle platziert ist. Sie ist zwar eindeutig ein makroökonomisches Thema, aber nicht wirklich eng verbunden mit der "`Neuen Klassischen Makroökonomie"'. Allerdings ist auch die "`Endogene Wachstumstheorie"' ein klarer Bruch mit der damals vorherrschenden Mainstream-Theorie dem "`Solow-Wachstumsmodell"'. Wie im gleichnamigen Kapitel beschrieben, wird ökonomisches Wachstum durch technologischen Fortschritt erklärt, der allerdings als exogene Variable betrachtet wird.
Die eigentliche Verbindung zur "`Neuen Klassik"' besteht vor allem über die handelnden Akteure: Der Begründer der "`Neuen Klassischen Makroökonomie"' ist auch einer der beiden Entwickler der "`Endogenen Wachstumstheorie"': Nämlich Robert Lucas. Der zweite Entwickler ist Paul Romer. Wie die "`typischen"' Vertreter der Neuen Klassik hatte auch er als Absolvent des Physikstudiums \textcite{Romer2018} einen stark quantitativen Background. Außerdem war Robert Lucas einer der Betreuer seiner Dissertation, die er in Chicago abschloss. Eigentlich scheint es also als wäre er selbst ein "`typischer"' Vertreter der "`Neuen Klassik"'. Erst bei genauerem hinsehen entdeckt man, dass er nicht der kompromisslosen mathematischen Argumentation erlegen ist wie eben die "`typischen Neuen Klassiker"'. Im Gegenteil: Im Jahr 2015 publizierte er eine Kritik an dieser strengen Mathematik-Gläubigkeit. \textit{Mathiness} nennt Romer die - seiner Meinung nach - häufig missbräuchliche Verwendung von Mathematik in wirtschaftswissenschaftlichen Journalartikeln. Inhaltlich schlechte oder falsche Annahmen würden hierbei überdeckt durch mathematische und damit scheinbar neutrale Abhandlungen. So weit so gut. Hauptziel seiner bemerkenswert scharfen Kritik\parencite{Romer2015} waren aber ausgerechnet die "`Neuen Klassiker"' Edward Prescott und Robert Lucas\footnote{Es wurden im selben Artikel aber auch Joan Robinson und Thomas Piketty - also zwei sehr "`linke"' Ökonomen ebenso scharf kritisiert.}. Man muss allerdings tatsächlich festhalten, dass sich gerade die Wirtschaftsmodelle und Prognosen von Lucas und Prescott in ihrer Reinform - an der beide stur festhielten - recht rasch als unzulänglich erwiesen. Die Kritik erinnert ein wenig an das oben beschriebene Zitat von Robert Solow, wonach man mit Robert Lucas nicht über ökonomische Modelle und mit Napoleon Bonaparte nicht über Kavallerietaktik diskutieren soll, weil man sich dann auf deren Spezialgebiet begeben hat. Das Problem seien vielmehr die Grundlagen dahinter.

Woher kam plötzliche Wiedererstarken des Interesses an Wachstumstheorien Mitte der 1980er Jahre? Nachdem Solow bereits 1956 sein Modell veröffentlicht hatte, war das Thema jahrzehntelang von den Bildschirmen verschwunden. Aber mit der Stagflation kamen eben nicht nur die schon beschriebenen Zweifel an keynesianischer Wirtschaftspolitik, sondern - da Wirtschaftswachstum eben nun ausblieb - stellte man wieder vermehrt Fragen woher dieses denn eigentlich komme? 
Natürlich spielte aber auch der sich ändernde Zeitgeist eine Rolle: Wachstums wurde seit den 1970er Jahren erstmals auch mit kritischen Augen gesehen. Sowohl die Frage ob Wachstum, wie wir es in den Jahren nach dem Zweiten Weltkrieg in den Industriestaaten gesehen hatten, auf lange Frist überhaupt möglich wäre. Als auch die Frage, ob Wirtschaftswachstum uneingeschränkt als positiv zu bewerten sei. Im Jahr 1972 schlug diesbezüglich ein Bericht des "`Club of Rome"' hohe Wellen: In "`Die Grenzen des Wachstums"' wurde prognostiziert, dass das wirtschaftliche Wachstum aufgrund der Ausbeutung von Rohstoffen spätestens Mitte des 21. Jahrhunderts kollabieren würde. Von der Allgemeinheit zu tragende Schäden, die durch zügelloses Wirtschaftswachstum verursacht wurden traten immer häufiger in den Mittelpunkt. Zunächst wurde die gesundheitsgefährdende Wirkung des Pflanzenschutzmittels DDT, dass erfolgreich zu Steigerung landwirtschaftlicher Erträge eingesetzt wurde, offensichtlich. Später rückten Themen wie der Saure Regen der das Waldsterben auslöste, ab Mitte der 1970er Jahre das Ozonloch in den Fokus der Öffentlichkeit. Bis heute beschäftigt uns der $C0_2$-Ausstoß und die damit verbundene globale Klimaveränderung.
In seiner Nobelpreis-Rede erinnerte sich \textcite{Romer2018} an dieses Umfeld, das die frühen 1980er-Jahre prägte, zurück. Es herrschte eher ein Pessimismus dahingehend vor, ob nachhaltiges Wachstum möglich sei. Als Doktoraststudent war auch für ihn als Absolvent eines Physikstudiums zunächst nur eine Wachstumstheorie sinnvoll: Jene von Thomas Malthus (vgl. Kapitel XXX), die bekanntermaßen eher apokalyptisch ist\parencite{Romer1986}. Seine Forschungen zu endogenem Wachstum sollte ihn schließlich davon überzeugen, dass nachhaltiges Wachstum sehr wohl möglich sei.

Die primäre Motivation für das Thema war, dass man nicht länger akzeptieren wollte, dass wirtschaftliches Wachstum ausschließlich durch technologischen Fortschritt verursacht wird \textit{und}, dass die Entstehung dieses technologischen Fortschritts nichts mit ökonomischen Prozessen zu tun habe \footnote{Dieses Problem brannte manchen Ökonomen schon länger unter den Fingernägeln wie das Zitat aus \textcite{Arrow1962} zeigt: \textit{Nevertheless a view of economic growth that depends so heavily on an exogenous variable, let alone one so difficult to measure as the quantity of knowledge, is hardly intellectually satisfactory.}}. In der exogenen Wachstumstheorie geht man davon aus, dass das BIP grundsätzlich dazu neigt auf gleichem Niveau zu bleiben, das Wachstum also stagniere. Nur durch technologischen Fortschritt - auf den aber nicht näher eingegangen wird, weil er eben als exogen betrachtet wird - kommt es also zu Wirtschaftswachstum.

\textcite{Romer1994} meint es gäbe zwei verschiedene Versionen über die Anfänge der "`Endogenen Wachstumstheorie"'. Die erste Version geht zurück auf die sogenannte "`Konvergenz-Kontroverse"'. Anfang der 1980er-Jahre erschien der erste Datensatz, der langfristige Zeitreihen für wichtige makroökonomische Kennzahlen für eine verschiedene Staaten zur Verfügung stellte \parencite{Maddison1982}. Darauf aufbauend kam es zu einer Debatte, ob es ärmeren Staaten im 20. Jahrhundert gelungen sei zum Wohlstand wohlhabender Staaten aufzuschließen. Man kam rasch zum Ergebnis, dass dies zumindest den meisten Staaten nicht gelang und die Entwicklung auch nicht darauf schließen lasse, dass dies in den nächsten Jahren gelingen würde. Robert Lucas und Paul Romer stellten fest, dass dies allerdings laut exogener Wachstumstheorie passieren hätte sollen: Technologischer Fortschritt ist in solchen Modellen nämlich eben exogen und sollte dementsprechend in armen wie in reichen Staaten gleichermaßen vorkommen. Außerdem gehen exogene Wachstumsmodelle davon aus, dass moderne Technologie auf der ganzen Welt angewendet werden kann. Warum aber sollte es laut "`exogenen Wachstumstheorien"' zu einer Konvergenz, also einem stärkeren Wirtschaftswachstum in armen Ländern als in reichen Ländern, kommen? Dazu müssen wir direkt an die Überlegungen aus Kapitel \ref{sec: Solow-Modell} anschließen. Dort wurde behauptet, dass sich langfristig Wirtschaftswachstum verlangsamen müsste, weil der Produktionsfaktor Arbeit stabil ist und eine ständige Erhöhung des Faktors Kapital zu immer geringeren Zuwachsraten beim Wachstum führt. Nur wenn sich die \textit{Qualität} des Faktors Kapital ständig erhöht, also technischer Fortschritt eintritt, kann dauerhaftes Wachstum entstehen. Und hier kommt die eigentlich notwendige Konvergenz ins Spiel: In einem armen Land ist per Definition das BIP pro Kopf niedriger als in einem reichen Land. Oder mit anderen Worten, da wir den Output ja "`pro Kopf"' betrachten, also um den Faktor Arbeit bereinigen, is der Kapitaleinsatz pro Kopf in einem armen Land geringer als in einem reichen Land. Nun haben wir im Kapitel \ref{sec: Solow-Modell} festgestellt, dass eine zusätzliche Input-Einheit den Gesamtoutput dann am stärksten erhöht, wenn diese Art des Inputs insgesamt noch unterrepräsentiert ist. Wir haben dies den "`Abnehmenden Grenzertrag"' genannt. Erinnern Sie sich an das Beispiel zurück, in dem der erste Computer einen höheren Produktivitätszuwachs bringt, als der x-te Computer. Wenn Technologie und technologischer Fortschritt nun auf der ganzen Welt, also in armen Ländern wie in reichen Ländern, in gleichem Ausmaß zur Verfügung stünde, wie die exogene Wachstumstheorie postuliert, dann wäre es doch viel sinnvoller zusätzliches Kapital in armen Ländern einzusetzen, als in reichen Ländern. In armen Ländern müsste dieses zusätzliche Kapital nämlich in viel höherem Ausmaß die Produktivität steigern, zu Wirtschaftswachstum führen und das BIP in armen Länder langsam auf das Niveau der reichen Ländern anwachsen lassen. Das BIP der beiden Staaten müsste eben konvergieren. Dies passte aber eben nicht mit den empirischen Beobachtungen zusammen. 
Beide, Romer und Lucas, versuchten sich in weiterer Folge darin, Modelle zu erstellen, welche die Entwicklung von technologischen Fortschritt nicht länger als Gott-gegeben annehmen, sondern in Wachstumsmodellen mit-erklären sollten. Technologischer Fortschritt ist also auch in den neuen Wachstumstheorien der wesentlicher Einflussfaktor auf Wirtschaftswachstum, allerdings wird dieser endogenisiert.

Zwei wesentliche Beobachtungen müssen festgehalten werden um die Richtigkeit und Wichtigkeit der Endogenisierung des technologischen Fortschritts zu begründen \parencite{Romer1994}. Erstens, technologischer Fortschritt ist kein Zufallsprozess, sondern das Ergebnis harter Arbeit von Menschen. Die Erfolgsaussichten der Forschungsarbeit hängen nicht unwesentlich mit den Umständen zusammen unter denen die Forscher arbeiten. Paul Romer machte dies mit einem einzigen Bild, dass er im Rahmen seiner Nobelpreis-Rede zeigte klar: Afrikanische Studenten sitzen auf diesem Bild unter dem Licht von Straßenlaternen um zu lernen. Offensichtlich war diese Lichtquelle die einzige, die sie nutzen konnten. Es ist klar, dass europäische oder nordamerikanische Studenten und Forscher einen Vorteil gegenüber diesen Studierenden haben, wenn es darum geht technologischen Fortschritt hervorzubringen. Zweitens, technologischer Fortschritt ist kein öffentliches Gut und damit eben nicht überall auf der Erde gleichermaßen verfügbar. In der Ökonomie spricht man von öffentlichem Gut, wenn ein Gut keine Rivalität und auch keine Ausschließbarkeit im Gebrauch aufweist. Ersteres ist bei wissenschaftlichen Erkenntnissen gegeben, weil es wissenschaftlichen Erkenntnissen nicht schadet, wenn sie von mehreren Personen gleichzeitig angewendet werden. Zweiteres ist aber nicht gegeben. Durch Geheimhaltung aber auch Patent- und Musterschutzrechte können Personen sehr wohl davon ausgeschlossen werden, neueste wissenschaftliche Erkenntnisse zu nutzen. In diesem Fall spricht man von Klubgütern anstatt von öffentlichen Gütern.
Diese zweite Beobachtung verändert die Anforderung an Wachstumsmodelle erheblich. Wenn technologischer Fortschritt für alle verfügbar ist, so haben alle Marktteilnehmer die gleichen Startvoraussetzungen. Wie auf einem Markt auf dem perfekter Wettbewerb herrscht. Die Ausschließbarkeit von Forschungsergebnissen führt aber dazu, dass Unternehmen, die über Forschungsergebnisse verfügen, dies auch verwerten können. Sie sind also Monopolanbieter für Güter, die aufgrund dieser Forschungsergebnisse produziert werden. 

Mitte der 1980er Jahre befand sich die Wachstumstheorie also in einem Dilemma: Nach dem etablierten Konzept abnehmender Grenzerträge, sollte Wirtschaftswachstum eigentlich durch Ressourcenknappheit beschränkt sein. Die exogene Wachstumstheorie, wonach stetiges Wachstum eben doch möglich sei, wenn man die Existenz von technologischem Fortschritt akzeptiere, dessen Entstehung aber nicht weiter nachverfolge, war zunehmend unbefriedigend. Dazu kam das Problem der fehlenden - aber eigentlich zu erwartenden - Konvergenz zwischen armen und reichen Ländern. 

Schritt für Schritt wurden in den kommenden Jahren die angesprochenen Unzulänglichkeiten berücksichtigt:

Zunächst wurden Modelle erstellt, die nicht mehr unbedingt von abnehmenden Grenzerträgen ausgingen. Diese basierten auf dem Vorläufer-Modell von \textcite{Arrow1962}, der die privaten Investitionen in Forschung und Entwicklung in den Vordergrund stellte. Dies Modelle werden heute häufig unter "`AK-Modelle"'-Modelle zusammengefasst. Vor allem die Arbeiten von \textcite{Romer1986} und \textcite{Rebelo1991} gehen auf diesen Ansatz zurück. Technisch gesehen bedienen sich diese Modelle eines kleinen Tricks: Die bereits in Kapitel \ref{sec: Cobb-Douglas-Produktionsfunktion} kennengelernte Cobb-Douglas-Funktion wird hier herangezogen - wie auch zum Beispiel beim exogenen Wachstumsmodell. Ohne auf die Details einzugehen wird hier ein Parameter so gesetzt, dass zunehmender Kapitaleinsatz nicht mehr zu abnehmenden Grenzerträgen führt. Inhaltlich wird hierbei argumentiert\parencite{Romer1994}, dass Forschungsausgaben von privaten Unternehmen den Inputfaktor Kapital so stark wachsen lassen, dass es dauerhaft positive Grenzerträge gibt, und dass über Spillover-Effekte diese privaten Forschungsausgaben auch gesamtwirtschaftlich zu technischem Fortschritt führen. Man konnte damit zumindest innerhalb des Modells erklären, warum dauerhaftes Wachstum möglich war, allerdings zeigte sich \textcite[S. 15]{Romer1994} selbst rasch nicht sehr glücklich mit der Rohform dieser Modelle.

Es dauerte auch nicht lange bis \textcite{Lucas1988} ein alternatives Modell vorstellte. Durch \textcite{Romer1986} konnte man zwar erklären wie es zu dauerhaftem Wachstum kommen kann, allerdings war die Frage ungeklärt, warum unterschiedliche Wachstumsraten in verschiedenen Entwicklungsländern auftraten. Also warum es nicht notwendigerweise zur oben beschriebenen Konvergenz kommt. Ein häufig wiedergegebenes Zitat aus \textcite[S. 5]{Lucas1988} bringt die Fragestellung auf den Punkt: \textit{"'Is there some action a government of India could take that would lead the Indian economy to grow like Indonesia's or Egypt's? If so, what, exactly? If not, what is it about the' nature of India' that makes it	so?"'}\footnote{Während in  und Indonesien Anfang der 1990er Jahre hohe Wachstumsraten zu verzeichnen waren, war Indien noch geprägt von geringem Pro-Kopf-Einkommen und niedrigen Wachstumsraten. Eine Entwicklung, die sich gerade wenn man Ägypten und Indien vergleicht nur wenige Jahre später umdrehen sollte.} Genau diese Fragestellung adressierte Lucas in weiterer Folge. Wobei das Paper selbst einigermaßen bemerkenswert ist: Es ist einer der am häufigsten zitierten Journal-Artikel\footnote{Zum Beispiel hier auf Platz 7: https://ideas.repec.org/top/top.item.nbcites.html, Stand 11.01.2020} in den Wirtschaftswissenschaften überhaupt. Was insofern überrascht als Robert Lucas im Kernbereich der "`Neuen Klassischen Makroökonomie"' vermeintlich wichtigere Beiträge geleistet hat. Robert Lucas selbst meint außerdem in den "`Acknowledgements"' \parencite[S. 41]{Lucas1988} des Artikels, dass er zwar ein berühmter Ökonom sei, aber von diesem Thema wenig verstehe und der Artikel außerdem eigentlich zu lang für einen Journal-Beitrag wäre.
Wie auch immer: Aus modelltechnischer Sicht war die Arbeit an \parencite{Uzawa1965} und auch \parencite{Romer1986}. Allerdings entwickelte Lucas in diesem Modell den Inputfaktor Kapital weiter, indem dieser nicht mehr ausschließlich aus physischem Kapital bestand, sondern er implementierte darin außerdem das von Gary S. Becker entwickelte Konzept des Humankapitals\footnote{Der Begriff Humankapital wird im nächsten Kapitel: \ref{sec: Becker} behandelt.}.
Die Erweiterung gegenüber der Arbeit von \textcite{Romer1986} besteht nun darin, dass die Forschungsbemühungen von privaten Unternehmen \textit{explizit betrachtet} werden. Konkret fließt ein Teil des Humankapitals natürlich in die Produktion der Güter, aber der zweite Teil wird dazu verwendet dieses individuelle Humankapital auszubauen, also Bildung und Ausbildung zu schaffen. Das heißt aber auch, dass man in der kurzen Frist auf Produktion verzichten muss um stattdessen mit den freigewordenen Zeit-Ressourcen Wissen (Ausbildung) aufzubauen. Erst langfristig führt dieses Wissen dazu, dass besser ausgebildetes Personal effizientere Arbeit leistet und so für ständig positive Grenzerträge sorgt. Wie auch in \textcite{Romer1986} kommt es also auch gesamtwirtschaftlich zu technischem Fortschritt, allerdings nicht mehr als Nebeneffekt nicht weiter spezifizierter Forschungsausgaben, sondern durch Investition in den Modellparameter Humankapital.
Durch unterschiedliche Bildungsniveaus in verschiedenen Regionen der Erde lassen sich die beobachteten Wachstumsraten also jetzt erklären. Denn die Investition in physisches Kapitel alleine führt nicht zu langfristigen Wachstum, wenn die Ausstattung mit Humankapital nicht gegeben ist. Einfach ausgedrückt: Eine mit moderner Technologie ausgestattete Fabrik alleine wird keine Erträge bringen, wenn die Arbeiter und Angestellten die dort arbeiten sollen nie die Möglichkeit hatten lesen und schreiben zu lernen.

Die beiden nun vorgestellten Modelle nennt man häufig auch "`Modelle mit konstanter Technologie"'. Dies ist etwas verwirrend, da wir ja ständig davon gesprochen haben, dass eben technologischer Fortschritt Wirtschaftswachstum verursacht. Allerdings beschränken sich sowohl \textcite{Lucas1988} als auch \textcite{Romer1986} darauf sich entweder auf Fortschritt beim Humankapital (Bildungsniveau) festzulegen, bzw. Fortschritt unspezifisch zu betrachten. Der fehlende Schritt besteht nun darin technischen Fortschritt im Sinne von neuen oder besseren Produkten und Produktionsverfahren zu integrieren. Dies ist deshalb wichtig, weil dieses Wissen durch Patente geschützt werden kann. Im Gegensatz zu "`Bildungsstand"' allgemein, also kein öffentliches Gut ist.

In \textcite{Romer1990} wurde technologischer Fortschritt als Produkt- bzw. Verfahrensweiterentwicklungen und mit Schutzmechanismen wie Patentregelungen und Urheberrechten modelliert. Damit erlangt man durch technischen Fortschritt eine Monopolstellung, da Produkte, die auf technologischem Fortschritt basieren, nur jenes Unternehmen anbieten kann, welches diese entwickelt hat. Das heißt aber auch, dass Modelle, die auf dem Prinzip der vollkommener Konkurrenz basieren durch Modelle mit monopolistischer Konkurrenz ersetzt werden\footnote{\textcite[S. 17]{Romer1994} nannte diese Modelle "`Neo-Schumpeter-Wachstumsmodelle. Schließlich spielte auch bei Schumpeter die Monopolstellung des Entrepreneurs eine wichtige Rolle bei der Entstehung von Wirtschaftswachstum. Der Begriff "`Neo-Schumpeter"' wird aber meines Erachtens nicht konsistent für diese Modelle verwendet}. In der historischen Entwicklung der "`Endogenen Wachstumstheorie"' ist dies ein interessanter Punkt. Denn diese Monopolmodelle wurden in anderen Bereichen vor allem von Neu-Keynesianern (vergleiche dazu Kapitel \ref{cha: Neu Keynes}) verwendet. In der "`Endogenen Wachstumstheorie"' forschten bislang vor allem Vertreter der "`Neuen Klassischen Makroökonomie"': Robert Lucas, den hier nicht speziell erwähnten Robert Barro und eben auch Paul Romer. Mit der Implementierung monopolistischer Modelle wendete sich Paul Romer nun aber ab von seinem Doktorvater Robert Lucas ab, der bis ausschließlich "`vollkommene Marktmodelle"' für richtig hält. In der bereits beschriebenen Mathiness-Debatte wird der Bruch offensichtlich: \textcite[S. 91f]{Romer2015} kritisiert Lucas mit scharfen Worten.

Zum Inhalt dieser Modelle: Erneut werden die Produktionsfaktoren Arbeit, Kapital und Humankapital als Inputs herangezogen. Weiters wird unterschieden zwischen dem "`allgemeinen Wissenstand"', und dem "`technischen Fortschritt"'. Letztgenannter ist durch Patente oder Urheberrechte geschützt. Dementsprechend kann dieser technische Fortschritt zur Generierung von Monopolgewinnen genutzt werden. Das formale Modell besteht schließlich aus drei Sektoren \parencite[S. 79]{Romer1990}
\begin{itemize}
	\item Der Forschungssektor produziert aus dem "`allgemeinen Wissenstand"' und Humankapital "`technischen Fortschritt"'. Die Ergebnisse aus dem Forschungssektor können nur zum Teil durch Patente gesichert werden, der andere Teil wird frei verfügbar. Dies kann man sich anhand eines Beispiels erklären: Bahnbrechende Entwicklungen, wie zum Beispiel der Mikrochip, bringen dem Erfinder hohe Erträge. Der gesamtwirtschaftliche Gewinn ist allerdings noch wesentlich größer. Ökonomen sprechen in diesem Fall von "`positiven externen Effekten"'. Man weiß, dass alle Tätigkeiten, die solche Effekte verursachen im privatwirtschaftlichen Sektor tendenziell unter-finanziert werden, da es eben keine volle Abgeltung der Gewinne daraus gibt.
	\item Der Zwischensektor erzeugt aus diesem "`technischen Fortschritt"', Kapital und Humankapital neue Produktionsverfahren. Da im Zwischensektor die Patente aus dem Forschungssektor verwendet werden, handelt es sich hierbei um den vorhin schon angesprochenen Monopol-Markt. Typischerweise treten hier negative externe Effekte auf: Der Monopolist wird das neue Produktionsverfahren in einem Ausmaß anwenden, der ihm erlaubt eine Monopolrente abzuschöpfen. Die Aussicht auf diese Monopolrente ist aber auch die treibende Kraft für Unternehmen innovativ tätig zu werden. Das langfristige endogene Wachstum wird also in diesem Sektor geschaffen.	
	\item Im dritten Sektor werden diese Verfahren schließlich eingesetzt um gemeinsam mit Humankapital und Kapital die Konsumgüter herzustellen. Von denen - wie schon aus den anderen Modellen bekannt - ein Teil konsumiert wird und ein Teil investiert wird.
\end{itemize} 
Als Conclusio kann aus diesem Paper gezogen werden, dass damit der dritte vorhin angesprochene Diskussionspunkt, nämlich monoplistische Konkurrenzmärkte, in die Wachstumstheorie eingebunden wurden. Die Implikationen aus dem Modell sind, dass steigendes Humankapital für stetiges Wachstum verantwortlich ist. Wie im Modell von \textcite{Lucas1988} lässt sich auch hier damit die fehlende Konvergenz zwischen armen und reichen Ländern, aber auch die anhaltenden hohen Wachstumsraten in Industriestaaten im 20. Jahrhundert erklären \parencite{Romer1990}. Zusätzlich kann aus dem Paper der Schluss gezogen werden, dass der Markt dazu tendiert tendenziell wenig Anreiz für Grundlagenforschung zu bieten. Staatliche Anreize könnten hier sinnvoll sein. Weiters kann man aus dem Modell ableiten, dass eine höhere Bevölkerungszahl - aufgrund im Modell technisch berücksichtigter positiver Skaleneffekte - mit höheren Wachstumszahlen einhergeht. Diese Implikation stieß früh auf Kritik. Da dies sowohl ökonomisch-inhaltlich als auch empirisch eine schwer haltbare Annahme ist.

Dieses Problem wurde durch \textcite{Jones1995} aufgegriffen und das Modell wurde technisch so verändert, dass das Problem eliminiert wurde. Insgesamt ist das Thema Wachstumstheorien eines an dem sehr lange intensiv geforscht wurde und teilweise noch immer geforscht wird. Wenn sich auch der Fokus seit der "`Great Recession"' ab dem Jahr 2007 etwas verschoben hat.

Vor allem aufbauend auf das Paper von \textcite{Romer1990} kam es bis Mitte der 1990er Jahre zu einigen bekannten Erweiterungen des endogenen Wachstumsmodells. Aufgrund des hohen Detailgrades der Diskussion sollen die wesentlichen Paper hier nur angeführt werden: \textcite{Lucas1990} brachte in einem Artikel noch einem die Diskussion ein warum nicht mehr Kapital in Entwicklungsländer fließt, da dieses dort ja eigentlich eine höheren Produktivitätszuwachs pro zusätzlich investierter Geldeinheit als in Industriestaaten haben sollte. Diese Diskussion wurde als \textit{Lucas-Paradoxon} bekannt.

Die frühen Vertreter des (späten) Neu-Keynesianismus \textcite{Mankiw1992} argumentierten mittels empirischer Untersuchung, dass das Solow-Modell sehr wohl geeignet sei die beobachteten Wachstumsraten zu erklären, wenn man um Effekte des Bevölkerungswachstums und die Effekte der Entwicklung des Inputfaktors Kapital bereinigt. Dieser Journalartikel stellte eine vielzitierte Gegenposition zum endogenen Wachstumsmodell dar. 

\textcite{Grossman1991a, Grossman1991b} legen den Fokus auf internationale Verflechtungen und analysieren die Auswirkungen von Außenhandel auf das Romer-Wachstumsmodell.

\textcite{Aghion1992} werfen den Blick - ebenfalls als Erweiterung zum Romer-Modell - noch genauer auf den Wachstumsprozess. Konkret in Anlehnung an Schumpeter auf den Prozess der "`kreativen Zerstörung"'. Welche Rolle spielt bei Entscheidungen ob in Forschung investiert werden soll die Gefahr, dass Vorsprung durch Forschung rasch wieder konkurrierende Forschung zunichte gemacht werden könnte. Die Forschung wurde von den beiden lange und erfolgreich weitergeführt und entsprechend vertieft \parencite{Aghion2005}.

Nicht vergessen werden sollte an dieser Stelle ein interessanter, aufstrebender Ansatz zu langfristigen Wachstumsmodellen: Daron Acemoglu führt langfristiges Wachstum auf die Existenz und Qualität von Institutionen zurück. Diesem Thema widmen wir uns etwas später im Rahmen des Kapitels \ref{sec: Neue Inst}. Zuvor gehen wir auf ein Thema ein, dass wir im aktuellen Kapitel unreflektiert eingeführt und ausgeschlachtet haben: Was ist überhaupt Humankapital?

\section{Becker: Rational Choice Theory}  \label{sec: Becker}

In der Mikroökonomie spielte rationales Entscheidungsverhalten schon beim Übergang von \textit{Klassik} zu \textit{Neoklassik} eine Rolle. Die "`Rational Choice Theory"' ist hierbei ein Überbegriff, der in der Humankapitaltheorie einen Höhepunkt erreicht hat, weil hier weitere Bereiche des menschlichen Zusammenlebens formal analysiert werden. Im Jahr 1947 wurde der \textit{Homo Oeconomicus} - also der rational handelnde Mensch - (vgl. Kapitel \ref{cha: Spieltheorie}) durch Von Neumann und Morgenstern in der Erwartungsnutzen-Theorie formalisiert \parencite{VonNeumann1944}. Wenig später allerdings kam es schon zu erheblichen Zweifeln an diesem Prinzip durch das \textit{Allais-Paradoxon}. Das rationale Verhalten in der Ökonomie ist seit jeher ein umstrittener Begriff, der die Wirtschaftswissenschaften geradezu spaltete. Die Kontroversen spielen sich auf verschiedenen Ebenen ab. Manche lehnen Rationalverhalten als menschliches Verhalten komplett ab (vgl. Behavioral Economics), wiederum andere unterscheiden akzeptieren das Rationalverhalten wenn es um rein wirtschaftliche Angelegenheiten geht, während andere das Rationalverhalten als Nutzenmaximierung auf alle Lebensbereiche ausdehnen. Am weitesten gingen auch hier die Vertreter der Chicago-School: Um den Beginn der 1960er Jahren stand der rational handelnde Mensch im Zentrum der neuen Humankapitaltheorie. Vorarbeiten leistete \textit{Theodore Schultz}, die Hauptprotagonisten aber waren \textit{Jacob Mincer} und vor allem \textit{Gary Stanley Becker}. Im der Humankapitaltheorie wurde wirklich alles als ökonomisches Problem betrachtet. Nicht nur das, ganz im Stile der "`Neuen Klassiker"' wurde auch alles formalisiert und entsprechend mit mathematischen Modellen erklärt.
In \textcite[Ausgabe von 1991: S. 108]{Becker1981} lautet der Titel des vierten Kapitels: "`Bildung von zueinander passenden Paaren auf dem Heiratsmarkt"'. Darin wimmelt es von mathematischen Formeln und es heißt unter anderem: "`In diesem Kapitel wird gezeigt, dass auf einem effizienten Heiratsmarkt in der Regel eine positive assortative Paarung vorliegt, bei der Männer mit hoher Qualität mit Frauen mit hoher Qualität und Männer mit niedriger Qualität mit Frauen mit niedriger Qualität zusammengebracht werden[...]."'
Man kann sich wahrscheinlich vorstellen, dass dieser Zugang innerhalb der Wirtschaftswissenschaften auf Widerstand stieß. In den Sozialwissenschaften - auf die Gary Becker seine ökonomische Theorie schließlich unweigerlich ausweitete - wurde er lange Zeit überhaupt ignoriert. "`Die meisten Ökonomen dachten nicht, dass [meine Arbeit] Ökonomie sind, Soziologen und Psychologen haben allgemein nicht akzeptiert, dass ich zu ihren Forschungsfeldern beitrage"' beschreibt \textcite{Becker1992}. Natürlich wirkt das Thema provokativ. Schon der Titel "`Humankapital"' deutet darauf hin, dass rein menschliche Eigenschaften kommerzialisiert wird, was natürlich umstritten ist.

Die Humankapitaltheorie ging in Chicago aus der Arbeitsmarktökonomie hervor. Wobei hier zunächst vor allem  \textcite{Schultz1961} als Pionier \parencite{Becker1992} tätig waren. \textcite{Becker1957, Becker1962} dehnte die Theorie aber schließlich auf das gesamte menschliche Verhalten aus. Seine frühen Arbeiten entstanden zeitlich eher parallel zu jenen der zweiten Generation der Chicago-School, zu der vor allem die Monetaristen um Milton Friedman gezählt werden. Dennoch wird die Humankapitaltheorie eher der dritten Generation der Chicago School zugeordnet. "`Gary [Becker] ist der Nachfolger von Milton Friedman in Mikroökonomie [...], Robert Lucas ist der Nachfolger in Makroökonomie"', brachte es der Ökonom Donald McCloskey in einem Interview über Gary Becker auf den Punkte \parencite[S. 137]{Warsh}. Tatsächlich wird die Humankapitaltheorie häufig mit der "`Neuen Klassik"' in Verbindung gebracht (weshalb sie auch in diesem Kapital platziert ist). Natürlich ist die "`Neue Klassik"' Teil der Makro-, die Humankapitaltheorie hingegen Teil der Mikroökonomie. Aber der streng mathematisch-analytische Zugang in Verbindung mit der strengen Marktorientierung zeigt deutlich die verbindenden Ähnlichkeiten beider Schulen.

Gary Becker verließ drei Jahre nach seiner Promotion die University of Chicago in Richtung Columbia University und National Bureau of Economic Research (NBER). Dort publizierte er sein Hauptwerk "`Human Capital"' \parencite{Becker1964} und arbeitete sehr produktiv mit Jacob Mincer zusammen. Hier entstanden jene Werke, die im Nachhinein den nachhaltigsten Einfluss auf die Wirtschaftswissenschaften hatten, weil sie später den Ausgangspunkt der neuen Wachstumstheorie darstellte, wie im letzten Kapitel \ref{sec: endogene} angeführt. Gemeinsam mit \textcite{Schultz1963} und \textcite{Mincer1974} lieferte \textcite{Becker1962} die theoretischen Grundlagen für die Auswirkungen von Bildung auf die wirtschaftliche Entwicklung. Schultz war hierbei der erste, der den Zusammenhang zwischen Investitionen in Bildung und Wirtschaftswachstum herstellte. Becker unterschied zwischen spezifische Ausbildung und Bildung allgemein. Mincer lieferte empirische Ergebnisse zum Thema.
Lange Zeit litt die Anerkennung der Arbeiten an der Kontroverse über die Inhalte. Schon der Begriff "`Humankapital"' wurde kritisiert, weil er den Menschen als Maschine darstellt und auf die Kennzahl Produktivität reduziert. Auch der Ansatz Bildung als "`Investition"' statt als "`kulturelle Erfahrung"' zu sehen, war neu und stieß auf breiten Widerstand. Gary Becker überlegte eigenen Aussagen zufolge lange, ob er sein Buch wirklich "`Human Capital"' nennen sollte, da der Begriff eben umstritten ist.

Erst nach seiner Rückkehr nach Chicago im Jahr 1970 erweiterte Becker seine Arbeit zunehmend auf weitere Bereiche des menschlichen Lebens, wie Familiengröße, Scheidung, Kinder und Altruistisches Verhalten. Diese Arbeiten resultierten schließlich in das bereits zitierte Buch \textcite{Becker1981}.

Die Forschung zu Humankapital hatte enorme Auswirkung auf die ökonomische Entwicklung. Zum einen im bereits genannten Forschungsfeld der "`endogenen Wachstumstheorie"', aber auch der riesige Bereich der mikroökonometrischen, empirischen Forschung, der um die Jahrtausendwende aufkam, behandelt wie selbstverständlich Themen, deren Etablierung in der Ökonomie auf die Arbeiten von Gary Becker zurückgehen.
Die Kontroversen darüber ob "`menschliches Verhalten"' Gegenstand wirtschaftswissenschaftlicher Forschung sein kann und darf, bestehen weiterhin. Allerdings sind in wirtschaftswissenschaftlichen Kreisen die Ressentiments dagegen schon wesentlich schwächer geworden. Längst wird die Forschung dazu nicht mehr ausschließlich von markt-gläubigen Ökonomen durchgeführt. Mit dem Nobelpreis 1992 erfuhr Gary Becker in gewisser Weise die Absolution, dass seine Forschung bedenkenlos und wertvoll ist.





\section{Wirkung und Bedeutung der Neuen Klassischen Makroökonomie}

Wurde die "`Neue Klassik"' zum neuen Mainstream? In den 1980er Jahren sah es vielleicht danach aus. Dennoch muss man dies entschieden ablehnen! Dazu war die "`Neue Klassik"' zu starr marktgläubig. Die Annahme, dass sich alle Märkte immer im Gleichgewicht befinden ist einfach nicht aufrechtzuerhalten. Ihre Vertreter waren (und sind bis heute) zu stur um die Modelle zugunsten empirisch sinnvollerer Annahmen abzuändern.
Wurde die "`Neue Klassik"' dann überwunden? Auch das muss man entschieden ablehnen! Dazu waren ihre Errungenschaften zu bahnbrechend und zu erfolgreich. Ihre Ideen wurden gerne aufgenommen von den "`Neu-Keynesianern"' (siehe nächstes Kapitel \ref{cha: Neu Keynes}). Diese lehnten zwar die Vertreter der "`Neuen Klassik"' strikt ab, nicht aber deren erfolgreichen Methoden. Ohne die starren Annahmen der "`Neuen Klassiker"' implementierten sie deren Ideen in eigene Modelle.
Die neue Mainstream-Ökonomie steht also nicht im Widerspruch zur Neuen Klassik. Vielmehr entstand aus den konkurrierenden Schulen der 1970er- und 1980er-Jahren - den "`Neuen Klassikern"' und den wenig miteinander verbundenen "`Neu-Keynesianern"' - Anfang der 1990er Jahre eine neue gemeinsame Mainstream-Ökonomie, eine "`Neue Synthese"'. (siehe Kapitel \ref{Neue Neoklassische Synthese})

Was ist von der "`Neuen Klassik"' geblieben? Geradezu revolutioniert wurde die Ökonomie durch die formalisierte Herangehensweise der "`Neuen Klassiker"' an ökonomische Fragestellungen. Heutige Publikationen enthalten zum größten Teil hochformalisierte Modelle. Zwar war die Mathematik auch schon bei den Keynesianern - weniger bei Friedman's Monetarismus - ein zentrales Element, aber die durchgehende Formalisierung einer Fragestellung in einen formalen Rahmen und unter Nebenbedingungen war neu und setzte sich im bis heute Mainstream durch.

Geblieben ist auch die Mikrofundierung der Makroökonomie. Diese ist verbunden mit der eben genannten Formalisierung der Ökonomie. Auch die Mikrofundierung der Makroökonomie war keine "Erfindung" der "Neuen Klassiker", sondern geht ursprünglich auf Edmund Phelps zurück. Aber durchgängig angewendet wurde das Konzept erstmals von Lucas, Sargent und Co.

Die Annahme der "`rationalen Erwartungen"' ist zwar umstritten, aber hat es dennoch in die ökonomischen Mainstream-Modelle geschafft. Unumstritten spielen rationale Erwartungen eine Rolle zum Beispiel in der Geldpolitik. Die Zentralbanken verfolgen mittlerweile nicht mehr primär ein Geldmengenziel, sondern ein Inflationsziel, außerdem arbeiten sie auch konkret mit Inflations\textit{erwartungen}.

Zumindest einen wesentlichen Einfluss auf die moderne, wissenschaftliche Ausrichtung der Zentralbanken hatten die Autoren, die mit der Neuen Klassik in Verbindung gebracht werden. Es würde zwar zu weit gehen, wenn man diese Entwicklung alleine der "`Neuen Klassischen Makroökonomie"' zuschreiben würde, aber vor allem die Arbeiten von Robert Barro können zumindest in gewissem Maße als theoretische Begründung dafür gesehen werden, dass die meisten modernen Notenbanken heute ein "`Inflation-Targeting"' betreiben und stärker als früher auch als tatsächlich politisch unabhängige Zentralbanken geführt werden.

Was hat sich als falsch erwiesen? Der größte Fehler war das Festhalten am perfekten Funktionieren der Märkte auch in der kurzen Frist. Dass es keine unfreiwillige Arbeitslosigkeit gäbe war rasch empirisch nicht zu halten und modelltheoretisch nicht notwendig wie spätere Modelle von "`Neu-Keynesianern"' zeigten. Außerdem entstanden rasch "`Neu-Keynesianische"' Modelle, die bewiesen, dass die Theorie der Rationalen Erwartung auch dann aufrecht erhalten werden kann, wenn sich Löhne und Preise nicht \textit{sofort} an das allgemeine Gleichgewicht anpassen, sondern langsam über mehrere "`Entscheidungsrunden"'. Damit fiel auch die Annahme, dass Geldpolitik und Fiskalpolitik komplett unwirksam seien. Die aktuelle Mainstream-Ökonomie geht davon aus, dass zumindest in der kurzen Frist beide wirksam sind.

