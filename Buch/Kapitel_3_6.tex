%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neoklassische Finanzierungstheorie}
\label{Finance}

Neoklassische Finance, Modern Finance, schlicht Finance

Bis heute hat die Finance eine interessante Entwicklung durchgemacht. Der Forschungsaufwand war in dieser Disziplin stärker in fast allen anderen ökonomischen Feldern, vor allem im privaten Bereich. Dies ist wenig überraschend, schließlich erhoffen sich viele bis heute durch Finanzanlagen reich zu werden. Dies ist insofern interessant, als die - bis heute gültige - grundlegende Annahme davon ausgeht, dass man zukünftige Kursentwicklungen von Assests nicht vorhersehen kann. Sich diese stattdessen entlang eines reinen Zufallspfades entwickeln. Und auch in der Praxis ist das Angebot an Finanzmarktprodukten zweigeteilt. Zum einen gibt es quantitativ-wissenschaftlich geführte Fonds. Aber daneben gibt es noch immer einen erheblichen Zulauf zu "`Gurus"' oder Anlageberatern, die überzeugt davon sind den Markt schlagen zu können. Die neoklassische Finance wurde und wird aber auch von wissenschaftlicher Seite regelmäßig in Zweifel gezogen. Auch dies ist nicht überraschend: Schließlich hat die wissenschaftliche Weiterentwicklung in diesem Gebiet in keinster Weise dazu beigetragen, die Anzahl von Kursstürzen an den Börsen zu verringern. Weder der "`Schwarze Montag"' im Jahr 1987, noch die "`Dot-Com-Blase"', die im Jahr 2000 platzte, noch der Börsencrash zu Beginn der "`Great Depression"' 2007 passen so recht in das Konzept der "`Effizienten Finanzmärkte"'. Nicht zuletzt deshalb haben sich mit der Behavioral Finance (vgl. Kapitel \ref{Behavioral}) und innerhalb des Post-Keynesianismus (vgl. Kapitel \ref{Post-Keynes}) starke heterodoxe Ansichten zur Finanzierungstheorie gebildet, deren Bedeutung bis heute hoch einzuschätzen ist (vgl. Kapitel \ref{Finanzmarkt}). Die "`Modern Finance"' ist tatsächlich von vielen Seiten her angreifbar und - wie jede Theorie - weit weg davon die Realität vollständig abbilden zu können. Ihr wesentlicher Vorteil liegt aber darin die Konzepte von Nutzen, Ertrag und Risiko unter recht plausiblen Annahmen in mathematisch extrem eleganter Form miteinander zu vereinen. 



\section{Vorläufer der Finanzierungstheorie}
\label{FisherundKnight}

HIER WEITER

Böhm-Bawerk, Fisher und Knight als Vorläufer

Begründer der Finanzmathematik: \textcite{Bachelier1900} Begründer der Random-Walk Theorie und schon Wiener Prozesse! 
Wichtigkeit der Normalverteilung der Renditen. Anleger machen in der Portfoliotheorie ihr Investment ausschließlich von erwarteter Rendite ($\mu$) und deren Standardabweichung ($sigma$) abhängig. Dies kann nur mit einer bestimmten Risikofunktion (quadratisch), oder aber durch die Annahme normalverteilter Renditen mit der Erwartungsnutzentheorie in Einklang gebracht werden. Querverweis Kritik daran durch Mandelbrot.


\section{Erwartungsnutzen und Arrow's Risikoaversion}
\label{Erwartungsnutzen}
Von-Neumann-Morgenstern-Nutzenfunktion (Querverweis auf Maurice Allais 1952)
St. Petersburg Paradoxon
Mean-Variance-Maß Arrow-Pratt-Maß
Auch die Vorlesungsfolien Master berücksichtigen



\section{Die Relevanz der Irrelevanz}
\label{Struktur}
Die Arbeit von \textcite{Modigliani1958} zur Kapitalstrukturtheorie gilt bis heute als \textit{die} Grundlage der wissenschaftlichen "`Corporate Finance"'. Bis zu diesem Zeitpunkt wurde die Finanzierung von Unternehmen als reiner Teil der Betriebswirtschaft gesehen. Unternehmen wurden ausschließlich individuell betrachtet. Franco Modigliani und Merton Miller revolutionierten diese Ansicht, indem sie sich erstmals wissenschaftlich der Kapitalstruktur von Unternehmen annäherten. Sie gingen von einem perfekten Kapitalmarkt aus, auf dem es keine Steuern und Transaktionskosten gibt und sich Unternehmen zu einem konstanten Fremdkapitalzinssatz finanzieren können. Der Wert eines Unternehmens bestimmt sich ausschließlich aus den zukünftigen, abgezinsten Cashflows. Und ausgehend von diesen Rahmenbedingungen, zeigten \textcite{Modigliani1958} mit einem einfachen Arbitrage-Argument, dass die Zusammensetzung der Finanzierung eines Unternehmens, also der Anteil von Eigenkapital und Fremdkapital, keinerlei Auswirkungen auf den Unternehmenswert hat. Das Konzept wurde als "`Irrelevanztheorie"' bekannt und klingt nicht besonders spannend. Damit sind aber interessante Punkte verbunden. Zunächst war der Ansatz, mittels Arbitrageargument einen wissenschaftlichen Beweis zu führen, richtungsweisend. Der "`Law of one Price"'-Ansatz blieb in der Finanzierungstheorie bis heute das wichtigste Konzept und bildet auch die Grundlage für die Effizienzmarkthypothese (vgl. Kapitel \ref{Efficient}). Inhaltlich revolutionär war aber die Erkenntnis, dass der Wert eines Unternehmens gänzlich von seiner Kapitalstruktur unabhängig ist. Das Arbitrageargument lautet hierfür wie folgt: Man stelle sich zwei identische Unternehmen vor, die sich ausschließlich darin unterscheiden, dass Unternehmen A aus zwei Investoren besteht, die beide das Eigenkapital des Unternehmens stellen, während Unternehmen B aus einem Eigenkapitalgeber und einem Fremdkapitalgeber besteht. Von den Gewinnen muss Unternehmer B zunächst einmal die Fremdkapitalzinsen bezahlen. Der Rest des Gewinns gehört aber dem Eigenkapitalgeber. Bei Unternehmen A fließen die kompletten Gewinne den Eigenkapitalgebern zu, diese müssen sich die Gewinne aber teilen. Die beiden Unternehmer B verdienen also weniger als der eine Eigenkapitalgeber in A. Es scheint so als wäre Unternehmen A dann auch wertvoller, schließlich wirft es mehr Gewinn für seine Eigentümer ab. Das stimmt aber nicht. Man darf nämlich nicht vergessen, dass im Verlustfall der eine Eigentümer A den gesamten Verlust tragen muss, während die beiden B-Investoren sich auch die Verluste aufteilen. Die höheren Ertragschancen in Unternehmen A werden durch das höhere Risiko genau ausgeglichen. Daraus resultiert der berühmte Hebeleffekt, auch im deutschsprachigen Raum meist einfach "`Leverage"' genannt: Durch die Aufnahme von Fremdkapital kann ich die erwartete Rendite erhöhen, allerdings steigt damit auch das Risiko des Investments. Das lässt sich wiederum erweitern auf die Kapitalkosten. Die berühmte WACC-Formel - bis heute Teil jeder einführenden Finanzierungs-, Controlling- und Finanzierungsvorlesung - geht direkt auf \textcite{Modigliani1958} zurück: Die durchschnittlichen, gewichteten Kapitalkosten eines Unternehmens bleiben, unabhängig von der Kapitalstruktur, konstant.

Die Arbeit von \textcite{Modigliani1958} war zwar rasch wissenschaftlich etabliert, allerdings war sie auch der Kritik ausgesetzt, dass die starken Annahmen eines perfekten Kapitalmarktes in der Realität nie anzutreffen ist. Insbesondere die Annahme, dass es keine Steuern gäbe, ist schlicht falsch. Aus diesem Grund wurde mit \textcite{Modigliani1963} eine Erweiterung des Modells veröffentlicht. Fremdkapital ist steuerlich begünstigt, da die Fremdkapitalzinsen als Aufwand den Gewinn und damit die Steuerlast des Unternehmens verringern. Abhängig von der Höhe des Steuersatzes lässt sich somit die Kapitalstruktur optimieren.

\section{Fama: Nichts arbeitet so effizient wie der Markt}
\label{Efficient}

Die Theorie der Effizienten Märkte ist eine Grundlage der anderen Konzepte der modernen Finanzwirtschaft. Markteffizienz bedeutet, dass der Markt sämtliche relevante Informationen zur Verfügung hat und die Kräfte aus Angebot und Nachfrage zu jedem Zeitpunkt dafür sorgen, dass der Preis in seinem wahrem Gleichgewicht liegt. Es gibt ein einfaches Argument dafür, dass diese Theorie grundsätzlich Sinn macht. Stellen Sie sich vor Sie hätten eine zuverlässige Information darüber, dass ein bestimmtes Asset morgen um 10\% an Wert gewinnt. Es wäre nur rational dieses Asset heute zu kaufen, also nachzufragen, um den freien Gewinn - den "`Free Lunch"' - mitzunehmen. Die dadurch generierte Nachfrage würden den Preis aber ansteigen lassen. Das Asset würde also nicht erst morgen um 10\% steigen, sondern durch die erhöhte Nachfrage heute schon. Denkt man dieses Konzept konsequent durch, bleibt nur die Lösung, dass die Märkte ständig die wahren Preise abbilden. Also effizient sind, es gibt keinen "`Free Lunch"'. Dennoch ist keine Theorie innerhalb der Modern Finance so umstritten wie jene der Effizienten Märkte. Dafür gibt es vor allem empirische Gründe. Die großen Kursschwankungen, vor allem aber rasche Einbrüche an den Finanzmärkten, wie jene am "`Schwarzen Donnerstag"' 1929, oder am "`Schwarzen Montag"' im Jahr 1987, lassen sich kaum mit der Idee der Markteffizienz vereinbaren. Zur Kritik später mehr.

Die Effizienzmarkttheorie ist auch relativ schwer eindeutig zuzuordnen. Die Idee wird oft in Verbindung mit der "`Random-Walk-Theorie"' gesehen, die - wie oben beschrieben - ursprünglich auf \textcite{Bachelier1900} zurückgeht. Demnach repräsentieren Aktienkurse immer effiziente Märkte und zukünftige Kursschwankungen erfolgen rein zufällig\footnote{Im Umkehrschluss muss aber eben nicht gelten, dass Kursschwankungen, die einem zufälligen Pfad folgen auch tatsächlich effizient sind.}. Unumstritten ist auf jeden Fall, dass \textcite{Fama1970} die Effizienzmarkthypothese als erster operationalisierte und auch ausführliche empirische Untersuchungen dazu veröffentlichte. Eugene Fama unterscheidet zwischen schwacher, mittelstarker und starker Informationseffizienz. Die schwache Effizienz sagt aus, dass die aktuellen Kurse alle historischen Kursinformationen schon berücksichtigt hat, die mittelstarke Effizienz umfasst zusätzlich alle öffentlich zugänglichen Informationen, wie Bilanzen, Veröffentlichungen oder Presseberichte. Für die Gültigkeit dieser beiden Ausprägungen der Effizienzmarkthypothese fand \textcite{Fama1970} recht starke empirische Argumente. Damit ist übrigens verbunden, dass so weitverbreitete Methoden wie die technische Analyse, oder die Fundamentalanalyse keinerlei Informationsgehalt haben. Bei Vorliegen der starken Effizienz würden selbst Insider-Informationen schon im Preis enthalten sein, dafür fand aber selbst \textcite{Fama1970} keine überzeugenden empirischen Anhaltspunkte.
Kritik an der Theorie fand sich schon bald nach deren Publikation. Einen interessanten theoretischen Kritikpunkt veröffentlichten \textcite{Grossman1980}. Sie argumentierten, dass die Beschaffung von Marktinformationen Kosten verursacht. Im Zusammenhang mit dem Vorliegen der Effizienzmarkthypothese entsteht folgende paradoxe Situation. Wenn die Preise alle Informationen bereits enthalten und die Informationsbeschaffung Kosten verursacht, aber keinen Nutzen bringt, dann wird kein rationaler Marktteilnehmer den Aufwand der Informationsbeschaffung auf sich nehmen. Wenn aber niemand Informationen beschafft, dann können im Umkehrschluss die Preise nicht Informations-effizient sein. Schwankender Informationsbeschaffungsaufwand seitens der Marktteilnehmer könnte damit das Auftreten von Finanzblasen erklären. Über die Finanzwelt hinaus berühmt würde auch Robert Shiller, vor allem durch die Einführung des Case-Shiller-Index - einem Immobilienindex - und das populärwissenschaftliche Buch "`Irrational Exuberance"', das knapp vor dem Platzen der "`Dot-Com-Blase"' im Jahr 2000 veröffentlicht wurde. Seinen wissenschaftlichen Durchbruch schaffte Shiller bereits im Jahr 1981. Er argumentierte empirisch-statistisch begründet, dass Aktienkurse viel zu stark schwanken um mit der Effizienzmarkthypothese vereinbar zu sein \parencite{Shiller1981}. Punktuelle Angriffspunkte auf die Effizienzmarkthypothese lieferten seit den 1970er Jahren die Vertreter der Behavioral Finance, die aber in Kapitel \ref{Behavioral} näher behandelt wird.

Dass die Effizienzmarkthypothese selbst innerhalb der Wirtschaftswissenschaften umstritten ist, zeigt sich unter anderem auch an der doch recht amüsant anmutenden Tatsache, dass im Jahr 2013 mit Eugene Fama und Robert Shiller zwei Ökonomen mit dem Nobelpreis ausgezeichnet wurden, die geradezu gegenteilige Meinungen zur Effizienzmarkthypothese vertreten. Dennoch ist sie nach wie vor Bestandteil der Mainstream-Ökonomie. Man ist sich der Schwächen des Konzepts zwar durchaus bewusst, aber wie jedes Modell scheint es eine ausreichend gute Näherung an die Realität darzustellen. Vor allem aber ist es die Grundlage für die gesamte Neoklassische Finanzierungstheorie, die ein theoretisch gut fundiertes System darstellt, bei dem ein Rädchen wunderbar ins andere greift. Die Alternativen, wie die Behavioral Finance (vgl. Kapitel \ref{Behavioral}) oder die Finanzmarkt-Ansätze der Post-Keynesianer (vgl. Kapitel \ref{Post-Keynes}) konnten bislang zwar punktuell wichtige Kritikpunkte aufwerfen und Teillösungen anbieten, aber eben kein so elegantes Modell wie jenes der Neoklassischen Finance.


\section{Markowitz: Don't put all your eggs in one basket}
\label{Portfolio}

Chronologisch gesehen ist die wahrlich bahnbrechende Arbeit zur Portfoliotheorie von \textcite{Markowitz1952} die erste Arbeit der neoklassischen Finanzierungstheorie. Neben \textcite{Modigliani1958} zählt sie somit zu deren "`Gründungsarbeiten"'. Zur Entstehung des Artikels "`Portfolio Selection"', der im wesentlichen auch die Doktorarbeit von Harry Markowitz darstellt, gibt es unzählige Geschichten. So erzählt Markowitz in einem Interview\footnote{Die Stelle findet sich hier: https://www.youtube.com/watch?v=RVWEhCd819E, Minute 00:50. In diesem Interview findet sich auch die Anekdote mit der Defensio bei Milton Friedman und  Jacob Marschak.}, dass eine Börsenhändler ihm den Tipp gegeben hätten sich einem Finanzthema zu widmen. Eine Anekdote von der abschließenden Defensio seiner Doktorarbeit gab Markowitz im Rahmen seiner Nobelpreis-Lectures zum Besten: "`Professor Milton Friedman argumentierte [Anm.: Wahrscheinlich scherzhaft], dass Portfolio-Theorie kein Teil der Ökonomie sei und sie ihm daher keinen Doktortitel in Ökonomie für eine Dissertation [dafür] geben können."' \parencite[S. 286]{Markowitz1990}. Beide Geschichten machen deutlich wie bahnbrechend seine Arbeit im Jahr 1952 gewesen ist. Etwas das auch \textcite{Rubinstein2002} im Rahmen des 50-Jahr Jubiläums von "`Portfolio Selection"' hervorhob: "`Am beeindruckendsten an Markowitz' 1952 Artikel fand ich, dass er aus dem Nichts zu kommen scheint"'. Tatsächlich lautete, leicht übertrieben, die Prämisse auf den Finanzmärkten vor 1952: Suche die Aktie von der du dir die höchste Rendite erwartest und kaufe sie. Das Konzept der naiven Diversifikation ist schon seit jeher bekannt und auch das Konzept vom Risiko-Rendite-Trade-Off war nicht neu. Aber es gab keine quantitativ-mathematischen Ansätze zur Formalisierung dieser Konzepte. Dies ist einigermaßen überraschend. Wertpapierhandels gibt es schließlich schon seit Jahrhunderten.  Und - anders als in der Makroökonomie - haben selbst die "`Great Depression"', bzw. die Kursverluste in Folge des "`Schwarzen Donnerstags"' im Jahr 1929, keinen Durchbruch in diesem Bereich ausgelöst. Warum war die Arbeit nun so bahnbrechend? Nun, \textcite{Markowitz1952} war die erste rein \textit{mathematisch-quantitative} Arbeit im Bereich der Finanzmarktanalyse. Als solche wurden darin gleich drei wesentliche Konzepte etabliert, deren Bedeutung bis heute unumstritten ist: Erstens, die Varianz der Aktienrenditen wurde als Risikomaß. Zweitens, der Risiko-Rendite-Trade-Off - also die Annahme, dass höhere erwartete Rendite immer auch mit höherem Ausfallsrisiko verbunden ist, und drittens, Die Bedeutung der Korrelation von Aktienrenditen. Letzteres ist nichts anderes als die mathematische Fundierung der Diversifikation. Dieser letzte Punkt wird häufig als der wesentliche bezeichnet und tatsächlich basiert darauf die zentrale Idee des Artikels: Aktienportfolios aus der Kombination von Einzeltiteln zu bilden, die das optimale Verhältnis zwischen Rendite und Risiko abbilden. Was heißt das konkret? Wenn Sie eine Aktie kaufen so erwarten sie in Zukunft eine Rendite von x\%. Diese Erwartung bildet sich aus den vergangenen Renditen dieser Aktie. Da die Rendite bei Aktien aber nie konstant ist, sondern zufälligen vgl. Kapitel \ref{Efficient} - Schwankungen unterliegt, ist diese Rendite stets nur eine Erwartung. Aus den vergangenen Schwankungen lässt sich eine durchschnittliche Schwankung - die Standardabweichung - berechnen. Diese wird in weiterer Folge als Risikomaß herangezogen. Je stärker der Wert der Aktie schwankt, desto schwieriger ist deren zukünftiger Wert zu prognostizieren. Oder mit anderen Worten: Desto höher ist ihr Risiko. Stellen Sie sich nun \textit{zwei} Aktien vor. Für beide können sie einen Rendite-Erwartungswert, sowie eine Standardabweichung berechnen. Wenn Sie beide Aktien zu gleichen Teilen kaufen, so entspricht ihr Rendite-Erwartungswert dieses Portfolios dem Mittelwert der Renditen der beiden Aktien. Für die Berechnung der Standardabweichung stimmt dies aber \textit{nicht}! Die Renditen von Aktien verlaufen niemals genau gleich. Das heißt sie korrelieren niemals zu 100\%. Viele Aktien bewegen sich zwar tendenziell in die gleiche Richtung, aber manche Assets sind unabhängig von anderen, bzw. korrelieren sogar negativ. Das heißt bei der Berechnung des Risikos des Portfolios reicht es nicht aus einfach den Mittelwert der Risiko-Werte der Einzeltitel heranzuziehen. Stattdessen muss auch die Korrelation zwischen den beiden Titeln berücksichtigt werden. Wenn zwei Titel perfekt negativ miteinander korrelieren (was ebensowenig vorkommt wie perfekt positive Korrelation), dann steigt eine Aktie immer dann wenn die andere fällt. Dies ist gut für das Portfolio-Risiko: Da der Gewinn der einen Aktie den Verlust der zweiten Aktie immer zumindest teilweise ausgleicht. Die Standardabweichung des Portfolios ist daher stets geringer als der gewichtete Durchschnitt der Standardabweichung der einzelnen Aktien. \textcite{Markowitz1952} zeigte dies erstmals mathematisch. In der Folge kann man natürlich Portfolios aus vielen Einzeltitel zusammenstellen. Sogenannte "`Effiziente Portfolios"' sind aber nur solche, bei denen für eine gegebene Rendite keine niedrigere Standardabweichung erzielt werden kann. Das heißt, es gibt bei Markowitz nich das \textit{eine} optimale Portfolio, sondern eine Reihe von optimalen Portfolios. Diese liegen allesamt auf der "`Efficient Frontier"'. Rationale Individuen sollten nur solche Portfolios erwerben. Welches genau hängt bei \textcite{Markowitz1952} noch von der individuellen Risikoaversion des Investors ab. Die Arbeit gilt heute, 70 Jahre später, noch immer als Ausgangspunkt für quantitatives Asset-Management. 

Mit einer recht intuitiven Idee wurde die Markowitz-Portfoliotheorie durch \textcite{Tobin1958} erweitert. Und zwar indem er die Berücksichtigung eines risikolosen Assets einführte. Die Markowitz-Portfoliotheorie behandelt ausschließlich risikobehaftete Assets. Wenn man jetzt zum Beispiel eine risikolose Anleihe heranzieht, so hat diese einen bestimmten Erwartungswert und eine Standardabweichung von - definitionsgemäß - Null. Dieser Anleihe wird als Fixpunkt betrachtet. Ausgehend von diesem Fixpunkt wird nun eine Tangente an die "`Efficient Frontier"' gelegt. Per Definition berührt diese Gerade die Efficient Frontier nur in einem einzigen Punkt. Dieser Punkt stellt das tatsächlich einzige optimale Portfolio - genannt "`Marktportfolio"' - dar. Die Verbindungslinie zwischen risikoloser Anleihe und Marktportfolio nennt man die "`Kapitalmarktlinie"' (Capital Market Line). Das Marktportfolio ist in diesem theoretischen Konstrukt das einzig sinnvolle Portfolio. Unabhängig von der Risikoaversion kann nämlich aus der Kombination aus risikoloser Anleihe und Marktportfolio stets eine höhere Rendite-Erwartung (bei fixiertem Risiko) erzielt werden, als auf einem beliebigen Punkt auf der Efficient Frontier. Diese Erkenntnis wurde als die \textsc{Tobin-Separation} bekannt. Der Name Separation steht hierbei dafür, dass bei Finanzinvestitionen zwei voneinander unabhängige Entscheidungen getroffen werden müssen. Erstens, es muss das Marktportfolio ermittelt werden. Dieses ist allerdings für jeden risiko-adversen Investor identisch. Zweitens, abhängig von der individuellen Risikoaversion muss ein Investor entscheiden welchen Anteil seines Vermögens er in das risikobehaftete Marktportfolio steckt und welchen Anteil in die risikolose Anleihe. Auch dieses Verhältnis kann man übrigens - für jedermann individuell - quantitativ berechnen. Für jedes risiko-adverse Individuum lässt sich eine Nutzenfunktion ermitteln. Die individuelle Risikoaversion (vgl. Kapitel \ref{Erwartungsnutzen}) kann als "`Mean-Variance"'-Maß\footnote{"'Mean"' bezeichnet hierbei der Erwartungswert der Renditen und "`Variance"' die Varianz, also das Quadrat der Standardabweichung und damit das Risiko.} ausgedrückt werden. Die Nutzenfunktion lässt sich somit mittels Indifferenzkurven in das Portfolio-Diagramm überführen. Der Tangentialpunkt von Indifferenzkurve und Kapitalmarktlinie bestimmt das optimale, individuelle Verhältnis zwischen risikoloser Anleihe und Marktportfolio.

In den 1960er Jahren wurde Tobin's Modell schließlich zum \textsc{Capital Asset Pricing Model (CAPM)} (sprich: CÄP-M) weiterentwickelt. Gleich vier Autoren haben dessen Entwicklung parallel vorangetrieben: \textcite{Sharpe1964}, \textcite{Lintner1965}, \textcite{Mossin1966} und später wurde auch \textcite{Treynor1961} die Idee zugeschrieben. Der Ausgangspunkt ist, dass mittels Diversifikation - wie bei Markowitz dargestellt - Unternehmens-spezifisches Risiko eliminiert werden kann. Einfach deswegen, weil man in viele verschiedene Unternehmen investiert und der Anteil eines bestimmten Unternehmen mit steigender Anzahl an Assets gegen Null geht. Das Unternehmens-spezifische Risiko wird hierbei unsystematisches Risiko genannt. Nicht weg-diversifizieren kann man das systematische Risiko. Dieses wird auch Marktrisiko genannt, bzw. im CAPM als ($\beta$) bezeichnet. Da man im CAPM davon ausgeht, dass man das unsystematische Risiko durch Diversifikation vollständig eliminieren kann, wird man für die eventuelle Übernahme von unsystematischem Risiko (durch fehlende Diversifikation) nicht belohnt. Im CAPM wird daher nur das Marktrisiko betrachtet. Die entsprechenden Darstellungen zeigen daher stets den Trade-Off zwischen erwarteter Rendite und Beta (statt Standardabweichung bei Markowitz und Tobin). Das Marktrisiko wird dabei auf das "`Risiko des Gesamtmarktes"' bei 1 standardisiert. Natürlich ist umstritten was in der Praxis "`der Gesamtmarkt"' ist. Durchgesetzt haben sich hierbei aber breit gefasste Indizes wie der MSCI World. Es werden aber auch Indizes eines Einzelstaates durchaus als Gesamtmarkt herangezogen. Die Renditen der einzelnen Unternehmen (Aktien) werden ins Verhältnis zu diesem Gesamtmarkt gesetzt. Das heißt man schaut sich für einen gewissen Zeitraum, zum Beispiel fünf Jahre, wie sich die monatlichen Marktrenditen zu den monatlichen Einzelaktien-Renditen verhalten haben. Dies wird mathematisch analog zu einer univariaten, linearen Regression gemacht: Die gemeinsame Varianz (Covarianz) von Marktrendite und Einzelaktie-Rendite wird ins Verhältnis zur Marktrendite gesetzt. Das Ergebnis ist eben der $\beta$-Wert. Oder mathematisch: Die Steigung der Regressionsgerade, die bei Regressionen eben auch $\beta$ genannt wird. Ist dies Wert kleiner als eins, so unterliegt die Einzelaktie geringeren Schwankungen als der Gesamtmarkt und umgekehrt. Dieses Maß für das systematische Risiko eines Unternehmens hat bis heute eine enorme Bedeutung in der Unternehmensbewertung. Trotz aller Kritik und weiterentwickelten Methoden, wird in den überwiegenden Fällen der Eigenkapitel-Wert eines Unternehmens noch immer mittels $\beta$-Werten geschätzt. \textcite{Hamada1972} erweiterte das Modell schließlich noch um eine Bereinigung von Effekten der Unternehmens-Kapitalstruktur (vgl. Kapitel \ref{Struktur}). Bis heute sind das CAPM und vor allem die daraus abgeleiteten Risikomaße $\beta$, oder das \textsc{Sharpe-Ratio} - die Überrendite einer Aktie im Verhältnis zu ihrer Standardabweichung - zentrale Kennzahlen in der Finanzwelt. Das ist einigermaßen überraschend, da das CAPM seine Prognosen nur auf Grundlage einer einzigen Kennzahl, der vergangenen Rendite, trifft. Außerdem bleibt die Frage was das \textit{eine} Marktportfolio sei. Zudem kann die Heranziehung unterschiedlich langer Betrachtungszeiträume, zu recht unterschiedlichen $\beta$-Werten führen. Als Alternative veröffentlichte \textcite{Ross1976} sein Arbitragepreismodell. Ebenfalls Eingang in die Standardliteratur fand die CAPM-Erweiterung von \textcite{Fama1993}. Neben der vergangenen Rendite, werden hierbei die Unternehmensgröße, sowie das Kurs-Buchwert-Verhältnis als zusätzliche Einflussfaktoren herangezogen. Eine praktisch überaus bedeutende Erweiterung der Portfolio-Theorie präsentierten \textcite{Black1992}. Darin drehten sie das Konzept der Portfolio-Theorie um, indem sie nicht zukünftige Renditen schätzen, sondern lassen stattdessen die Portfolio-Zusammensetzung für gegebene Renditen vom Modell berechnen. Dies umgeht das praktische Problem, dass Schätzmethoden für Renditen immer mit großen Unsicherheiten behaftet sind.

\section{Die Bepreisung von Optionen}
\label{Optionen}

Der eben genannte Fischer Black lieferte sein Meisterstück bereits in den frühen 1970er Jahren mit der nach ihm und seinem Forschungskollegen Myron Scholes benannten Optionspreisformel. Aber langsam. Zu Beginn der 1970er Jahre veränderten sich die Anforderungen an die internationalen Finanzmärkte grundlegend. Das Bretton-Woods-System, das geschaffen worden war um stabile Wechselkurse und somit finanzielle Planungssicherheit im internationalen Handel zu sichern, brach 1973 endgültig zusammen. Die Wechselkurse waren den Marktkräften ausgesetzt. Fixe Wechselkurse konnte man sich in der Folge nur noch durch Finanztermingeschäfte sichern. Im selben Jahr wurde in den USA das Chicago Board of Options Exchange (CBOE) gegründet. Bis heute die zentrale Terminbörse der USA. Der Markt für Termingeschäfte war also im Wachsen. Was aber fehlte war eine theoretisch fundierte Theorie zur Bepreisung von Optionsgeschäften. Und diese lieferten im gleichen Jahr \textcite{Black1973}. Die resultierende Optionspreisformel ist seither als "`Black-Scholes-Modell"', nur selten "`Black-Scholes-Merton-Modell"' bekannt. Die Arbeit von \textcite{Merton1973} erweiterte das Modell indem er die Existenz von Dividenden und schwankenden Zinssätze berücksichtigte \parencite{Scholes1997}.  Die faire Bepreisung von Optionen ist alles andere als ein banales Problem. Ausschlaggebend dafür ist die asymmetrische Pay-Off-Struktur von Optionen. Optionen gleichen einer Wette: Für einen bestimmten Einsatz - den Optionspreis - hat der Käufer einer Option das Recht - aber nicht die Verpflichtung - zu einem bestimmten Zeitpunkt in der Zukunft ein dahinter liegendes Asset (Underlying) zu kaufen. Wenn der Kurs dieses Assets zwischen Kauf der Option und dem festgelegten Ausübungszeitpunkt steigt, so macht der Käufer der Option einen Gewinn. Steigt der Kurs des Underlyings nicht, so lässt er seine Option einfach verfallen. Sein Verlust beschränkt sich dann auf den Optionspreis, den er bereits bezahlt hat.
Das heißt für die Bepreisung, dass man eine Wahrscheinlichkeitsverteilung finden muss, aus der man ablesen kann, welchen Wert das Underlying im Ausübungszeitpunkt der Option mit welcher Wahrscheinlichkeit hat. Zur Berechnung dieser bedienten sich \textcite{Black1973} einer Formel aus der Physik. Nämlich der Brownschen-Bewegung, mit der man zum Beispiel die Ausbreitung von Wärme berechnen kann. Man geht auch bei diesem Konzept davon aus, dass zukünftige Renditen einem Zufallsprozess folgen, der aber eine logarithmische Normalverteilung abbildet, dies bildet eben die Brownsche Bewegung ab. Die Black-Scholes-Formel - eine stochastische Differenzialgleichung - berechnet unter diesen Annahmen aus dem derzeitigen Kurs des Underlyings, dem Ausübungspreis, sowie der Standardabweichung des Underlyings\footnote{Außerdem benötigt man einen konstanten, risikolosen Zinssatz und die Prämissen, dass keine vorzeitige Ausübung möglich ist und keine Dividenden bezahlt werden, müssen nach \textcite{Black1973} ebenfalls gelten.} den fairen Optionspreis.
Ende der 1970er Jahre entwickelten \textcite{Rubinstein1979} übrigens das sogenannte Binominal-Modell zur Optionspreisbewertung.Es handelt sich hierbei aber um ein diskretes Modell. Das heißt man geht davon aus, dass das Underlying in jeder Periode mit einer gewissen Wahrscheinlichkeit steigt bzw. mit der Gegenwahrscheinlichkeit fällt. Erhöht man die Anzahl der Perioden, entsteht ein Baum an dessen Enden man jeweils den Wert der Option für eine bestimmte Wahrscheinlichkeit ablesen kann. Erhöht man die Anzahl der Perioden gegen unendlich liefert das Binominalmodell genau das gleiche Ergebnis wie die Black-Scholes-Formel. In den folgenden Jahrzehnten wurden Derivative Wertpapiere wie Optionen vielfach weiter entwickelt und deren Verbreitung steigerte sich enorm. Vor allem in der Nachbetrachtung der "`Great Depression"' gerieten Derivate in der Öffentlichkeit in Verruf. Ein Verbot bestimmter Derivate wurde gefordert und zweitweise auch in gewissen Bereichen eingeführt. Tatsächlich sind reine Finanz-Wetten auf Soft-Commodities wie zum Beispiel Weizen oder Schweinehälften verwerflich. Man muss aber so realistisch sein, dass man eben kaum unterscheiden kann zwischen dem sinnvollen Einsatz von Derivaten, zum Beispiel im Rahmen von Absicherungsgeschäften, und reinen Finanzspekulations-Geschäften. Faktum ist, dass viele moderne Derivate - zum Beispiel Knock-Out Zertifikate - eine komplizierte Auszahlungsstruktur haben und häufig nur mehr mittels numerischer Rechenmethoden bewertet werden können. Auch die Bewertungsmethoden haben sich in den letzten Jahrzehnten also wesentlich weiterentwickelt.

In seiner Nobelpreisrede \parencite{Scholes1997} und in \textcite[S. 311]{Bernstein1996} wird die Geschichte der drei Schöpfer der Optionspreistheorie, Fischer Black, Myron Scholes und Robert Merton, lebhaft erzählt. Besonders interessant ist die Tatsache, dass das Paper \textcite{Black1973} von drei renommierten Journals abgelehnt wurde, bis es schließlich - nach Interventionen - doch noch im \textit{Journal of Political Economy} veröffentlicht wurde \parencite[S. 136]{Scholes1997}. Ähnlich wie die oben angeführte Geschichte vom Zweifel Friedman's an Markowitz' Portfoliotheorie, zeigt auch diese Story, dass die damalige Einführung mathematisch-quantitativer Methoden in die Finanzmarkttheorie damals als völlig unkonventionell angesehen wurde. Im Nachhinein gesehen aber auf jeden Fall aber bahnbrechend. Die Finanzmarkttheorie war in den 1970er Jahren in doppelter Hinsicht eine "`junge"' Disziplin: Markowitz war bei der Veröffentlichung seiner "`Portfolio Selection"' gerade 25 Jahre alt, William Sharpe bei der Entwicklung des CAPMs knapp 30 und die drei Optionspreistheoretiker ebenso um die 30 bei der Veröffentlichung ihrer Formel. Eine oft zitierte Side-Story zu Myron Scholes und Robert Merton ist deren Engagement beim Hedge-Fund "`Long-Term Capital Management"' (LTCM). Die beiden waren Direktoren dieses Fonds. Das Geschäftsmodell bestand - grob gesagt - darin Assets mit ähnlichem Risiko, aber unterschiedlicher Bewertung zu identifizieren. Es wurden in weiterer Folge gehebelte Wetten darauf platziert, dass sich dieser "`Spread"' zwischen den Bewertungen schließen sollte - an sich eine erwartbare Entwicklung. Mit der russischen Finanzkrise 1998 geriet dieses Konzept allerdings vollkommen aus den Fugen. Der Fonds schlitterte in die Pleite und musste schließlich sogar staatlich gerettet werden, da Auswirkungen auf die gesamte Finanzwelt befürchtet wurden.

Die vier genannten Bereiche - die Kapitalstrukturtheorie, die Effizienzmarkthypothese, die Porfoliotheorie und die Optionspreistheorie - bilden bis heute das Rückgrat der "`Modern Finance"'. Vor allem in der Hochschulausbildung bilden diese vier Konzepte die theoretische Basis jedes "`(Corporate) Finance"'-Kurses. Die Entwickler dieser Disziplin wurden in den folgenden Jahrzehnten übrigens fast allesamt mit dem Nobelpreis geehrt\footnote{Wenn auch teilweise primär für andere Beiträge zur Ökonomie, wie zum Beispiel im Fall von Franco Modigliani und James Tobin.}: James Tobin 1981, Franco Modigliani 1985, Harry Markowitz, Merton Miller und William Sharpe 1990, Robert Merton und Myron Scholes 1997\footnote{Fischer Black war bereits zuvor verstorben} und schließlich Eugene Fama 2013. Dies ist doch einigermaßen überraschend, da die Finanzmarkttheorie ja nur ein kleiner Teilbereich der Ökonomie ist.





