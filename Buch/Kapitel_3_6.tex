%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Neoklassische Finanzierungstheorie}
\label{Finance}



Bis heute hat die Neoklassische Finanzierungstheorie (Neoklassische Finance, Modern Finance, oder schlicht Finance) eine interessante Entwicklung durchgemacht. Der Forschungsaufwand war in dieser Disziplin stärker in fast allen anderen ökonomischen Feldern, vor allem auch in privatwirtschaftlichen Institutionen wird hier viel Forschung betrieben. Dies ist wenig überraschend, schließlich erhoffen sich viele bis heute durch Finanzanlagen reich zu werden. Dies ist insofern interessant, als die - bis heute gültige - grundlegende Annahme davon ausgeht, dass man zukünftige Kursentwicklungen von Assets in keiner Weise vorhersehen kann, sich diese stattdessen entlang eines reinen Zufallspfades entwickeln. In der Praxis ist das Angebot an Finanzmarktprodukten zweigeteilt. Zum einen gibt es quantitativ-wissenschaftlich geführte Fonds. Aber daneben gibt es noch immer einen erheblichen Zulauf zu "`Gurus"' oder Anlageberatern, die überzeugt davon sind, den Markt schlagen zu können. Die neoklassische Finance ist und war aber auch von wissenschaftlicher Seite regelmäßig vehementer Kritik ausgesetzt. Auch dies ist nicht überraschend: Schließlich hat die wissenschaftliche Weiterentwicklung in diesem Gebiet in keinster Weise dazu beigetragen, die Anzahl von Kursstürzen an den Börsen zu verringern. Weder der "`Schwarze Montag"' im Jahr 1987, noch die "`Dot-Com-Blase"', die im Jahr 2000 platzte, noch der Börsencrash zu Beginn der "`Great Depression"' 2007 passen so recht in das Konzept der "`Effizienten Finanzmärkte"'. Nicht zuletzt deshalb haben sich mit der Behavioral Finance (vgl. Kapitel \ref{Behavioral}) und innerhalb des Post-Keynesianismus (vgl. Kapitel \ref{Post-Keynes}) starke heterodoxe Ansichten zur Finanzierungstheorie gebildet, deren Bedeutung bis heute nicht zu unterschätzen ist. Die "`Modern Finance"' ist tatsächlich von vielen Seiten her angreifbar und - wie jede Theorie - weit weg davon die Realität vollständig abbilden zu können. Ihr wesentlicher Vorteil liegt aber darin die Konzepte von Nutzen, Ertrag und Risiko unter recht plausiblen Annahmen in mathematisch extrem eleganter Form miteinander zu vereinen. 


\section{Vorläufer der Finanzierungstheorie}
\label{FisherundKnight}

Die erste theoretische Rechtfertigung des Zinses, die jener der Gegenwart entspricht, wird dem Vertreter der Österreichischen Schule (vgl. Kapitel \ref{Austria}) Eugen von Böhm-Bawerk zugeschrieben. So wie die meisten Ökonomen zur damaligen Zeit, versuchte er eine "`Gesamterklärung"' der Ökonomie zu liefern, was in seinem zweibändigen Hauptwerk "`Kapital und Zins"' mündete. Von Bedeutung bis in die Gegenwart ist heute vor allem seine frühe Theorie des Zinses (Agiotheorie). Er schreibt: "`Gegenwärtige Güter sind in der Regel mehr wert, als künftige Güter gleicher Art und Zahl"' \parencite[S. 248]{BohmBawerk1888}. Dieser Satz ist insofern bemerkenswert, da er heute noch im ersten Kapitel eines modernen Finanzierungsbuchs sinngemäß identisch abgedruckt ist. Die erste Lektion lautet dort eben, dass Zahlungen, die zu unterschiedlichen Zeitpunkten erfolgen nur dann miteinander verglichen werden können, wenn sie vorher auf ihren Barwert \textit{abgezinst} werden. Interessant ist auch, dass diese Grundaussage von  \textcite{BohmBawerk1888} bis heute Gültigkeit hat, seine drei angeführten Gründe dafür heute als falsch angesehen werden \parencite[S. 316]{Rosner2012}. \textcite[S. 258ff]{BohmBawerk1888} diskutierte zwar bereits den Einfluss von Unsicherheit und Risiko (Wahrscheinlichkeit der Rückzahlung), fügte aber hinzu, dass diese Risikoprämie nichts mit dem Zinssatz zu tun hat \parencite[S. 261]{BohmBawerk1888}. Dennoch lieferte er eben als erster die bis heute gültige Theorie, dass Einkommen aus Kapital darauf zurückgeht, dass man gegenwärtigen Konsum durch Sparen in zukünftigen Konsum tauscht. Eine allgemeine Anmerkung, die an dieser Stelle besonders gut passt: Wirtschaftswissenschaftliche Arbeiten um 1900 sehen grundsätzlich anders aus, als heutige Arbeiten. Sucht man heute in verschiedenen Quellen nach Eugen von Böhm-Bawerk, so bekommt man den Eindruck er hätte primär an einem sehr eingeschränkten Bereich der Kapitalmarkttheorie gearbeitet. Aber das stimmt so nicht. Die beiden Bände von "`Kapital und Zins"' umfassen jeweils um die 500 Seiten. So wie alle Ökonomen dieser Zeit, versuchte auch Böhm-Bawerk ein "`allumfassendes"' Werk zu schaffen. Als bahnbrechende Leistung hat es natürlich nur jeweils ein Bruchteil dieser Arbeiten ins Bewusstsein der Gegenwart geschafft. Aus heutiger Sicht auch nur mehr schwer nachvollziehbar, wenn auch eigentlich nicht sehr überraschend: Die meisten Arbeiten zielten darauf ab die Arbeiten von Karl Marx und dessen kommunistischen Theorien zu widerlegen. 

Der US-amerikanische Ökonom Irving Fisher - der ja bereits aus dem Kapitel \ref{Neoklassik} bekannt ist - schuf auf mehreren Ebenen die Grundlage für die moderne Kapitalmarkttheorie. Zunächst nahm er die Zinstheorie von Böhm-Bawerk auf und formalisierte sie zur Theorie der \textsc{Intertemporalen Konsumentscheidung}, die bis heute in der Mikroökonomie "`State-of-the-Art"' ist - jeder Student kennt das entsprechende "`Fisher-Diagramm"'. Demnach maximiert ein rationales Individuum seinen Nutzen aus aktuellem Konsum und zukünftigem Konsum in Abhängigkeit vom aktuellen Einkommen, zukünftigem Einkommen und dem Zinssatz. Je niedriger der Zinssatz, desto höher ist die Attraktivität des aktuellen Konsums \parencite{Fisher1930}. Daraus lässt sich herleiten, dass Zinssenkungen zu höherem Konsum führen und Zinserhöhungen den Konsum drosseln. Aus dieser Theorie der Zeitpräferenz lässt sich auf die Höhe des risikolosen Marktzinssatzes schließen: Angenommen ich schulde ihnen 100 Euro. Wenn sie frei wählen können ob sie diese 100 Euro heute oder in einem Jahr bekommen wollen, dann wird jeder die sofortige Übergabe bevorzugen. Wenn ich aber das Angebot ändere auf: "`Heute 100 Euro oder aber in einem Jahr 100+X Euro"', wird jeder einen Wert x finden bei dem die spätere Zahlung bevorzugt (oder eigentlich als gleichwertig einschätzt). Das x ist nichts anderes als der Zinssatz. Die Person, die sich mit dem niedrigsten x zufriedengibt, wird von mir diesen Kreditauftrag bekommen. In einer seiner ersten wissenschaftlich bedeutenden Arbeiten formalisierte Fisher explizit den Zusammenhang, der als "`Fisher-Gleichung"' bekannt wurde: Realzinssatz entspricht Nominalzinssatz minus Inflation. Außerdem beschrieb er darin den Unterschied zwischen diskreter und stetiger Verzinsung\parencite[S. 191ff]{Fisher1906}. Beides mag zwar heute banal klingen, man darf aber nicht vergessen, dass diese Konzept bis heute unverändert Bedeutung haben. Bedeutend für die Entwicklung einer alleinstehenden Kapitalmarkttheorie ist zudem das "`Fisher-Separationstheorem"', mit diesem legt \textcite[S. 125f]{Fisher1930} dar, dass rationale Indiviuden auf einem vollkommenen Kapitalmarkt ihre Investitions- und Finanzierungsentscheidungen völlig unabhängig voneinander treffen können. Investitionen werden demnach rein nach ihrem Kapitalwert bewertet, der wiederum vom Zinssatz abhängig ist. Ist der Kapitalwert positiv soll das Projekt auf jeden Fall realisiert werden. Erst danach - praktisch auf der nächsten Entscheidungsebene, daher Separation - wird entschieden \textit{wie} die Investition finanziert wird. Auf dieser Grundlage macht es Sinn den Kapitalmarkt als von den Realmärkten völlig unabhängigen Markt zu betrachten. Bisher - sowohl von Böhm-Bawerk, als auch von Fisher - außen vor gelassen wurden aber alle Überlegungen zum Zusammenhang zwischen Rendite (Zins) und Risiko.

Frank Knight wird häufig, gemeinsam mit Jacob Viner, als der Begründer der "`Chicago School"' bezeichnet. Tatsächlich wurde vor allem der makroökonomischen Zweig der Chicago School um Milton Friedman und dessen Monetarismus, sowie später die Neue Klassische Makroökonomie um Robert Lucas weltbekannt. Aber auch die mikroökonomische, neoklassische Finanzierungstheorie wurde zu einem erheblichen Teil in Chicago entwickelt. Aus wissenschaftlicher Sicht spielt Knight vor allem aufgrund seiner Unterscheidung zwischen fundamentaler Unsicherheit und Risiko eine Rolle \parencite{Knight1921}. Bei erstgenannter können keinerlei Informationen über die Eintrittswahrscheinlichkeiten zukünftiger Zahlungsströme gemacht werden. Bei Risiko hingegen kann man zukünftige Zahlungsströme mit Eintrittswahrscheinlichkeiten - meist in Form von Verteilungen - versehen. Damit kann man Erwartungswerte und Standardabweichungen berechnen. Dieses Konzept bildete später die Grundlage für die Entwicklung der Entscheidungstheorie.

Als Vorläufer der Finanzmathematik gilt heute \textcite{Bachelier1900}. Er ist de facto der Begründer der Random-Walk Theorie und verwendete als Erster stochastische Prozesse zur Darstellung von Aktienrenditen. Konkret ging schon Bachelier davon aus, dass stetige Renditen durch eine normalverteilte Zufallsvariable dargestellt werden können. Dies begründete auch die Wichtigkeit der Normalverteilung von Renditen in der Modern Finance. Anleger machen demnach die Entscheidung über ihr Investment ausschließlich von erwarteter Rendite und dem Risiko einer logarithmisch-normalverteilten Zufallsvariable abhängig. Diese Annahme erwies sich als extrem nützlich. Die Normalverteilung ist nämlich durch die ersten beiden Momente vollständig beschrieben. Das erste Moment, der Mittelwert $(\mu)$, kann dabei in der Finance als Erwartungswert der Renditen interpretiert werden. Das zweite Moment, die Standardabweichung $(\sigma)$, wird als Maß für das Risiko herangezogen. Das dritte Moment, die Schiefe der Verteilung, kann bei Normalverteilungen vernachlässigt werden, da diese symmetrisch sind und der Wert daher immer 0 ist. Für die Finance bedeutet dies, dass Gewinne und Verluste immer gleich wahrscheinlich sind, was perfekt zur Annahme effizienter Märkte passt. Das vierte Moment der Normalverteilungen, die Wölbung, nimmt immer den Wert 3 an. Dies ist der Pferdefuß der Normalverteilungsannahmen. Man weiß nämlich mittlerweile aus empirischen Untersuchungen, dass Aktienrenditen eine stärkere Wölbung aufweisen, also "`leptokurtisch"' sind. Die Normalverteilungsannahme ist daher nur näherungsweise erfüllt. Insbesondere der Mathematiker Mandelbrot verfasst dazu schon früh Arbeiten \parencite{Mandelbrot1963}. Er wendete das von ihm geprägte Konzept der Fraktalen Geometrie auch auf den Finanzmarkt an und schlug statt der Normalverteilung eine Verteilung mit nicht-endlicher Varianz, zum Beispiel eine Levy-Verteilung vor. Tatsächlich ist das zentrale Problem der Normalverteilungsannahme, dass in empirischen Untersuchungen fast immer zu "`schwere Ränder"', also zu viele Extremwerte, beobachtet werden. So zeigten \textcite{Dowd2008}, dass Tagesrenditen, die außerhalb von drei Standardabweichungen einer log-Normalverteilung liegen, nur einmal in drei Jahren vorkommen dürften. In der Realität beobachtete man aber gerade zu Beginn von großen Finanzkrisen vereinzelt Abweichungen, die außerhalb von sechst Standardabweichungen lagen, sogenannte "`Six-Sigma-Events"'. Diese dürften laut \textcite{Dowd2008} nur einmal in vier Millionen Jahren vorkommen. Trotz aller Kritik überwiegen für die meisten Ökonomen die Vorteile der Annahme log-normalverteilter Renditen, da sie - wie gleich dargestellt wird - eine Voraussetzung für die Anwendbarkeit bestimmter Modelle ist.

\section{Erwartungsnutzen und Pratt's Risikoaversion}
\label{Erwartungsnutzen}
Die \textsc{Erwartungsnutzentheorie} und das Konzept der Risikoaversion sind zentrale Voraussetzung für die neoklassische Finanzierungstheorie. Die Erwartungsnutzentheorie als Teil der Entscheidungstheorie ist eng verbunden mit dem Konzept des \textsc{Homo \oe conomicus}. Der "`rational handelnde Mensch"' ist eigentlich schon eine implizite Voraussetzung in der klassischen Ökonomie, auf jeden Fall aber in der Neoklassik. Auch in der modernen Makroökonomie gibt es das Konzept, wird dort aber "`repräsentativer Agent"' genannt. Für die Finanzierungstheorie von entscheidender Bedeutung ist die Erwartungsnutzentheorie für den Spezialfall der Entscheidungen unter Risiko. Das heißt, ein Zahlungsstrom in der Zukunft kann nicht mit Sicherheit vorhergesagt werden, sondern es gibt immer ein gewisses Risiko, ob überhaupt und wenn ja in welcher Höhe die Zahlung erfolgt. Ökonomen sprechen bei solchen unsicheren Zahlungsströmen häufig von Lotterie. Wir haben bereits im letzten Unterkapitel die "`Einteilung"' von Risiko nach Frank Knight kennen gelernt. Da man für Zahlungsströme unter "`fundamentaler Unsicherheit"' wenig Aussagen treffen kann, geht man davon aus, dass zumindest eine Risikoverteilung zu zukünftigen Zahlungsströmen angegeben werden kann. Dadurch können wir eine bestimmte Wahrscheinlichkeit angeben, in welcher Höhe die zukünftigen Zahlungsströme erfolgen. Häufig bedient man sich, der Einfachheit halber, diskreter Wahrscheinlichkeiten. Das heißt man zum Beispiel an, dass ein Kreditnehmer seine Schulden mit 80\% Wahrscheinlichkeit vollständig bezahlen kann und mit 20\% Wahrscheinlichkeit gar nicht. Vom Konzept her gleich, aber mathematisch etwas herausfordernder, ist die Annahme stetiger Wahrscheinlichkeitsverteilungen. Ebenfalls im letzten Unterkapitel haben wir die Arbeit von Louis Bachelier beleuchtet, der bereits um die Jahrhundertwende davon ausging, dass Aktienrenditen sich nach einem Zufallsprozess entwickeln. Die Vorteile der Annahme logarithmisch-normalverteilter Renditen haben wir oben schon beschrieben. 

Aus diesem Konzept, dass wir zukünftige Zahlungsströme als Wahrscheinlichkeitsverteilung betrachten können, lässt sich die Erwartungsnutzentheorie ableiten. Der Beginn unserer Überlegungen führt uns dabei zurück ins 18. Jahrhundert. Der Schweizer Mathematiker Daniel Bernoulli veröffentlichte 1738, sinngemäß, folgendes Gedankenexperiment: Es wird das Spiel "`Kopf oder Zahl?"' gespielt. Bei "`Kopf"' erhalten Sie 2 Euro und das Spiel geht weiter, wobei jede Runde ihr Gewinn verdoppelt wird. Nach der zweiten Runde Kopf steht ihr Gewinn also bei 4 Euro, nach der dritten bei 8 Euro und so weiter. Erscheint aber das erste mal "`Zahl"' ist das Spiel vorbei. Die Frage, die sich \textcite{Bernoulli1738} stellt, lautet, wie hoch der faire Preis für so eine Lotterie nun wäre? Als intuitive Entscheidungsregel denkt man grundsätzlich einmal an den Erwartungswert. Dazu würde man den Durchschnitt der Lotterie-Ergebnisse berechnen. Wenn also \textit{einmal} eine Münze geworfen wird und bei "`Kopf"' 2 Euro, bei "`Zahl"' hingegen nichts bezahlt wird, wäre der faire Preis 1 Euro. Wendet man diese Formel allerdings beim oben genannten Gedankenexperiment an, erhält man als Ergebnis unendlich! Sie haben zwar jede Runde eine Gewinnchance von nur 50\% aber dafür wird auch jede Runde der Gewinn verdoppelt. Das Spiel wurde als "`St. Petersburg Paradoxon"' bekannt und gilt vor allem unter angehenden Roulette-Spielern als \textit{die} Gewinnstrategie\footnote{Das ist natürlich nur scherzhaft gemeint. Als (angehender) Ökonom wissen Sie: Würde das Konzept funktionieren, wären Roulette-Tische schon ausgestorben. Tatsächlich ist das Wesentliche an der Strategie übrigens das ständigen verdoppeln der Einsätze, die Wahl der Farbe ist hingegen natürlich völlig unbedeutend. Dadurch erreichen Sie an Roulette-Tischen auch recht rasch das Tischlimit}. Die erste ökonomische Grundaussage von \textcite{Bernoulli1738} lautet damit, dass der Erwartungswert als Entscheidungsmodell unzureichend ist. Das war damals eine neue Erkenntnis. Die berühmten Begründer der Wahrscheinlichkeitstheorie - Pascal und Fermat - waren nämlich noch vom Erwartungswert als ausreichendes Entscheidungsprinzip ausgegangen. \textcite{Bernoulli1738} lieferte aber noch eine weitere Erkenntnis, die ebenfalls mit einem einfachen Münzwurfbeispiel erklärt werden kann. Versetzen Sie sich in das oben beschriebene, \textit{einmalige} "`Kopf-oder-Zahl"' Spiel. Das genannte Spiel um zwei Euro, bei 1 Euro Einsatz würden wir vielleicht ohne groß zu überlegen spielen, einfach weil es Spaß macht, oder um einem Kind  Freude zu bereiten. Würden Sie die Lotterie aber auch dann eingehen, wenn Gewinnchance und Einsatz jeweils verzehnfacht würden? Oder wenn es gar um einen Gewinn von 200.000 Euro ginge, bei einem Einsatz von 100.000 Euro? Zumindest im letzten Fall würde wohl jeder ablehnen. Beachten Sie aber, dass nach dem Prinzip des Erwartungswertes das Spiel um 200.000 Euro ebenso fair bewertet wird, wie das Spiel um 2 Euro. Wahrscheinlich würden die meisten von uns die letzte Wette aber sogar dann ablehnen, wenn der Einsatz von 100.000 Euro auf 90.000 Euro gesenkt würde. \textcite{Bernoulli1738} schloss daraus, dass Menschen offenbar neben dem Erwartungswert und auch das entstehende Risiko bei ihren Entscheidungen berücksichtigen und vor allem mit zunehmenden Einsatz immer vorsichtiger werden. Während bei Risikoneutralität der Erwartungswert als Entscheidungskriterium ausreicht, maximieren Menschen in der Realität ihren \textit{Nutzen}. Diese Erkenntnis ist bahnbrechend, aber es entsteht daraus auch ein Problem. Den Erwartungswert kann man leicht in Geldeinheiten ausdrücken. Für den Nutzen hingegen gibt es kein Maß. Mehr noch: Der Nutzen einer bestimmten Lotterie ist für jeden Menschen unterschiedlich. Zwar ist jeder Risiko grundsätzlich negativ eingestellt - Ökonomen bezeichnen dies als "`Risikoaversion"' - das Maß dieser Risikoscheu ist kaum quantifizierbar und von Individuum zu Individuum unterschiedlich. Es gibt also keine einheitliche Formel wie man Wohlstand in Nutzen umrechnen kann. 

Dieses Problem wurde erst im 20. Jahrhundert gelöst. Die Arbeit von \textcite{Bernoulli1738} war in Latein verfasst und seine Erkenntnis zwischendurch praktisch verlorengegangen. Erst Anfang des 20. Jahrhunderts wurde sie wiederentdeckt und schließlich 1954, in Englisch übersetzt, veröffentlicht. Eine wesentliche Weiterentwicklung stellte aber \textcite{Morgenstern1944} dar. Das Monumentalwerk revolutionierte nicht nur die Erwartungsnutzentheorie - tatsächlich werden Nutzenfunktionen bis heute meist "`Von Neumann-Morgenstern-Nutzenfunktionen"' genannt - es bildete auch die Grundlage der Spieltheorie (vgl. Kapitel \ref{Spieltheorie}). Das Buch wurde bereits 1944 erstmals veröffentlicht, wird aber meist in der Ausgabe von 1953 zitiert. Laut \textcite[S. 235]{Bernstein1996} war der Papiermangel im Zweiten Weltkrieg dafür verantwortlich, dass das Buch zwar 1944 erschien, aber erst 1953 in größerer Anzahl gedruckt wurde. Das Erwartungsnutzenkonzept, wie es bis heute Bestand hat, wurde dabei im kurzen Kapitel 3 (Seiten 15-29) des 640 Seiten-Wälzers entwickelt. Die Grundaussage dabei lautet, dass Nutzen nicht kardinal gemessen werden kann sondern nur ordinal. Das heißt, Nutzen kann weder in einer messbaren Einheit angegeben werden, noch kann ihm ein bestimmter Zahlenwert sinnvoll zugewiesen werden. Aber auch mit dem ordinalem Nutzenkonzept kann man quantitative Aussagen machen, solange bestimmte Voraussetzungen erfüllt sind. Diese wurden in einem Axiomensystem von \textcite[S. 26f]{Morgenstern1944} beschrieben. So müssen Entscheidungsträger alle verfügbaren Alternativen in eine Präferenz-Reihenfolge bringen können (Vollständige Ordnung) und Kombinationen aus verschiedenen Alternativen bilden können, die den gleichen Nutzen liefern wie andere Kombinationen (Stetigkeit). Schließlich müssen die Alternativen Unabhängigkeit aufweisen: Die Kombination schlechter individueller Alternativen darf nicht besser bewertet werden als die Kombination guter individueller Alternativen. Die Erwartungsnutzentheorie ist ebenso mächtig wie umstritten. \textcite{Allais1953} veröffentlichte eine Kritik, die im Journal Econometrica in französischer Sprache, nur mit einer englischen Zusammenfassung, abgedruckt wurde. Man konnte in empirischen Experimenten rasch zeigen, dass die Axiome aus \textcite{Morgenstern1944} nicht immer aufrecht zu erhalten sind. Diese Kritik an der Erwartungsnutzentheorie von \textcite{Allais1953} gilt heute als Grundlage der Behavioral Economics (vgl. Kapitel \ref{Behavioral}). Der Vorteil der Erwartungsnutzentheorie ist, dass durch bestimmte Transformationen aus dem Erwartungswert sehr einfach ein Erwartungsnutzen berechnet werden kann, der allen formalen Anforderungen laut \textcite{Morgenstern1944} erfüllt. Konkret eignen sich dazu Transformationen, die aus der linearen Erwartungswertfunktion (der Erwartungswert steigt im Gleichen Verhältnis wie die Lotterieauszahlungen) eine konkave Erwartungsnutzenfunktion machen (der Erwartungsnutzen steigt weniger stark an als die Lotterieauszahlungen). Diese Funktion mit ständiger kleiner werdender Steigung bildet die Tatsache ab, dass mit höheren Lotteriewerten die Risikoaversion steigt. Konkrete Nutzenfunktionen sind häufig Wurzelfunktionen, oder Logarithmusfunktionen. Zum Abschluss noch eine interessante Querverbindung: Die Risikoaversion führt also zu einer Nutzenfunktion, die zwar ständig ansteigt, aber deren Steigung immer geringer wird. Eine ähnliche Beobachtung haben wir bereits im Kapitel \ref{Neoklassik} gemacht. Der Vorläufer der Neoklassik Hermann Heinrich Gossen postulierte als erster den abnehmenden Grenznutzen: Je mehr Güter ich besitze, desto geringer ist der Nutzenzuwachs durch ein zusätzliches Gut - in Kapitel \ref{Neoklassik} nannten wir dies das Prinzip des abnehmenden Grenznutzens. Und tatsächlich stammt die Risikoaversion aus diesem Prinzip. Risikoaversion und Abnehmender Grenznutzen sind identisch, nur aus anderen Blickwinkeln betrachtet! Wenn ich 100.000 Euro besitze und 90.000 davon drohe ich durch ein einzelnes Ereignis zu verlieren, dann werde ich tunlichst versuchen diesen Verlust zu vermeiden. Wenn ich 100 Mio. Euro besitze, werden mir die 90.000 Euro eher egal sein - Mit höheren Vermögen nimmt die Risikoaversion ab. Wenn ich 100.000 Euro besitze und 90.000 gewinne, werde ich mich sprichwörtlich "`freuen wie ein Schneekönig"'. Wenn ich 100 Mio. Euro besitze und 90.000 Euro gewinne, wird meine Freude nicht ganz so euphorisch sein - Mit höherem Vermögen nimmt der Grenznutzen ab.

Unabhängig voneinander erweiterten drei Autoren die Anwendung von Erwartungsnutzenfunktionen. \textcite{DeFinetti1952}, \textcite{Arrow1963b} und \textcite{Pratt1964} entwickelten Ein Maß mit dem die absolute Risikoaversion (ARA) quantifiziert werden kann. Entscheidend hierbei ist, dass dieses sogenannte "`Arrow-Pratt-Maß"' direkt aus Erwartungsnutzenfunktionen abgeleitet wird und die Varianz die Höhe des Risikos darstellt. Die Kennzahl für die absolute Risikoaversion erlaubt uns den Wert von Zahlungen, die nur mit einer bestimmten Wahrscheinlichkeit eintreffen um die Risikoeinstellung zu "`bereinigen"'. Wie wir wissen hat jeder Mensch eine individuelle Risikoaversion und damit eine individuelle Nutzenfunktion. Wenn wir aber die Nutzenfunktion einer bestimmten Person kennen, können wir daraus ein Maß, also eine Zahl, für die absolute Risikoaversion ableiten. Mit Hilfe dieses Maßes können wir wiederum berechnen, welcher sichere Geldbetrag für diese Person gleich viel wert ist wie eine bestimmte Lotterie. Ökonomen nennen diesen Betrag "`Sicherheitsäquivalent"'. Wie wir oben bereits beschrieben haben, kennen wir von einer Lotterie die möglichen Zahlungen und die dazu gehörigen Wahrscheinlichkeiten, mit denen diese Zahlungen realisiert werden. Daraus lässt sich der Erwartungswert der Zahlungen $(\mu)$, sowie die durchschnittliche Abweichung von diesen Zahlungen berechnen, was wir als Risikomaß heranziehen werden $(\sigma)$. Das Sicherheitsäquivalent $(\phi)$ ergibt sich nun - leicht vereinfacht - aus dem Erwartungswert der Lotterien abzüglich des Risikos der Lotterien, gewichtet mit der individuellen absoluten Risikoaversion\footnote{Die tatsächliche Formel lautet $\phi = \mu - \frac{1}{2}*ARA*\sigma^2$.}. Man nennt dieses Konzept den "`Mean-Variance-Ansatz"', beziehungsweise das $\mu-\sigma$-Prinzip. Dieses hat eine große Bedeutung in der gesamten Neoklassischen Finanzierungstheorie. Wie wir im Kapitel \ref{Portfolio} in Kürze sehen werden, baut auch die Bewertung von Portfolios auf den gleichen Kennzahlen, also Erwartungswert und Standardabweichung (=Wurzel der Varianz) auf. Das Konzept der Erwartungsnutzentheorie ist daher vollständig integrierbar in die Modelle der Modern Finance, insbesondere die Portfoliotheorie und das CAPM. Dies gilt aber nur dann, wenn tatsächlich nur die ersten beiden Momente, also Erwartungswert (Mittelwert) und Standardabweichung relevant, der Verteilung der Lotterie eine Rolle spielen. Dies ist zum Beispiel dann der Fall, wenn die einzelnen Ausprägungen der Lotterie durch eine Normalverteilung abbildbar sind. Genau deshalb hält man in der Finanzierungstheorie an der umstrittenen Normalverteilungsannahme fest. Nur unter dieser Prämisse greifen die Konzepte der Erwartungsnutzentheorie und der Modern Finance so schön und mathematisch elegant ineinander.


\section{Die Relevanz der Irrelevanz}
\label{Struktur}
Die Arbeit von \textcite{Modigliani1958} zur Kapitalstrukturtheorie gilt bis heute als \textit{die} Grundlage der wissenschaftlichen "`Corporate Finance"'. Bis zu diesem Zeitpunkt wurde die Finanzierung von Unternehmen als reiner Teil der Betriebswirtschaft gesehen. Unternehmen wurden ausschließlich individuell betrachtet. Franco Modigliani und Merton Miller revolutionierten diese Ansicht, indem sie sich erstmals wissenschaftlich der Kapitalstruktur von Unternehmen annäherten. Sie gingen von einem perfekten Kapitalmarkt aus, auf dem es keine Steuern und Transaktionskosten gibt und sich Unternehmen zu einem konstanten Fremdkapitalzinssatz finanzieren können. Der Wert eines Unternehmens bestimmt sich ausschließlich aus den zukünftigen, abgezinsten Cashflows. Und ausgehend von diesen Rahmenbedingungen, zeigten \textcite{Modigliani1958} mit einem einfachen Arbitrage-Argument, dass die Zusammensetzung der Finanzierung eines Unternehmens, also der Anteil von Eigenkapital und Fremdkapital, keinerlei Auswirkungen auf den Unternehmenswert hat. Das Konzept wurde als "`Irrelevanztheorie"' bekannt und klingt nicht besonders spannend. Damit sind aber interessante Punkte verbunden. Zunächst war der Ansatz, mittels Arbitrageargument einen wissenschaftlichen Beweis zu führen, richtungsweisend. Der "`Law of one Price"'-Ansatz blieb in der Finanzierungstheorie bis heute das wichtigste Konzept und bildet auch die Grundlage für die Effizienzmarkthypothese (vgl. Kapitel \ref{Efficient}). Inhaltlich revolutionär war aber die Erkenntnis, dass der Wert eines Unternehmens gänzlich von seiner Kapitalstruktur unabhängig ist. Das Arbitrageargument lautet hierfür wie folgt: Man stelle sich zwei identische Unternehmen vor, die sich ausschließlich darin unterscheiden, dass Unternehmen A aus zwei Investoren besteht, die beide das Eigenkapital des Unternehmens stellen, während Unternehmen B aus einem Eigenkapitalgeber und einem Fremdkapitalgeber besteht. Von den Gewinnen muss Unternehmer B zunächst einmal die Fremdkapitalzinsen bezahlen. Der Rest des Gewinns gehört aber dem Eigenkapitalgeber. Bei Unternehmen A fließen die kompletten Gewinne den Eigenkapitalgebern zu, diese müssen sich die Gewinne aber teilen. Die beiden Unternehmer B verdienen also weniger als der eine Eigenkapitalgeber in A. Es scheint so als wäre Unternehmen A dann auch wertvoller, schließlich wirft es mehr Gewinn für seine Eigentümer ab. Das stimmt aber nicht. Man darf nämlich nicht vergessen, dass im Verlustfall der eine Eigentümer A den gesamten Verlust tragen muss, während die beiden B-Investoren sich auch die Verluste aufteilen. Die höheren Ertragschancen in Unternehmen A werden durch das höhere Risiko genau ausgeglichen. Daraus resultiert der berühmte Hebeleffekt, auch im deutschsprachigen Raum meist einfach "`Leverage"' genannt: Durch die Aufnahme von Fremdkapital kann ich die erwartete Rendite erhöhen, allerdings steigt damit auch das Risiko des Investments. Das lässt sich wiederum erweitern auf die Kapitalkosten. Die berühmte WACC-Formel - bis heute Teil jeder einführenden Finanzierungs-, Controlling- und Finanzierungsvorlesung - geht direkt auf \textcite{Modigliani1958} zurück: Die durchschnittlichen, gewichteten Kapitalkosten eines Unternehmens bleiben, unabhängig von der Kapitalstruktur, konstant.

Die Arbeit von \textcite{Modigliani1958} war zwar rasch wissenschaftlich etabliert, allerdings war sie auch der Kritik ausgesetzt, dass die starken Annahmen eines perfekten Kapitalmarktes in der Realität nie anzutreffen ist. Insbesondere die Annahme, dass es keine Steuern gäbe, ist schlicht falsch. Aus diesem Grund wurde mit \textcite{Modigliani1963} eine Erweiterung des Modells veröffentlicht. Fremdkapital ist steuerlich begünstigt, da die Fremdkapitalzinsen als Aufwand den Gewinn und damit die Steuerlast des Unternehmens verringern. Abhängig von der Höhe des Steuersatzes lässt sich somit die Kapitalstruktur optimieren.

\section{Fama: Nichts arbeitet so effizient wie der Markt}
\label{Efficient}

Die Theorie der Effizienten Märkte ist eine Grundlage der anderen Konzepte der modernen Finanzwirtschaft. Markteffizienz bedeutet, dass der Markt sämtliche relevante Informationen zur Verfügung hat und die Kräfte aus Angebot und Nachfrage zu jedem Zeitpunkt dafür sorgen, dass der Preis in seinem wahrem Gleichgewicht liegt. Es gibt ein einfaches Argument dafür, dass diese Theorie grundsätzlich Sinn macht. Stellen Sie sich vor Sie hätten eine zuverlässige Information darüber, dass ein bestimmtes Asset morgen um 10\% an Wert gewinnt. Es wäre nur rational dieses Asset heute zu kaufen, also nachzufragen, um den freien Gewinn - den "`Free Lunch"' - mitzunehmen. Die dadurch generierte Nachfrage würden den Preis aber ansteigen lassen. Das Asset würde also nicht erst morgen um 10\% steigen, sondern durch die erhöhte Nachfrage heute schon. Denkt man dieses Konzept konsequent durch, bleibt nur die Lösung, dass die Märkte ständig die wahren Preise abbilden. Also effizient sind, es gibt keinen "`Free Lunch"'. Dennoch ist keine Theorie innerhalb der Modern Finance so umstritten wie jene der Effizienten Märkte. Dafür gibt es vor allem empirische Gründe. Die großen Kursschwankungen, vor allem aber rasche Einbrüche an den Finanzmärkten, wie jene am "`Schwarzen Donnerstag"' 1929, oder am "`Schwarzen Montag"' im Jahr 1987, lassen sich kaum mit der Idee der Markteffizienz vereinbaren. Zur Kritik später mehr.

Die Effizienzmarkttheorie ist auch relativ schwer eindeutig zuzuordnen. Die Idee wird oft in Verbindung mit der "`Random-Walk-Theorie"' gesehen, die - wie oben beschrieben - ursprünglich auf \textcite{Bachelier1900} zurückgeht. Demnach repräsentieren Aktienkurse immer effiziente Märkte und zukünftige Kursschwankungen erfolgen rein zufällig\footnote{Im Umkehrschluss muss aber eben nicht gelten, dass Kursschwankungen, die einem zufälligen Pfad folgen auch tatsächlich effizient sind.}. Unumstritten ist auf jeden Fall, dass \textcite{Fama1970} die Effizienzmarkthypothese als erster operationalisierte und auch ausführliche empirische Untersuchungen dazu veröffentlichte. Eugene Fama unterscheidet zwischen schwacher, mittelstarker und starker Informationseffizienz. Die schwache Effizienz sagt aus, dass die aktuellen Kurse alle historischen Kursinformationen schon berücksichtigt hat, die mittelstarke Effizienz umfasst zusätzlich alle öffentlich zugänglichen Informationen, wie Bilanzen, Veröffentlichungen oder Presseberichte. Für die Gültigkeit dieser beiden Ausprägungen der Effizienzmarkthypothese fand \textcite{Fama1970} recht starke empirische Argumente. Damit ist übrigens verbunden, dass so weitverbreitete Methoden wie die technische Analyse, oder die Fundamentalanalyse keinerlei Informationsgehalt haben. Bei Vorliegen der starken Effizienz würden selbst Insider-Informationen schon im Preis enthalten sein, dafür fand aber selbst \textcite{Fama1970} keine überzeugenden empirischen Anhaltspunkte.
Kritik an der Theorie fand sich schon bald nach deren Publikation. Einen interessanten theoretischen Kritikpunkt veröffentlichten \textcite{Grossman1980}. Sie argumentierten, dass die Beschaffung von Marktinformationen Kosten verursacht. Im Zusammenhang mit dem Vorliegen der Effizienzmarkthypothese entsteht folgende paradoxe Situation. Wenn die Preise alle Informationen bereits enthalten und die Informationsbeschaffung Kosten verursacht, aber keinen Nutzen bringt, dann wird kein rationaler Marktteilnehmer den Aufwand der Informationsbeschaffung auf sich nehmen. Wenn aber niemand Informationen beschafft, dann können im Umkehrschluss die Preise nicht Informations-effizient sein. Schwankender Informationsbeschaffungsaufwand seitens der Marktteilnehmer könnte damit das Auftreten von Finanzblasen erklären. Über die Finanzwelt hinaus berühmt würde auch Robert Shiller, vor allem durch die Einführung des Case-Shiller-Index - einem Immobilienindex - und das populärwissenschaftliche Buch "`Irrational Exuberance"', das knapp vor dem Platzen der "`Dot-Com-Blase"' im Jahr 2000 veröffentlicht wurde. Seinen wissenschaftlichen Durchbruch schaffte Shiller bereits im Jahr 1981. Er argumentierte empirisch-statistisch begründet, dass Aktienkurse viel zu stark schwanken um mit der Effizienzmarkthypothese vereinbar zu sein \parencite{Shiller1981}. Punktuelle Angriffspunkte auf die Effizienzmarkthypothese lieferten seit den 1970er Jahren die Vertreter der Behavioral Finance, die aber in Kapitel \ref{Behavioral} näher behandelt wird.

Dass die Effizienzmarkthypothese selbst innerhalb der Wirtschaftswissenschaften umstritten ist, zeigt sich unter anderem auch an der doch recht amüsant anmutenden Tatsache, dass im Jahr 2013 mit Eugene Fama und Robert Shiller zwei Ökonomen mit dem Nobelpreis ausgezeichnet wurden, die geradezu gegenteilige Meinungen zur Effizienzmarkthypothese vertreten. Dennoch ist sie nach wie vor Bestandteil der Mainstream-Ökonomie. Man ist sich der Schwächen des Konzepts zwar durchaus bewusst, aber wie jedes Modell scheint es eine ausreichend gute Näherung an die Realität darzustellen. Vor allem aber ist es die Grundlage für die gesamte Neoklassische Finanzierungstheorie, die ein theoretisch gut fundiertes System darstellt, bei dem ein Rädchen wunderbar ins andere greift. Die Alternativen, wie die Behavioral Finance (vgl. Kapitel \ref{Behavioral}) oder die Finanzmarkt-Ansätze der Post-Keynesianer (vgl. Kapitel \ref{Post-Keynes}) konnten bislang zwar punktuell wichtige Kritikpunkte aufwerfen und Teillösungen anbieten, aber eben kein so elegantes Modell wie jenes der Neoklassischen Finance.


\section{Markowitz: Don't put all your eggs in one basket}
\label{Portfolio}

Chronologisch gesehen ist die wahrlich bahnbrechende Arbeit zur Portfoliotheorie von \textcite{Markowitz1952} die erste Arbeit der neoklassischen Finanzierungstheorie. Neben \textcite{Modigliani1958} zählt sie somit zu deren "`Gründungsarbeiten"'. Zur Entstehung des Artikels "`Portfolio Selection"', der im wesentlichen auch die Doktorarbeit von Harry Markowitz darstellt, gibt es unzählige Geschichten. So erzählt Markowitz in einem Interview\footnote{Die Stelle findet sich hier: https://www.youtube.com/watch?v=RVWEhCd819E, Minute 00:50. In diesem Interview findet sich auch die Anekdote mit der Defensio bei Milton Friedman und  Jacob Marschak.}, dass eine Börsenhändler ihm den Tipp gegeben hätten sich einem Finanzthema zu widmen. Eine Anekdote von der abschließenden Defensio seiner Doktorarbeit gab Markowitz im Rahmen seiner Nobelpreis-Lectures zum Besten: "`Professor Milton Friedman argumentierte [Anm.: Wahrscheinlich scherzhaft], dass Portfolio-Theorie kein Teil der Ökonomie sei und sie ihm daher keinen Doktortitel in Ökonomie für eine Dissertation [dafür] geben können."' \parencite[S. 286]{Markowitz1990}. Beide Geschichten machen deutlich wie bahnbrechend seine Arbeit im Jahr 1952 gewesen ist. Etwas das auch \textcite{Rubinstein2002} im Rahmen des 50-Jahr Jubiläums von "`Portfolio Selection"' hervorhob: "`Am beeindruckendsten an Markowitz' 1952 Artikel fand ich, dass er aus dem Nichts zu kommen scheint"'. Tatsächlich lautete, leicht übertrieben, die Prämisse auf den Finanzmärkten vor 1952: Suche die Aktie von der du dir die höchste Rendite erwartest und kaufe sie. Das Konzept der naiven Diversifikation ist schon seit jeher bekannt und auch das Konzept vom Risiko-Rendite-Trade-Off war nicht neu. Aber es gab keine quantitativ-mathematischen Ansätze zur Formalisierung dieser Konzepte. Dies ist einigermaßen überraschend. Wertpapierhandels gibt es schließlich schon seit Jahrhunderten.  Und - anders als in der Makroökonomie - haben selbst die "`Great Depression"', bzw. die Kursverluste in Folge des "`Schwarzen Donnerstags"' im Jahr 1929, keinen Durchbruch in diesem Bereich ausgelöst. Warum war die Arbeit nun so bahnbrechend? Nun, \textcite{Markowitz1952} war die erste rein \textit{mathematisch-quantitative} Arbeit im Bereich der Finanzmarktanalyse. Als solche wurden darin gleich drei wesentliche Konzepte etabliert, deren Bedeutung bis heute unumstritten ist: Erstens, die Varianz der Aktienrenditen wurde als Risikomaß. Zweitens, der Risiko-Rendite-Trade-Off - also die Annahme, dass höhere erwartete Rendite immer auch mit höherem Ausfallsrisiko verbunden ist, und drittens, Die Bedeutung der Korrelation von Aktienrenditen. Letzteres ist nichts anderes als die mathematische Fundierung der Diversifikation. Dieser letzte Punkt wird häufig als der wesentliche bezeichnet und tatsächlich basiert darauf die zentrale Idee des Artikels: Aktienportfolios aus der Kombination von Einzeltiteln zu bilden, die das optimale Verhältnis zwischen Rendite und Risiko abbilden. Was heißt das konkret? Wenn Sie eine Aktie kaufen so erwarten sie in Zukunft eine Rendite von x\%. Diese Erwartung bildet sich aus den vergangenen Renditen dieser Aktie. Da die Rendite bei Aktien aber nie konstant ist, sondern zufälligen vgl. Kapitel \ref{Efficient} - Schwankungen unterliegt, ist diese Rendite stets nur eine Erwartung. Aus den vergangenen Schwankungen lässt sich eine durchschnittliche Schwankung - die Standardabweichung - berechnen. Diese wird in weiterer Folge als Risikomaß herangezogen. Je stärker der Wert der Aktie schwankt, desto schwieriger ist deren zukünftiger Wert zu prognostizieren. Oder mit anderen Worten: Desto höher ist ihr Risiko. Stellen Sie sich nun \textit{zwei} Aktien vor. Für beide können sie einen Rendite-Erwartungswert, sowie eine Standardabweichung berechnen. Wenn Sie beide Aktien zu gleichen Teilen kaufen, so entspricht ihr Rendite-Erwartungswert dieses Portfolios dem Mittelwert der Renditen der beiden Aktien. Für die Berechnung der Standardabweichung stimmt dies aber \textit{nicht}! Die Renditen von Aktien verlaufen niemals genau gleich. Das heißt sie korrelieren niemals zu 100\%. Viele Aktien bewegen sich zwar tendenziell in die gleiche Richtung, aber manche Assets sind unabhängig von anderen, bzw. korrelieren sogar negativ. Das heißt bei der Berechnung des Risikos des Portfolios reicht es nicht aus einfach den Mittelwert der Risiko-Werte der Einzeltitel heranzuziehen. Stattdessen muss auch die Korrelation zwischen den beiden Titeln berücksichtigt werden. Wenn zwei Titel perfekt negativ miteinander korrelieren (was ebensowenig vorkommt wie perfekt positive Korrelation), dann steigt eine Aktie immer dann wenn die andere fällt. Dies ist gut für das Portfolio-Risiko: Da der Gewinn der einen Aktie den Verlust der zweiten Aktie immer zumindest teilweise ausgleicht. Die Standardabweichung des Portfolios ist daher stets geringer als der gewichtete Durchschnitt der Standardabweichung der einzelnen Aktien. \textcite{Markowitz1952} zeigte dies erstmals mathematisch. In der Folge kann man natürlich Portfolios aus vielen Einzeltitel zusammenstellen. Sogenannte "`Effiziente Portfolios"' sind aber nur solche, bei denen für eine gegebene Rendite keine niedrigere Standardabweichung erzielt werden kann. Das heißt, es gibt bei Markowitz nich das \textit{eine} optimale Portfolio, sondern eine Reihe von optimalen Portfolios. Diese liegen allesamt auf der "`Efficient Frontier"'. Rationale Individuen sollten nur solche Portfolios erwerben. Welches genau hängt bei \textcite{Markowitz1952} noch von der individuellen Risikoaversion des Investors ab. Die Arbeit gilt heute, 70 Jahre später, noch immer als Ausgangspunkt für quantitatives Asset-Management. 

Mit einer recht intuitiven Idee wurde die Markowitz-Portfoliotheorie durch \textcite{Tobin1958} erweitert. Und zwar indem er die Berücksichtigung eines risikolosen Assets einführte. Die Markowitz-Portfoliotheorie behandelt ausschließlich risikobehaftete Assets. Wenn man jetzt zum Beispiel eine risikolose Anleihe heranzieht, so hat diese einen bestimmten Erwartungswert und eine Standardabweichung von - definitionsgemäß - Null. Dieser Anleihe wird als Fixpunkt betrachtet. Ausgehend von diesem Fixpunkt wird nun eine Tangente an die "`Efficient Frontier"' gelegt. Per Definition berührt diese Gerade die Efficient Frontier nur in einem einzigen Punkt. Dieser Punkt stellt das tatsächlich einzige optimale Portfolio - genannt "`Marktportfolio"' - dar. Die Verbindungslinie zwischen risikoloser Anleihe und Marktportfolio nennt man die "`Kapitalmarktlinie"' (Capital Market Line). Das Marktportfolio ist in diesem theoretischen Konstrukt das einzig sinnvolle Portfolio. Unabhängig von der Risikoaversion kann nämlich aus der Kombination aus risikoloser Anleihe und Marktportfolio stets eine höhere Rendite-Erwartung (bei fixiertem Risiko) erzielt werden, als auf einem beliebigen Punkt auf der Efficient Frontier. Diese Erkenntnis wurde als die \textsc{Tobin-Separation} bekannt. Der Name Separation steht hierbei dafür, dass bei Finanzinvestitionen zwei voneinander unabhängige Entscheidungen getroffen werden müssen. Erstens, es muss das Marktportfolio ermittelt werden. Dieses ist allerdings für jeden risiko-adversen Investor identisch. Zweitens, abhängig von der individuellen Risikoaversion muss ein Investor entscheiden welchen Anteil seines Vermögens er in das risikobehaftete Marktportfolio steckt und welchen Anteil in die risikolose Anleihe. Auch dieses Verhältnis kann man übrigens - für jedermann individuell - quantitativ berechnen. Für jedes risiko-adverse Individuum lässt sich eine Nutzenfunktion ermitteln. Die individuelle Risikoaversion (vgl. Kapitel \ref{Erwartungsnutzen}) kann als "`Mean-Variance"'-Maß\footnote{"'Mean"' bezeichnet hierbei der Erwartungswert der Renditen und "`Variance"' die Varianz, also das Quadrat der Standardabweichung und damit das Risiko.} ausgedrückt werden. Die Nutzenfunktion lässt sich somit mittels Indifferenzkurven in das Portfolio-Diagramm überführen. Der Tangentialpunkt von Indifferenzkurve und Kapitalmarktlinie bestimmt das optimale, individuelle Verhältnis zwischen risikoloser Anleihe und Marktportfolio.

In den 1960er Jahren wurde Tobin's Modell schließlich zum \textsc{Capital Asset Pricing Model (CAPM)} (sprich: CÄP-M) weiterentwickelt. Gleich vier Autoren haben dessen Entwicklung parallel vorangetrieben: \textcite{Sharpe1964}, \textcite{Lintner1965}, \textcite{Mossin1966} und später wurde auch \textcite{Treynor1961} die Idee zugeschrieben. Der Ausgangspunkt ist, dass mittels Diversifikation - wie bei Markowitz dargestellt - Unternehmens-spezifisches Risiko eliminiert werden kann. Einfach deswegen, weil man in viele verschiedene Unternehmen investiert und der Anteil eines bestimmten Unternehmen mit steigender Anzahl an Assets gegen Null geht. Das Unternehmens-spezifische Risiko wird hierbei unsystematisches Risiko genannt. Nicht weg-diversifizieren kann man das systematische Risiko. Dieses wird auch Marktrisiko genannt, bzw. im CAPM als ($\beta$) bezeichnet. Da man im CAPM davon ausgeht, dass man das unsystematische Risiko durch Diversifikation vollständig eliminieren kann, wird man für die eventuelle Übernahme von unsystematischem Risiko (durch fehlende Diversifikation) nicht belohnt. Im CAPM wird daher nur das Marktrisiko betrachtet. Die entsprechenden Darstellungen zeigen daher stets den Trade-Off zwischen erwarteter Rendite und Beta (statt Standardabweichung bei Markowitz und Tobin). Das Marktrisiko wird dabei auf das "`Risiko des Gesamtmarktes"' bei 1 standardisiert. Natürlich ist umstritten was in der Praxis "`der Gesamtmarkt"' ist. Durchgesetzt haben sich hierbei aber breit gefasste Indizes wie der MSCI World. Es werden aber auch Indizes eines Einzelstaates durchaus als Gesamtmarkt herangezogen. Die Renditen der einzelnen Unternehmen (Aktien) werden ins Verhältnis zu diesem Gesamtmarkt gesetzt. Das heißt man schaut sich für einen gewissen Zeitraum, zum Beispiel fünf Jahre, wie sich die monatlichen Marktrenditen zu den monatlichen Einzelaktien-Renditen verhalten haben. Dies wird mathematisch analog zu einer univariaten, linearen Regression gemacht: Die gemeinsame Varianz (Covarianz) von Marktrendite und Einzelaktie-Rendite wird ins Verhältnis zur Marktrendite gesetzt. Das Ergebnis ist eben der $\beta$-Wert. Oder mathematisch: Die Steigung der Regressionsgerade, die bei Regressionen eben auch $\beta$ genannt wird. Ist dies Wert kleiner als eins, so unterliegt die Einzelaktie geringeren Schwankungen als der Gesamtmarkt und umgekehrt. Dieses Maß für das systematische Risiko eines Unternehmens hat bis heute eine enorme Bedeutung in der Unternehmensbewertung. Trotz aller Kritik und weiterentwickelten Methoden, wird in den überwiegenden Fällen der Eigenkapitel-Wert eines Unternehmens noch immer mittels $\beta$-Werten geschätzt. \textcite{Hamada1972} erweiterte das Modell schließlich noch um eine Bereinigung von Effekten der Unternehmens-Kapitalstruktur (vgl. Kapitel \ref{Struktur}). Bis heute sind das CAPM und vor allem die daraus abgeleiteten Risikomaße $\beta$, oder das \textsc{Sharpe-Ratio} - die Überrendite einer Aktie im Verhältnis zu ihrer Standardabweichung - zentrale Kennzahlen in der Finanzwelt. Das ist einigermaßen überraschend, da das CAPM seine Prognosen nur auf Grundlage einer einzigen Kennzahl, der vergangenen Rendite, trifft. Außerdem bleibt die Frage was das \textit{eine} Marktportfolio sei. Zudem kann die Heranziehung unterschiedlich langer Betrachtungszeiträume, zu recht unterschiedlichen $\beta$-Werten führen. Als Alternative veröffentlichte \textcite{Ross1976} sein Arbitragepreismodell. Ebenfalls Eingang in die Standardliteratur fand die CAPM-Erweiterung von \textcite{Fama1993}. Neben der vergangenen Rendite, werden hierbei die Unternehmensgröße, sowie das Kurs-Buchwert-Verhältnis als zusätzliche Einflussfaktoren herangezogen. Eine praktisch überaus bedeutende Erweiterung der Portfolio-Theorie präsentierten \textcite{Black1992}. Darin drehten sie das Konzept der Portfolio-Theorie um, indem sie nicht zukünftige Renditen schätzen, sondern lassen stattdessen die Portfolio-Zusammensetzung für gegebene Renditen vom Modell berechnen. Dies umgeht das praktische Problem, dass Schätzmethoden für Renditen immer mit großen Unsicherheiten behaftet sind.

Fassen wir noch einmal die Erkenntnisse der letzten Unterkapitel in einem Beispiel zusammen. Gesetzt es gibt so etwas wie effiziente Märkte. Dann kann man davon ausgehen, dass sich Kursänderungen, zum Beispiel von Aktienkursen, nur aus nicht vorhersehbaren, zufälliger Ereignissen ergeben. Die resultierenden Aktienrenditen verhalten sich also wie eine Zufallsvariable, die - wie empirische Beobachtungen zumindest näherungsweise bestätigen - durch eine logarithmische Normalverteilung abgebildet werden kann. Rendite und Risiko einer Aktie lassen sich also durch die Parameter Erwartungswert $(\mu)$ und Standardabweichung $(\sigma)$ einer log-Normalverteilung abbilden. Über das Konzept der Markowitz-Portfoliotheorie und deren Erweiterung der Tobin-Separation, werden Diversifikationseffekte, die durch die Korrelation von Aktien untereinander entstehen, berücksichtigt. Es gilt aber auch für gesamte Portfolios, dass diese mittels Erwartungswert und Standardabweichung vollkommen bewertet werden können.

Durch eine Von Neumann-Morgenstern-Nutzenfunktion (Erwartungsnutzenfunktion) lassen sich Erwartungswerte in Nutzen-Erwartungswerte transformieren. Zwar hat jeder Mensch eine individuelle Erwartungsfunktion, allerdings kann diese durch empirische Tests auch tatsächlich ermittelt werden. Das Arrow-Pratt-Maß liefert aus dieser Erwartungsnutzenfunktion ein Maß für die absolute Risikoaversion. Durch dieses Maß lässt sich die Erwartungsnutzenfunktion anhand der schon bekannten Parameter Erwartungswert $(\mu)$ und Standardabweichung $(\sigma)$ ausdrücken und in Form von Indifferenzkurven abbilden. Wir können als eine Koordinatensystem bilden in dem auf der y-Achse der Erwartungswert und auf der x-Achse die Standardabweichung abgebildet ist. Darin können wir sowohl die Efficient-Frontier, bzw. die Kapitalmarktlinie aus der Portfoliotheorie bzw. der Tobin-Separation einzeichnen, also auch die aus der Erwartungsnutzenfunktion abgeleiteten Indifferenzkurven für ein bestimmtes Individuum. Der Tangentialpunkt aus Kapitalmarktlinie und Indifferenzkurve bildet \textit{das} optimale Portfolio für einen bestimmten Menschen ab.

\section{Die Bepreisung von Optionen}
\label{Optionen}

Der eben genannte Fischer Black lieferte sein Meisterstück bereits in den frühen 1970er Jahren mit der nach ihm und seinem Forschungskollegen Myron Scholes benannten Optionspreisformel. Aber langsam. Zu Beginn der 1970er Jahre veränderten sich die Anforderungen an die internationalen Finanzmärkte grundlegend. Das Bretton-Woods-System, das geschaffen worden war um stabile Wechselkurse und somit finanzielle Planungssicherheit im internationalen Handel zu sichern, brach 1973 endgültig zusammen. Die Wechselkurse waren den Marktkräften ausgesetzt. Fixe Wechselkurse konnte man sich in der Folge nur noch durch Finanztermingeschäfte sichern. Im selben Jahr wurde in den USA das Chicago Board of Options Exchange (CBOE) gegründet. Bis heute die zentrale Terminbörse der USA. Der Markt für Termingeschäfte war also im Wachsen. Was aber fehlte war eine theoretisch fundierte Theorie zur Bepreisung von Optionsgeschäften. Und diese lieferten im gleichen Jahr \textcite{Black1973}. Die resultierende Optionspreisformel ist seither als "`Black-Scholes-Modell"', nur selten "`Black-Scholes-Merton-Modell"' bekannt. Die Arbeit von \textcite{Merton1973} erweiterte das Modell indem er die Existenz von Dividenden und schwankenden Zinssätze berücksichtigte \parencite{Scholes1997}.  Die faire Bepreisung von Optionen ist alles andere als ein banales Problem. Ausschlaggebend dafür ist die asymmetrische Pay-Off-Struktur von Optionen. Optionen gleichen einer Wette: Für einen bestimmten Einsatz - den Optionspreis - hat der Käufer einer Option das Recht - aber nicht die Verpflichtung - zu einem bestimmten Zeitpunkt in der Zukunft ein dahinter liegendes Asset (Underlying) zu kaufen. Wenn der Kurs dieses Assets zwischen Kauf der Option und dem festgelegten Ausübungszeitpunkt steigt, so macht der Käufer der Option einen Gewinn. Steigt der Kurs des Underlyings nicht, so lässt er seine Option einfach verfallen. Sein Verlust beschränkt sich dann auf den Optionspreis, den er bereits bezahlt hat.
Das heißt für die Bepreisung, dass man eine Wahrscheinlichkeitsverteilung finden muss, aus der man ablesen kann, welchen Wert das Underlying im Ausübungszeitpunkt der Option mit welcher Wahrscheinlichkeit hat. Zur Berechnung dieser bedienten sich \textcite{Black1973} einer Formel aus der Physik. Nämlich der Brownschen-Bewegung, mit der man zum Beispiel die Ausbreitung von Wärme berechnen kann. Man geht auch bei diesem Konzept davon aus, dass zukünftige Renditen einem Zufallsprozess folgen, der aber eine logarithmische Normalverteilung abbildet, dies bildet eben die Brownsche Bewegung ab. Die Black-Scholes-Formel - eine stochastische Differenzialgleichung - berechnet unter diesen Annahmen aus dem derzeitigen Kurs des Underlyings, dem Ausübungspreis, sowie der Standardabweichung des Underlyings\footnote{Außerdem benötigt man einen konstanten, risikolosen Zinssatz und die Prämissen, dass keine vorzeitige Ausübung möglich ist und keine Dividenden bezahlt werden, müssen nach \textcite{Black1973} ebenfalls gelten.} den fairen Optionspreis.
Ende der 1970er Jahre entwickelten \textcite{Rubinstein1979} übrigens das sogenannte Binominal-Modell zur Optionspreisbewertung.Es handelt sich hierbei aber um ein diskretes Modell. Das heißt man geht davon aus, dass das Underlying in jeder Periode mit einer gewissen Wahrscheinlichkeit steigt bzw. mit der Gegenwahrscheinlichkeit fällt. Erhöht man die Anzahl der Perioden, entsteht ein Baum an dessen Enden man jeweils den Wert der Option für eine bestimmte Wahrscheinlichkeit ablesen kann. Erhöht man die Anzahl der Perioden gegen unendlich liefert das Binominalmodell genau das gleiche Ergebnis wie die Black-Scholes-Formel. In den folgenden Jahrzehnten wurden Derivative Wertpapiere wie Optionen vielfach weiter entwickelt und deren Verbreitung steigerte sich enorm. Vor allem in der Nachbetrachtung der "`Great Depression"' gerieten Derivate in der Öffentlichkeit in Verruf. Ein Verbot bestimmter Derivate wurde gefordert und zweitweise auch in gewissen Bereichen eingeführt. Tatsächlich sind reine Finanz-Wetten auf Soft-Commodities wie zum Beispiel Weizen oder Schweinehälften verwerflich. Man muss aber so realistisch sein, dass man eben kaum unterscheiden kann zwischen dem sinnvollen Einsatz von Derivaten, zum Beispiel im Rahmen von Absicherungsgeschäften, und reinen Finanzspekulations-Geschäften. Faktum ist, dass viele moderne Derivate - zum Beispiel Knock-Out Zertifikate - eine komplizierte Auszahlungsstruktur haben und häufig nur mehr mittels numerischer Rechenmethoden bewertet werden können. Auch die Bewertungsmethoden haben sich in den letzten Jahrzehnten also wesentlich weiterentwickelt.

In seiner Nobelpreisrede \parencite{Scholes1997} und in \textcite[S. 311]{Bernstein1996} wird die Geschichte der drei Schöpfer der Optionspreistheorie, Fischer Black, Myron Scholes und Robert Merton, lebhaft erzählt. Besonders interessant ist die Tatsache, dass das Paper \textcite{Black1973} von drei renommierten Journals abgelehnt wurde, bis es schließlich - nach Interventionen - doch noch im \textit{Journal of Political Economy} veröffentlicht wurde \parencite[S. 136]{Scholes1997}. Ähnlich wie die oben angeführte Geschichte vom Zweifel Friedman's an Markowitz' Portfoliotheorie, zeigt auch diese Story, dass die damalige Einführung mathematisch-quantitativer Methoden in die Finanzmarkttheorie damals als völlig unkonventionell angesehen wurde. Im Nachhinein gesehen aber auf jeden Fall aber bahnbrechend. Die Finanzmarkttheorie war in den 1970er Jahren in doppelter Hinsicht eine "`junge"' Disziplin: Markowitz war bei der Veröffentlichung seiner "`Portfolio Selection"' gerade 25 Jahre alt, William Sharpe bei der Entwicklung des CAPMs knapp 30 und die drei Optionspreistheoretiker ebenso um die 30 bei der Veröffentlichung ihrer Formel. Eine oft zitierte Side-Story zu Myron Scholes und Robert Merton ist deren Engagement beim Hedge-Fund "`Long-Term Capital Management"' (LTCM). Die beiden waren Direktoren dieses Fonds. Das Geschäftsmodell bestand - grob gesagt - darin Assets mit ähnlichem Risiko, aber unterschiedlicher Bewertung zu identifizieren. Es wurden in weiterer Folge gehebelte Wetten darauf platziert, dass sich dieser "`Spread"' zwischen den Bewertungen schließen sollte - an sich eine erwartbare Entwicklung. Mit der russischen Finanzkrise 1998 geriet dieses Konzept allerdings vollkommen aus den Fugen. Der Fonds schlitterte in die Pleite und musste schließlich sogar staatlich gerettet werden, da Auswirkungen auf die gesamte Finanzwelt befürchtet wurden.

Die vier genannten Bereiche - die Kapitalstrukturtheorie, die Effizienzmarkthypothese, die Porfoliotheorie und die Optionspreistheorie - bilden bis heute das Rückgrat der "`Modern Finance"'. Vor allem in der Hochschulausbildung bilden diese vier Konzepte die theoretische Basis jedes "`(Corporate) Finance"'-Kurses. Die Entwickler dieser Disziplin wurden in den folgenden Jahrzehnten übrigens fast allesamt mit dem Nobelpreis geehrt\footnote{Wenn auch teilweise primär für andere Beiträge zur Ökonomie, wie zum Beispiel im Fall von Franco Modigliani und James Tobin.}: James Tobin 1981, Franco Modigliani 1985, Harry Markowitz, Merton Miller und William Sharpe 1990, Robert Merton und Myron Scholes 1997\footnote{Fischer Black war bereits zuvor verstorben} und schließlich Eugene Fama 2013. Dies ist doch einigermaßen überraschend, da die Finanzmarkttheorie ja nur ein kleiner Teilbereich der Ökonomie ist.





