%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Die Neoklassik neben Keynes}
\label{Neoklassik_nach1945}

Teil des Kapitels könnte auch Finanzierungstheorie, Spieltheorie sein (sind aber eigene Kapitel), aber auch die ganze Input-Output-Analyse.


\section{Pigou: Die Neoklassik erfindet sich neu}
\label{sec: Pigou}

Beginnend mit Arthur Cecil Pigou entwickelte sich die Neoklassik in eine neue Richtung. Wie in Kapitel \ref{Neoklassik} dargestellt, kann die Neoklassische Theorie mit Marshall in gewisser Hinsicht als abgeschlossen gelten. Dies vor allem in dem Hinblick, dass sich die mikroökonomischen Ausgangstheorien seit damals tatsächlich kaum noch geändert haben. Selbst in modernen Mikroökonomie-Büchern sind die ersten Kapitel im wesentlich identisch in den Arbeiten Marshall's zu finden. Die Mikroökonomie wurde seither nicht mehr grundlegend verändert, in dem Sinne, dass zuvor geltende Konzepte über den Haufen geworfen wurden, sondern sie wurde seither immer wieder erweitert. Marshall's Principles sind nach wie vor richtig. Aber man kann sie eben nicht mehr als Erklärung aller wirtschaftlichen Tätigkeiten heranziehen, sondern gelten mittlerweile nur mehr für einen sehr speziellen Fall. Heute wissen wir, dass die damals angenommenen Voraussetzungen, wie vollkommene Märkte und perfekte Konkurrenz nicht die Regel, sondern die Ausnahme sind.

Arthur Cecil Pigou ist der letzte große Ökonom in der Reihe der englischen Vertreter der Klassik und Neoklassik in Cambridge. Nach Pigou entwickelte sich die Ökonomie dort in Richtung Keynesianismus weiter, wie wir im letzten Kapitel (vgl. Kapitel \ref{Keynes}) bereits gelesen haben. Pigou ist wohl einer der meist unterschätzten Ökonomen des 20. Jahrhunderts. Schließlich war er der erste Mainstream-Ökonom, der postulierte und auch anerkannte, dass rein marktwirtschaftliche Ergebnisse nicht immer effizient sein müssen. Er erkannte also bereits vor Keynes die Sinnhaftigkeit von Staatseingriffe in bestimmten Situationen, wenn auch deren Wirkung und Berechtigung von ganz anderer Art und Weise sind. Während Keynes die makroökonomische Wirtschaftspolitik begründete, tat Pigou dies für die mikroökonomische Wirtschaftspolitik. Seine daraus unter anderem resultierenden Thesen zu den verschiedenen Formen des Marktversagen sind heute - Stichwort Klimawandel - aktueller denn je, wie gleich konkretisiert werden wird. Pigou's Karriere begann dann auch wie die eines ganz großen: Er war der Lieblingsschüler Marshall's der damals unangefochtenen Lichtgestalt der englischen Ökonomie. Mit erst 30 Jahre wurde Pigou schließlich dessen Nachfolger an der Universität in Cambridge. Und bereits 1912 legte er das bedeutende Werk "`Wealth and Welfare"' (\parencite{Pigou1912}) auf, das in der Neuauflage von 1920 als "The Economics of Welfare" (\parencite{Pigou1920}) ein bis heute bedeutendes Werk darstellt. Diese Jahreszahlen legen nahe, dass Pigou eigentlich auch in Kapitel \ref{Neoklassik} gut aufgehoben wäre. Inhaltlich von Bedeutung wurde der Forschungsbereich, den Pigou damit eröffnete, aber erst nach 1945. 

Mit zwei wesentlichen Punkten hat Pigou in seinem Hauptwerk die Neoklassik revolutioniert. Erstens, hat er mit dem Credo aufgeräumt, das seit Adam Smith die Neoklassik prägte, nämlich, dass individuell-nutzenmaximierendes Verhalten auch für die gesamte Gesellschaft erstrebenswert sei und das optimale gesamtwirtschaftliche Ergebnis hervorbringt \parencite[S. 111]{Pigou1920}. Das hat enorme Auswirkungen. Damit verbunden ist nämlich die Notwendigkeit staatlicher Eingriffe, immer dann, wenn der Markt darin versagt eine optimale Allokation hervorzubringen. Das riesige Gebiet der mikroökonomischen Wirtschaftspolitik war damit geboren. Zweitens, wollte er einen formalen Ansatz zur Analyse des \textit{gesamt}wirtschaftlichen Nutzens etablieren - was die Begründung der Wohlfahrtsökonomie bedeutete. Diese wurde aber recht rasch als gescheitert betrachtet - dazu aber später mehr. Werfen wir zunächst einen Blick auf seine Ansätze zum ersten Punkt. Grundlage sind die Gleichgewichtstheorien in der Tradition von Walras (vgl. Kapitel \ref{Walras}). Darin wird davon ausgegangen, dass alle Märkte im Gleichgewicht sind. Damit wurde stets auch impliziert, dass die Märkte jeweils das \textit{optimale} Ergebnis liefern. \textcite{Pigou1920} zeigte nun systematisch Beispiele, in denen das nicht der Fall ist. Der Ausgangspunkt sind hier stets Externalitäten \parencite[S. 115]{Pigou1920}. Um diese systematisch zu analysieren führt er den Vergleich zwischen privaten und sozialen Grenzprodukten ein \parencite[S. 114]{Pigou1920}. Nur wenn die sozialen und privaten Grenzerträge übereinstimmen, ist die Marktlösung auch eine gesamtwirtschaftlich optimale Lösung. Weichen die sozialen Grenzerträge von den privaten ab, liegt ein externer Effekt vor. Der könnte zum Beispiel darin bestehen, dass der Produzent A bei der Herstellung seiner Güter Kosten auf die Allgemeinheit überträgt, für die er nicht aufkommen muss. Zum Beispiel könnte seine Fabrik das Umland verschmutzen ohne, dass er eine entsprechende Reinigungsgebühr entrichten muss. Diese sozialen Kosten trägt stattdessen die Bevölkerung. Als Resultat sind die privaten Kosten des Fabrikanten zu niedrig, was wiederum in einer zu hohen Produktion und zu niedrigen Preisen führt. Nur wenn der Staat eingreift und durch Steuern die sozialen Kosten wieder auf den Produzenten überträgt, ist das gesamtwirtschaftliche Ergebnis tatsächlich optimal. Damit begründete er die heute wieder so oft zitierte Pigou-Steuer. Überhaupt erlangte Pigou mit dem Aufkommen der Umweltproblematik eine enorme Bekanntheit in den letzten Jahren. Seine Arbeiten zu Marktversagen begründeten die mikroökonomische Wirtschaftspolitik. Allerdings muss man einschränkend sagen, dass in \textcite{Pigou1920} zwar bereits verschiedene Marktversagensformen wie natürliche Monopole \parencite[S. 240]{Pigou1920}, Informationsmängel \parencite[S. 131]{Pigou1920} und eben externe Effekte, behandelt werden, allerdings eher anhand von Beispielen und nicht systematisch-analytisch. Auch finden sich noch theoretische Fehler in seinen Abhandlungen, wie zum Beispiel der fehlende Unterschied zwischen technologischen und pekuniären negativen Effekten \parencite[S. 242]{Cansier1989}. Letztgenannte sind definiert als Kosten, die einem Unternehmen erwachsen, wenn ein Konkurrent ein identisches Gut zu besseren Preisen anbieten kann. In diesem Fall ist keine Korrektur durch staatliche Einriffe sinnvoll, weil die "Kosten" darin bestehen, dass die Gewinne eines Unternehmens reduziert werden, und eben keine gesamtwirtschaftlichen Kosten entstehen. Pigou war relativ stur diesen Fehler einzugestehen \parencite[S. 153]{Johnson1960}. Er betrachtete externe Effekte außerdem stets eindimensional. Schließlich gibt es grundsätzlich zwei verschiedene Möglichkeiten, wie externe Effekte internalisiert werden können. Entweder zahlt der Verursacher der Gesellschaft einen Ausgleich für den Schaden, oder aber die Gesellschaft zahlt umgekehrt dem Verursacher die Kosten zur Vermeidung des Schadens. Ronald Coase (vgl. Kapitel \ref{Neue Institut}) kritisierte Pigou später vehement für diese einseitige Darstellung \parencite[S. 243]{Cansier1989}.

Der zweite neuartige Punkt in \textcite{Pigou1920} erscheint zwar reizvoll, wurde aber fast umgehend formal widerlegt: Der Versuch Pigou's eine gesamtwirtschaftliche Nutzenbetrachtung durchzuführen. Pigou beschäftigte sich dabei zunächst ausführlich damit den Begriff des Volkseinkommens, bzw. des Sozialprodukts zu definieren. In weiterer Folge wollte eine Funktion für die gesamtwirtschaftliche Wohlfahrt ableiten. Diese sei abhängig von der Höhe, der Verteilung und der Stabilität des Sozialprodukts \parencite[S. 42]{Pigou1920}. Der Ansatz klingt natürlich zunächst vielversprechend: Wir wissen ja, dass das Vermögen alleine nur eingeschränkte Aussagen über die tatsächliche Wohlfahrt, also den erlebten Nutzen, zulässt. Es war \textit{die} zentrale Errungenschaft der Neoklassik das Wertparadoxon der Klassiker zu überwinden und den Nutzen in den Vordergrund von Maximierungsentscheidungen zu stellen. Verlockend ist es daher eine "gesamtwirtschaftliche" Wohlfahrtsfunktion anzustreben. Das Bruttoinlandsprodukt (Volkseinkommen) als aggregierte Wertschöpfung der Bevölkerung in einem Staat, sagt wenig über die Wohlfahrt aus. Man könnte stattdessen die einzelnen Vermögen der Individuen als Nutzenwerte ausdrücken und das Aggregat dieser Werte als "nationalen Gesamtnutzen" - oder eben Wohlfahrt - interpretieren. \textcite[S. 48]{Pigou1920} legte also die individuelle Grenznutzen-Theorie auf die Gesamtwirtschaft um: "Das Gesetz des abnehmenden Grenznutzens lehrt uns, dass ein steigendes Sozialprodukt nur zu einem unterproportionalen Wohlfahrtsgewinn führt." Aus der Annahme des abnehmenden Grenznutzens lässt sich laut \textcite[S. 53]{Pigou1920} außerdem ableiten, dass eine zusätzliche Geldeinheit einer armen Person mehr Nutzen stiftet, als einer reichen Person. Oder mit anderen Worten: Neben einem möglichst hohem Gesamt-Volkseinkommen erhöht auch eine möglichst gleiche Verteilung der Einkommen zu einer höheren gesamtwirtschaftlichen Wohlfahrt. Daher sollte der Staat auch dahingehend eingreifen und zum Beispiel durch progressive Besteuerung die Einkommenshöhe angleichen. Pigou missachtete dabei allerdings die wesentlichen Erkenntnisse seiner Vorgänger: Nutzen ist nicht kardinal, also in Geldeinheiten ausgedrückt, messbar. Stattdessen behilft man sich mit ordinalen Nutzenkonzepten wie in Kapitel \ref{Pareto} dargestellt. Es ist ein Irrtum zu glauben, Pigou wäre sich dessen nicht bewusst gewesen. Er wusste, dass interpersonelle Nutzenvergleiche formal-mathematisch nicht möglich sind. Aber er war auch pragmatisch genug um anzuerkennen, dass zum Beispiel bei unverändertem Sozialprodukt eine gleichmäßigere Einkommensverteilung gesamtwirtschaftlich einen positiven Wohlfahrtseffekt hat. Auch eine bewusst vereinfachend einheitliche Einkommens-Nutzenfunktion dachte Pigou zumindest an \parencite[S. 237]{Pigou1920}. Das Konzept von \textcite{Pareto1906} plädiert hingegen für eine die Unzulässigkeit von interpersonellen Nutzenvergleichen. Man kann eben gerade \textit{nicht} behaupten, dass eine arme Person durch eine zusätzliche Geldeinheit mehr Nutzen generiert als ein Millionär. Der Effekt von Umverteilungsmaßnahmen auf die Gesamtwohlfahrt ist damit in der Neoklassik in keinster Weise abbildbar. Stark kritisiert \parencite[S. 123]{Robbins1932} wurde Pigou für seine Idee der gesamtwirtschaftlichen Wohlfahrtsfunktion eben, weil die formale Unzulänglichkeit seines Konzepts schon seit dem Werk von \textcite{Pareto1906} bekannt war. Interessant ist der Ansatz von Pigou aus heutiger Sicht aber möglicherweise dennoch. Natürlich, formal-mathematisch ist es anerkannt, dass Nutzen nicht kardinal gemessen werden kann. Das stattdessen angewendete  Konzept von \textcite{Pareto1906}, bzw. der modernere Ansatz mittels Grenzrate der Substitution nach \textcite{Hicks1934b} ist eine elegante und in formaler Hinsicht höchst erfolgreiche Lösung. Diese führt aber eben auch dazu, dass Fragen der Einkommensverteilung in der Neoklassik einfach keinen Platz haben. Gerade solche Fragen sind in den letzten Jahren aber wieder vermehrt in den Vordergrund getreten.

Mit der Veröffentlichung von "`Wealth and Welfare"' und gerade einmal 43 Jahren war Pigou allerdings bereits auf den Zenit seiner Karriere angekommen. Denn sein eigener Assistent, ein uns bereits aus dem letzten Kapitel bekannter, gewisser John Maynard Keynes, revolutionierte kurze Zeit später die Ökonomie. Pigou nahm während dieser Revolution eine eher unglückliche Position ein. Er war einer jener Ökonomen, der an der fast magischen Strahlkraft von Keynes und dessen Werk fast zerbrach. Ähnlich ging es übrigens Joseph Schumpeter, Michal Kalecki und in gewissen Maße auch Friedrich Hayek. Ab den 1930er Jahren wendete sich Pigou nämlich von der Weiterentwicklung der "`Wohlfahrtsökonomie"' ab und stattdessen anderen Inhalten zu. Getrieben wurde er dazu natürlich von der "`Great Depression"', später aber auch vom Aufstieg Keynes'.  
Pigou könnte als das genau Gegenteil von Keynes beschrieben werden. Nach erschütternden Ereignissen als Sanitäter im Ersten Weltkrieg, wurde er zum exzentrischen Einzelgänger \parencite[S. 153]{Johnson1960}. Der lockere Bloomsbury-Group-Lebemann Keynes trat schon diesbezüglich ganz anders auf. Auch wird Pigou in seiner Position als Berater als Vertreter veralteter Ideen beschrieben, der unter anderem die Wiedereinführung des Goldstandards empfahl \parencite[S. 232]{Cansier1989}. Eine Idee, die Keynes als "`barbarisches Relikt"' ansah und nach 1918 auch nicht mehr wirklich erfolgreich implementiert werden konnte \parencite[S. 232]{Cansier1989}. Zwar haben wir soeben gelesen, dass Pigou staatliche Eingriffe als erster moderner Ökonom in vielen Situationen befürwortete, in Bezug auf die "`Great Depression"' schlug er aber keine konjunkturfördernden Maßnahmen vor. Im Gegenteil, die hohe Arbeitslosigkeit erklärte er typisch neoklassisch als Missmatch zwischen Angebot und Nachfrage auf dem Arbeitsmarkt \parencite[S. 232]{Cansier1989}. Seine neoklassische Analyse der Arbeitslosigkeit (\textcite{Pigou1933}: The Theory of Unemployment) kam zur Unzeit, am Höhepunkt der "`Great Depression"'. Es war die erste umfassende neoklassische Beschäftigungstheorie, aber gerade während der Weltwirtschaftskrise waren diese Erklärungsmuster unpassend. So wurde das Werk schließlich der Angriffspunkt schlechthin für Keynes. Überhaupt war Pigou, als führender Vertreter der neoklassischen Schule in den 1930er Jahren, \textit{die} Zielscheibe von Keynes' Kritik in der "`General Theory"' \parencite[S. 154]{Johnson1960}. Sehr häufig liest man darin über die "`falschen Schlussfolgerungen von Prof. Pigou"' \parencite[S. 73]{Keynes1936}. Dieser reagiert trotzig und damit genau falsch. Anstatt sich auf Keynes' Theorien genauer einzulassen und diese dann eingehend zu analysieren, verfasste \textcite{Pigou1936} eine giftige Kritik über die Art und Weise wie Keynes seine Ideen darstellte, ohne dabei wirklich auf inhaltliche Unklarheiten tiefer einzugehen. Erst später änderte Pigou seine Meinung und akzeptierte die bahnbrechenden Erkenntnisse von \textcite{Keynes1936} \parencite[S. 154]{Johnson1960}. Seine späteren Werke \textcite{Pigou1941}: "`Employment and Equilibrium"' und \textcite{Pigou1943}, beinhalten schon die Anerkennung keynesianischer Ideen, beziehungsweise auch abweichende Meinungen, wie zum Beispiel die später als "`Pigou-Effekt"' beschriebene positive Wirkung sinkender Preise. Im Widerspruch zu Keynes führen sinkende Preise demnach zu einer höheren Nachfrage, weil die Kaufkraft des Geldes durch Deflation steigt. Der Pigou-Effekt ist nach wie vor in vielen Lehrbüchern zu finden, seine positive Wirkung blieb aber eher Minderheitenmeinung.

Angriffe auf das zweite Standbein von \textcite{Pigou1920}, die Notwendigkeit von Staatseingriffen bei Marktversagen, kamen nach dem Zweiten Weltkrieg von Seiten der neu aufkommenden Politischen Ökonomie bzw. des Neuen Institutionalismus (vgl. Kapitel \ref{Pol_Econ} bzw. \ref{Neue Institut}). Zuerst kritisierte \textcite{Coase1960} die einseitige Betrachtung Marktversagen können nur durch Staatseingriffe beseitigt werden. Seiner Meinung nach wären marktwirtschaftlichen Lösungen ebenso möglich. Weiters wurde erstmals das Problem des möglichen Staatsversagens aufgegriffen. Zwar wurde anerkannt, dass es Situationen gibt, in denen der Markt zu nicht-effizienter Allokation führen würde, allerdings wurde zunehmend angezweifelt, dass Staatsvertreter für eine bessere Allokation sorgen würden. Für das Problem der natürlichen Monopole wurde von \textcite{Baumol1982} das alternative Konzept der angreifbaren Märkte entwickelt.

Pigou ging dennoch als revolutionärer Ökonom in die Geschichte ein. Er erweiterte die rein marktwirtschaftliche Analyse der neoklassischen Theorie um Aspekte des Marktversagens und der gesamtwirtschaftlichen Wohlfahrt. Er brachte damit den Staat als wichtigen Player ins Spiel, ohne aber von den grundsätzlichen Ideen der Mikroökonomie abzugehen. \textcite{Pigou1920} stellt damit den Beginn der mikroökonomischen Wirtschaftspolitik dar.


\section{Die moderne Wohlfahrtsökonomie}
\label{Wohlfahrt}

Mit dem Kapitel der Wohlfahrtsökonomie betritt die Volkswirtschaftslehre ganz neuen Boden. Schon rein methodisch unterscheidet sich die Wohlfahrtsökonomie von der klassischen und neoklassischen Ökonomie: Sie ist eine normative Theorie \parencite[S. 77]{Scitovsky1941}. Während positivistische Theorien die ökonomischen Vorgänge beobachten und daraus Gesetzmäßigkeiten ableiten, sind sich normative Theorien ihres Einflusses auf die ökonomischen Vorgänge bewusst. Positivistische Theorien sind nicht wertend und können dafür stets objektiv validiert werden. Die Wohlfahrtstheorie hingegen akzeptiert, dass Ökonomen wirtschaftspolitische Empfehlungen abgeben um "`die Welt zu verbessern"', sie enthalten stets auch "`Werturteile"'. Ihre Fragestellungen können streng genommen nur subjektiv bewertet werden. Zum Beispiel: Ist eine gleichmäßigere Einkommensverteilung gerechter und besser für eine Gesellschaft?  Schon alleine diese Subjektivität machte die Wohlfahrtsökonomie von Anfang an umstritten und zwar auf zwei Ebenen. Erstens, die inhaltliche Ebene. Auf die Fragen der Wohlfahrtsökonomie gibt es oftmals plausible gegenteilige Antworten. Diese können zudem nicht abschließend, zum Beispiel mit empirisch-statistischen Untersuchungen, geklärt werden. Zweitens, gibt es eine übergeordnete Ebene, die in den Bereich der Philosophie vordringt. Macht es überhaupt Sinn, Fragen der Wohlfahrt, die man eben nie abschließend objektiv klären kann, in der Ökonomie zu behandeln? Bis heute ist die Wohlfahrtsökonomie ein stark beforschter Zweig der Wirtschaftswissenschaften. Wegen der fehlenden Möglichkeit einer Validierung allerdings wird bis heute heftig diskutiert, ob ihre Ergebnisse rein wissenschaftliche betrachtet einen wertvollen Beitrag liefern. 

Auch die Platzierung der Wohlfahrtsökonomie ist weder einfach noch klar. Die Einordnung in dieses Überkapitel macht vor allem wegen ihrer Ursprünge Sinn, alternativ könnte man sie auch als "`Social Choice Theory"' (Sozialwahl-Theorie)\footnote{Die "`Social Choice Theory"' ist eine Teildisziplin der Wohlfahrtsökonomie.} gemeinsam mit der "`Public Choice Theory"' im Kapitel \ref{Pol_Econ} darstellen. Wohlfahrtsökonomie und Public Choice Theorie beziehen sich auf ähnliche Grundkonzepte, die allerdings zu teils grundverschiedenen Ergebnisse kommen. Die Wohlfahrtstheorie ist allerdings Teil der Mikroökonomie, während die Public Choice Theorie der Neuen Politischen Ökonomie zugeordnet wird. 

Die Arbeiten von \textcite{Pareto1906} und vor allem \textcite{Pigou1920} gelten bis heute als die Ursprünge der Wohlfahrtstheorie. In Hinblick auf Pigou's "`The Economics of Welfare"' wurde dies bereits im letzten Kapitel \ref{sec: Pigou} dargelegt. Kommen wir noch einmal kurz darauf zurück: Durch die Kritik von \textcite{Robbins1932} an den notwendigen interpersonellen Nutzenvergleichen galt dessen Ansatz rasch als widerlegt. Das Nutzenkonzept von \textcite{Pareto1906} und \textcite{Hicks1934a} galt als formal überlegen. Dieses etablierte sich als Teil der neoklassischen Mainstream-Theorie: Das Pareto-Prinzip und die ordinale Nutzenmessung waren der Kern dieser "`Neuen Wohlfahrtsökonomie"'. Anerkannt war, erstens, dass Wohlfahrtsökonomie die Gesamtwohlfahrt einer Gesellschaft zu bewerten als Ziel hat. Es war aber eben, nach der einflussreichen, kritischen Arbeit von \textcite{Robbins1932}, zweitens, auch bereits klar, dass die \textit{Summe} der empfundenen Nutzen (Wohlfahrt) nicht geeignet sei die Gesamtwohlfahrt zu messen (vgl. zum Beispiel \textcite{Lange1942}) Die Wohlfahrtsökonomie wurde daher ab Ende der 1930er Jahre auf neue Beine gestellt. Damals wurden die heute noch angeführten zwei Hauptsätze der Wohlfahrtstheorie als solche ausformuliert:
\begin{itemize}
	\item Erstes Wohlfahrtstheorem: Auf einem vollkommenen Markt - also mit vollkommener Information, bei vollkommener Konkurrenz und ohne externe Effekte - sind Gleichgewichtslösungen Pareto-optimal. Pareto-optimal (bzw. Pareto-effizient) sind Marktlösungen dann, wenn keine Person besser gestellt werden kann, ohne dass eine einzige Person schlechter gestellt wird.
	\item Zweites Wohlfahrtstheorem: Jedes Pareto-Optimum kann durch Marktgleichgewicht realisiert werden. Das heißt für jedes Pareto-Optimum existiert eine Einkommensverteilung, bei der alle Haushalte und Unternehmen ihre Nutzen bzw. Gewinne maximieren.
\end{itemize}
Die erstmalige Formulierung der Wohlfahrtstheoreme in ihrer modernen Form kann heute nicht mehr eindeutig zugeordnet werden. Das erste Theorem folgt im Prinzip schon direkt aus \textcite{Pareto1906}. Die erste mathematische Beweisführung wird häufig \textcite{Lange1942} zugeschrieben. 

Zwei verschiedene Ansätze prägten die frühe Zeit der "`Neuen Wohlfahrtsökonomie"'.  Der erste wurde von \textcite{Bergson1938} formuliert, aber erst durch \textcite{Samuelson1947} bekannt gemacht. Ähnlich wie bei Pigou gibt es hier eine "`Soziale Wohlfahrtsfunktion"', die den gesamtgesellschaftlichen Nutzen abbildet. Allerdings besteht dies nicht aus der \textit{Summe} der individuellen Nutzenwerte, sondern aus einem Vektor, der alle individuellen Nutzenwerte \textit{enthält}. Die Optimierungsaufgabe besteht nun nicht darin die Summe der Nutzen zu maximieren - was ja an den nicht-vergleichbaren und nicht-kardinalen individuellen Nutzenwerten scheitert - sondern darin, den optimalen Vektor zu bestimmen. Ein Vektor mit Nutzenwerten dominiert einen anderen Vektor immer dann, wenn er das Pareto-Kriterium erfüllt. Also kein Nutzen-Wert darf schlechter sein als im Vergleichsvektor \parencite[S. 9]{Suzumura2016}. Damit lässt sich eine "`Soziale Wohlfahrtsfunktion"' finden, die keine interpersonellen Nutzenvergleiche notwendig macht. Diese Bergson-Samuelson Sozial-Wohlfahrtsfunktion wurde von der ökonomischen Forschung bald wieder weitgehend verworfen. Anwendungsprobleme und Widersprüche zum Unmöglichkeitstheorem, auf das wir in Kürze eingehen werden, waren dafür verantwortlich. Weitgehend durchgesetzt hat sich hingegen der Ansatz, der auf \textcite{Kaldor1939} und \textcite{Hicks1940} zurückgeht. Noch heute wird oftmals auf das "`Kaldor-Hicks-Kriterium"', oder sogenannte "`Kompensation-Tests"' verwiesen, wenn es darum geht eine Kosten-Nutzen-Analyse vor Realisation eines Projektes durchzuführen. Worum geht es darin? Nun, das Pareto-Kriterium ist sehr streng wenn es darum geht Wohlfahrtsveränderungen herbeizuführen, da ja \textit{keine einzige} Person auch nur eine kleine Verschlechterung erfahren darf. \textcite{Hicks1940} und \textcite{Kaldor1939} argumentieren nun, dass eine Pareto-Verbesserung auch dann eintritt, wenn zwar der Wohlfahrtsverbesserung von Person A eine Wohlfahrtsverringerung bei Person B entgegensteht, die Wohlfahrtsverbesserung bei A aber zu einem Teil abgeschöpft wird um damit die Wohlfahrtsverringerung bei B zu kompensieren. Auch an diesem Konzept gibt es Kritikpunkte \parencite{Baumol1946}. Der erste ist inhaltlicher Natur. Während \textcite{Pigou1920} noch explizit darauf achtete bei seiner Form der Wohlfahrtsökonomie auch eine ethische Komponente zu umfassen, fehlt dies in der "`Neuen Wohlfahrtsökonomie"' gänzlich. Für Pigou war es klar, dass bei sonst konstanter Wohlfahrt eine Änderung der Einkommensverteilung zugunsten armer Haushalte eine höhere Gesamtwohlfahrt impliziert. Das darf man beim Kaldor-Hicks-Kriterium nicht annehmen. Wessen Wohlfahrt verringert wird um Kompensation bei einem Geschädigten zu erreichen und ob diese Kompensation auch tatsächlich realisiert wird, ist nicht Teil der Überlegungen bei Kaldor und Hicks \parencite[S. 11]{Suzumura2016}. Der zweite Kritikpunkt war ein rein formal-logischer. \textcite{Scitovsky1941} zeigte anhand eines einfachen Beispiels, dass es Fälle gibt, bei denen die Rückabwicklung einer ursprünglich Wohlfahrts-steigernden Maßnahme ebenfalls zu einer Wohlfahrtssteigerung führt. Das Kaldor-Hicks-Kriterium ist dann inkonsistent bei der Bestimmung der Wohlfahrtseffekte ökonomischer Maßnahmen. Dieses Phänomen wurde als Scitovsky-Paradoxon bekannt \parencite[S. 12]{Suzumura2016}.

Anfang der 1950er Jahre wurde die Wohlfahrtstheorie von Kenneth Arrow mit dessen Unmöglichkeitstheorem um eine vollkommen neue Problematik erweitert. Seine Arbeit war übrigens auch ein Anstoß für die neue Forschungsrichtung der "`Neuen Politischen Ökonomie"' ("`Public Choice Theory"'), die in Kapitel \ref{Neue_Politik} behandelt wird. Konkret auf die Wohlfahrtsökonomie angewendet in \textcite[S. 329]{Arrow1950}, und wenig später in seinem bahnbrechendem Werk \parencite{Arrow1951} dargestellt, zeigt Arrow auf, dass rationales Wahlverhalten zur Unmöglichkeit konsistenter demokratischer Entscheidungen führt. Dieses Problem wurde schon von \textcite{Condorcet1785} erkannt: Wenn drei Personen bei drei alternativen Abstimmungsmöglichkeiten A, B und C jeweils eine andere Reihenfolge präferieren, so bevorzugt eine Mehrheit von zwei Personen die Alternative A gegenüber B und B gegenüber C. Aber es findet sich - unter anderer Abstimmungsreihenfolge - auch eine Mehrheit, die die Alternative C gegenüber A bevorzugt. \textcite{Arrow1950} beschreibt, dass nicht nur spezielle Probleme, wie das Scitovsky-Paradoxon beim Kaldor-Hicks-Kriterium, oder das Condorcet-Paradoxon bei der Bergson-Samuelson Sozial-Wohlfahrtsfunktion, Probleme bereiten. Stattdessen gibt es für jedes Entscheidungskriterium, das darauf basiert, die individuellen Präferenzen von Individuen aggregiert heranzuziehen um eine demokratische Lösung zu finden, Beispiele, die Inkonsistenzen aufweisen \parencite[S. 330]{Arrow1950}. Das sogenannten "`Arrow'sche Unmöglichkeitstheorem"' - und damit auch die "`Social Choice Theory"' im engeren Sinn\footnote{\textcite{Fleurbaey2021} schreiben richtigerweise, dass die Social Choice Theory bereits durch zwei Journalbeiträge von Duncan Black im Jahr 1948 (\textcite{Black1948a} und \textcite{Black1948b}) begründet wurden. Darin werden Mehrheitsregeln und spezielle Mehrheitsregeln formal-logisch untersucht \parencite{Fleurbaey2021}.} - waren geboren. \textcite{Arrow1950} argumentiert damit, das nicht nur kardinale Nutzenmessung unmöglich ist, sondern auch ordinale Nutzenmessung - wie in der "`Neuen Wohlfahrtsökonomie"' angewendet - zumindest problematisch ist. Was hat es mit dem Unmöglichkeitstheorem auf sich? \textcite{Arrow1950} erstellt darin ein logisches System mit fünf Bedingungen. Diese beinhalten rein logische Statements, aber auch die Bedingung, dass die Individuen in einer Gesellschaft rational handeln, sowie frei über ihre Präferenzen entscheiden dürfen. Weiterhin gelten die Wohlfahrtstheoreme, also das Pareto-Prinzip, sowie, dass  interpersonelle Nutzenvergleiche nicht sinnvoll möglich sind. Die Aufgabe besteht nun darin eine Soziale Wohlfahrtsfunktion aus den Präferenzen der Individuen abzuleiten, die zudem die fünf genannten Bedingungen erfüllt \parencite[S. 339]{Arrow1950}. Mit einer konsequenten Beweisführung zeigt \textcite[S. 339ff]{Arrow1950}, dass demokratisches Abstimmungsverhalten zu keiner konsistenten "`Sozialen Wohlfahrtsfunktion"' führen wird. So eine Funktion muss entweder als gegeben angenommen werden, oder aber eine Autorität, also ein Diktator, übernimmt die Präferenzordnung für alle Individuen einer Gesellschaft. Zusammengefasst: Man kann aus individuellen Präferenzen keine Soziale Wohlfahrtsfunktion ableiten. Man kann damit in einer Demokratie keine gesamt-gesellschaftliche Nutzenmaximierung durchführen. Oder mit anderen Worten: Innerhalb des Konzepts der neoklassischen Mikroökonomie lässt sich keine befriedigende Wohlfahrtsökonomie etablieren. Tatsächlich schien die Wohlfahrtsökonomie in der Folge in einer Sackgasse. Und was deren weitere Entwicklung innerhalb der Mainstream-Ökonomie angeht, muss man das wohl auch bestätigten. 

Zwar gab es weiterhin verschiedenste Ansätze zur Wohlfahrtsökonomie, diese im Detail zu analysieren wäre hier jedoch nicht zielführend. \textcite{Fleurbaey2021} folgend sei aber erwähnt, dass es einen starken Zweig in Richtung Spieltheorie gibt. Wie schon erwähnt, sind wohlfahrtstheoretische Überlegungen mit Ansätzen der Neuen Politischen Ökonomie verwandt. Aber auch in den Gebieten Logik und "`Computergestützte Sozialwahl"' gibt es abgeleitete Forschungszweige. Alternativ dazu gibt es auch Ansätze, die vom Inhalt her mit der Wohlfahrtstheorie in Verbindung gebracht werden, allerdings klar im Bereich der Heterodoxen Ökonomie anzusiedeln sind. Erwähnenswert ist hier die Glücksforschung, die seit den 1950er Jahren vor allem von \textcite{Easterlin1974} durchgeführt wurde. Auf Basis von Umfragen wird versucht zu erheben, was in Menschen tatsächlich Glücksgefühle auslöst. Wohlfahrt wird nicht mehr ausschließlich anhand des Sozialprodukts gemessen. Diese Ansätze unterscheiden sich fundamental von den davor dargestellten. Zum einen sind diese interdisziplinär, eine Mischung aus Ökonomie, Psychologie und Verhaltenswissenschaften. Zum anderen sind die Ansätze empirisch getrieben und verzichten auf die neoklassischen Modellannahmen, wie zum Beispiel die Annahme von Rationalität. Als modernen Ansatz zur Wohlfahrtsökonomie könnte man auch die Neuroökonomie ansehen. Mit bildgebenden Verfahren werden hier Vorgänge im Gehirn hinsichtlich sozialer Präferenzen und bei ökonomischen Entscheidungen gemessen. Dieser Forschungsbereich wird der Verhaltensökonomie zugezählt. Möglicherweise bietet der Ansatz in Zukunft aber die Unmöglichkeit das Problem der interpersonellen Nutzenvergleiche zu überwinden. Dieser Zweig der Wissenschaft publizierte zuletzt sehr erfolgreich in hoch angesehenen Journalen \parencite{Fehr2000, Fehr2003}.

Als historisch bedeutendste Weiterentwicklung, aber auch klare Neuorientierung der Wohlfahrtsökonomie, müssen die Arbeiten von Amartya Sen genannt werden. Neben dem extrem einflussreichen Werk des Polit-Philosophen \textcite{Rawls1971}: "`A Theory of Justice"',  führte vor allem Sen die Wohlfahrtsökonomie in den 1970er Jahren auf neue Wege. Zunächst - Anfang der 1970er Jahre - in Richtung Erweiterung der "`Social Choice Theory"'. In einem kurzen Beitrag führt \textcite{Sen1970} das Pareto-Prinzip in gewissen Situationen ad absurdum. Er führt dazu das Entscheidungskriterium des "`Liberalismus"' ein. Er selbst gibt zu, dass der Name etwas unglücklich gewählt ist. Es geht hierbei einfach darum, dass es Entscheidungen gibt, die nur auf eine Person, nämlich den Entscheider selbst, Auswirkungen hat. Am Beispiel des Buches "`Lady Chatterley's Lover"' - einer der ersten Erotik-Romane, der 1928 erschien und in vielen Ländern zensiert wurde - zeigt \textcite{Sen1970}, dass eine individuelle Präferenzordnung im Sinne des ordinalen Nutzenprinzips, das Pareto-Prinzip und das von Sen eingeführte liberale Prinzip nicht miteinander vereinbar sind. \textcite{Sen1970} wollte damit vor allem die Unzulänglichkeiten des Pareto-Prinzips als ökonomisches Entscheidungsprinzip aufzeigen. 
\textcite{Sen1970b} weichte in weiterer Folge die seit \textcite{Robbins1932} uneingeschränkt geltende Unzulässigkeit von interpersonellen Nutzenvergleichen auf. Dabei ging er streng formal-mathematisch vor. Insgesamt vertrat er den Standpunkt, dass interpersonelle Nutzenvergleiche in gewissem Ausmaß durchaus realistisch sind. "`Wir sollten keine großen Zweifel daran haben, dass Kaiser Nero's Nutzen von der Feuerbrunst in Rom kleiner war, als die Summe der Nutzenverluste aller anderen Römer"', meinte in seiner Rede anlässlich der Verleihung des Nobelpreises 1998 \parencite[S. 356]{Sen1999}. Sen hatte wenig Berührungsängste mit philosophischen Ansätzen. Er vertritt den Standpunkt, dass Wohlfahrtsökonomie auch ganz bewusst ethische Fragen behandeln darf. Dementsprechend entdeckte er Verteilungsfragen und Fragen der Armut für die Wohlfahrtsökonomie wieder. Durch viele Publikationen in hochwertigen Journalen und natürlich nicht zuletzt durch die Vergabe des Nobelpreises an Sen im Jahr 1998 erlangte die Wohlfahrtsökonomie in der öffentlichen Wahrnehmung wieder an Bedeutung. Allerdings entfernte sich die Wohlfahrtsökonomie damit immer stärker von der Mainstream-Ökonomie. Derzeit spielt die Wohlfahrtsökonomie innerhalb der Mainstream-Forschung maximal eine Nebenrolle, wie \textcite{Atkinson2011, Atkinson2001} kritisierte. Sen schuf aber vor allem auch Verbindungen zu den Arbeiten zur Einkommensverteilung von Anthony Atkinson (vgl. Kapitel \ref{Ungleichheit}) und zu den Arbeiten zur Armut von Angus Deaton (vgl. Kapitel \ref{Armut}). Man kann diese Bereiche als Weiterentwicklung der Wohlfahrtstheorie sehen, so gesehen gewinnt diese Disziplin in letzter Zeit wieder an Bedeutung.


\section{Bäume, die in den Himmel wachsen: Die Neoklassische Wachstumstheorie}
\label{sec: Wachstum}

Die Wachstumstheorie ist eine \textit{der} zentralen Disziplinen innerhalb der Ökonomie. Der Kern dieses Kapitels beschäftigt sich großteils mit dem historisch gesehen bedeutendsten Modell-theoretischem Erklärungsansatz: Der Exogenen Wachstumstheorie, häufig auch schlicht neoklassische Wachstumstheorie genannt. Diese entstand in den 1950er Jahren und wird bis heute in den meisten Lehrbüchern als Mainstream-Modell dargestellt. Auch wenn ihr die Endogenen Wachstumsmodelle diesen Rang in den letzten Jahrzehnten zunehmend ablaufen. 

Bevor wir uns den theoretischen Modellen zuwenden, gehen wir aber kurz fundamentaler auf das Phänomen Wirtschaftswachstum ein. Mehr als andere ökonomische Disziplinen beschäftigt es Ökonomen aus verschiedensten Richtungen. Tatsächlich ist die Geschichte der modernen Ökonomie - wenn man diese mit Adam Smith beginnen lässt - eine Geschichte des Wirtschaftswachstums. Seit der industriellen Revolution, die eben auch grob zusammenfällt mit der Publikation der "`Wealth of Nations"' in der zweite Hälfte des 18. Jahrhunderts, erlebt die wirtschaftliche Entwicklung auf einen beispiellosen Aufwärtstrend. Natürlich unterbrochen durch Wirtschaftskrisen, Kriege, Revolutionen und auch Seuchen. In keiner geschichtlichen Epoche davor konnte eine vergleichbare Entwicklung beobachtet werden. Ebenso interessant ist die Tatsache, dass der wirtschaftliche Aufstieg nicht gleichmäßig über den gesamten Globus erfolgte, sondern stattdessen einzelne Staaten und Wirtschaftsblöcke einen enormen Aufschwung erlebten, während dieser in anderen Teile der Erde bis heute ausblieb. Diese Thematik ist durchaus viel beforscht. Hier können die verschiedenen Erklärungsansätze dazu nur kurz angestreift werden: Bekannt sind die "`Fünf Stadien der Entwicklung"' - von der Tauschgesellschaft zum Massenkonsum - des insgesamt recht umstrittenen Politikers und Ökonomen \textcite{Rostow1960}. Der Wirtschaftshistoriker \textcite{Gerschenkron1962} nennt ebenso einen "`Modernisierungsanstoß"' als notwendigen Auslöser für einen dann folgenden großen Entwicklungssprung. Zwar wahrten die beiden Autoren eine kritische Distanz zueinander, ihre Arbeiten werden dennoch unter dem Begriff "`Modernisierungstheorie"` zusammengefasst. Im Gegensatz dazu entwickelte ab 1949 vor allem der Argentinier Raul Prebisch die "`Dependenztheorie"', die davon ausgeht, dass die Rückständigkeit der "`Peripherie-Länder"' durch deren Abhängigkeit von den Industriestaaten ("`Zentrum"') zustande kommt. Einflussreich schlug in dieselbe Kerbe die "`Welt-System-Theorie"' von \textcite{Wallerstein1974}. 

Die sogenannte "`Great Divergence"', also das Auseinanderdriften der Entwicklungsstadien in verschiedenen Staaten oder Wirtschaftsblöcken, spielte schließlich auch eine Rolle in der Evolution der Modell-theoretischem Erklärungsansätze, die wir in weiterer Folge im Detail betrachten. Das Solow-Modell postuliert eigentlich, dass es zu einer Angleichung des wirtschaftlichen Entwicklungsstandes verschiedener Länder kommen sollte. Allerdings ist dies eher nicht zu beobachten, was im "`Lucas Paradoxon"' formuliert wurde. Die Endogenen Wachstumstheorien (vgl. Kapite \ref{sec: endogene}) erklären die anhaltende Divergenz mittels unterschiedlichen Bildungsmöglichkeiten in unterschiedlichen Ländern. Einen anderen Ansatz verfolgt der "`Neue Institutionalismus"' von Daren Acemoglu (\ref{sec: Neue Inst}), der die Bedeutung von funktionierenden Institutionen als Voraussetzung für Wirtschaftswachstum hervorhebt. 

Die Erklärung langfristigen Wachstums, ist mehr als die meisten anderen ökonomischen Teilbereiche, eine Disziplin der Wirtschaft\textit{geschichte}, wie \textcite{Baumol1986} beschreibt. Zur Analyse der langfristigen, wirtschaftlichen Entwicklung sind aber auch entsprechende Datensätze notwendig. Bekannt geworden sind in diesem Zusammenhang die Arbeiten von Angus Maddison \parencite{Maddison2010}, der bis an sein Lebensende der Aufgabe nachging historische BIP-Daten zu sammeln oder zu rekonstruieren. Das Projekt wird seit seinem Tod im Jahr 2010 von ehemaligen Kollegen weitergeführt \parencite{Maddison2023}.

Nach diesem kurzen Exkurs zum eigentlichen Inhalt des Kapitels: Wirtschaftswachstum aus Modell-theoretischer Sicht. Die Vorgeschichte der Wachstumstheorien ist lang und vielfältig. Die bedeutendsten Ökonomen verschiedenster Richtungen hatten sich allesamt auch mit Wachstumstheorien beschäftigt. So etwa Adam Smith, der optimistisch im Hinblick auf langfristiges Wachstum war. Was die Klassiker angeht, wird auch die pessimistische Prognose Malthus' bis heute häufig zitiert, obwohl sich diese bislang nicht bewahrheitet hat. Nicht zuletzt widmete sich auch Karl Marx Wachstumstheorien. Mit der marginalistischen Revolution verlagerte sich die ökonomische Forschung auf die mikroökonomische Ebene. Allgemeines Wirtschaftswachstum wurde damit nicht mehr explizit im Rahmen einer eigenen Theorie thematisiert, aber natürlich implizit zum Beispiel im Rahmen der Produktivitätstheorien. Insbesondere die Arbeiten von \textcite{Wicksteed1894} und \textcite{Wicksell1922} waren in dieser Ära wichtige Wegbereiter der modernen Wachstumstheorie, wie wir gleich sehen werden. Während der "`Great Depression"' und der hohen Zeit des frühen Keynesianismus dominierten Theorien zur Krisenerklärung. Zwar entstanden in dieser Zeit die wichtigen (Vorläufer-) Arbeiten zur Wachstumstheorie - insbesondere die Cobb-Douglas-Produktionsfunktion, sowie das Ramsey-Modell - die hohe Bedeutung, die diese bis heute genießen, wurde ihnen allerdings erst nach dem Zweiten Weltkrieg zuteil. Betrachten wir zunächst die Entwicklung der Produktionsfunktion.

\subsection{Die Cobb-Douglas-Produktionsfunktion} \label{sec: Cobb-Douglas-Produktionsfunktion}
Cobb-Douglas-Produktionsfunktionen sind noch heute in jeden Ökonomie-Studium omnipräsent. Sie scheinen eines jener Konzepte, die die Evolution der Ökonomie unbeschadet überstehen. Interessant ist hierbei, dass der Name \textit{Cobb-Douglas}-Produktionsfunktion dabei nicht auf die eigentlichen Entwickler des \textit{theoretischen} Konzepts zurückgeht. Paul H. Douglas war ein Ökonomie-Lehrender an verschiedenen US-amerikanischen Universitäten, der, abgesehen von den Arbeiten zur Produktionsfunktion, keine bedeutenden wirtschaftswissenschaftlichen Forschungsarbeiten hervorbrachte. Douglas war allerdings als Politiker erfolgreich und lange Zeit für die Demokratische Partei im US-Senat. In \textcite{Douglas1976} beschreibt er, wie er im Jahr 1927, eher zufällig, die so langlebige Produktionsfunktion entwickelte. Er erstellte und verglich Indexzahlen für die Anzahl der Arbeitnehmer, sowie Höhe des eingesetzten Kapitals für amerikanische Produktionsunternehmen über die Jahre 1899 bis 1922. Als er die Werte in logarithmischer Form ins Verhältnis zu einem Produktionsindex setzte, bemerkte er, dass der Abstand der drei Funktionen über die Zeit annähernd konstant blieb \parencite[S. 904]{Douglas1976}. Er zog den befreundeten Mathematiker Charles Cobb zurate, wie denn dieses Ergebnis zu interpretieren sei. Dieser schlug vor den empirischen Zusammenhang in eine Formel zu gießen:

$$P = b*L^k*K^{1-k}$$

Die Produktion$P$ ergibt sich also aus den Faktoren Arbeit$L$ und Kapital$K$, die jeweils mit $k$ gewichtet wurden. Diese Formel ist eben bis heute als Cobb-Douglas-Produktionsfunktion weltweit bekannt. Die Gewichtung der Faktoren mit $k$ und $(1-k)$ bildet hierbei konstante Skalenerträge ab. Das heißt, wenn beide Produktionsfaktoren verdoppelt werden, verdoppelt sich auch der Output der Produktion$P$. Die Annahme konstanter Skalenerträge ist bis heute umstritten. Später gingen Cobb und Douglas dazu über die Exponenten der Faktoren unabhängig voneinander zu bestimmen, stellten aber empirisch fest, dass die Summe dieser Exponenten ohnehin jeweils ungefähr eins beträgt \parencite[S. 904]{Douglas1976}. 

Ihre Erkenntnisse veröffentlichten die beiden 1928 im Journal of Political Economy als "`A Theory of Production"' \parencite{Cobb1928}. Das der darin empirisch gezeigte Zusammenhang bereits zuvor von \textcite{Wicksteed1894} und \textcite{Clark1899} in ähnlicher Form postuliert wurde, war den beiden bekannt und sie zitierten entsprechend auch deren Arbeiten \parencite[S. 151]{Cobb1928}. Die Beiträge von \textcite{Wicksell1922}, der die Theorie bereits um 1890 darstellte und heute als der \textit{eigentliche} Urheber der Produktionsfunktion gilt, waren den beiden hingegen nicht bekannt. Zu Beginn wurde die Arbeit wenig akzeptiert, auch weil empirische Daten weitgehend fehlten um deren Aussagekraft überhaupt zu verifizieren. Vor allem der erste Ökonomie-Nobelpreisträger und damals führende quantitative Ökonom Ragnar Frisch kritisierte die Theorie als weitgehend nutzlos und die Aussagen, aufgrund der dünnen Datenlage als nicht haltbar \parencite[S. 905]{Douglas1976}. Cobb und Douglas konnten mit weiteren empirischen Untersuchungen die Gültigkeit ihrer Produktionsfunktion unterlegen. Ihre bahnbrechende Bedeutung erlangte sie aber erst später - dann bereits unter dem Namen "`Cobb-Douglas-Produktionsfunktion"' - unter anderem in der Makroökonomie als Ausgangspunkt der neoklassischen Wachstumstheorie, die wir im nächsten Kapitel betrachten (vgl. Kapitel \ref{sec: Solow-Modell}). 

Eine ähnliche Art einer Produktionsfunktion ist jene nach Wassily Leontief. In dessen mikroökonomischer Input-Output-Analyse bildet die Leontief-Produktionsfunktion die Transformation von eingesetzten Gütern und dem erzeugten Output ab. Die Input-Output-Analyse war damit einer der ersten konkreten Anwendungsfälle von Produktionsfunktionen. Sie wurde von Wassily Leontief mit den Werken \textcite{Leontief1936} und \textcite{Leontief1941} begründet und spielt bis heute eine wichtige Rolle in der Volkswirtschaftlichen Gesamtrechnung, zum Beispiel bei der BIP-Berechnung und in der Konjunkturtheorie.

Bis heute werden in der Volkswirtschaft "`Cobb-Douglas-"' und "`Leontief"'-Produktionsfunktionen gelehrt. Zusammengefasst werden diese unter dem Begriff CES-Funktionen ("`Constant Elasticity of Substitution"'). Wobei bei einer Leontief-Produktionsfunktion ein Produktionsfaktor nicht durch einen anderen ersetzt werden kann (Substitutions-Elastizität von konstant Null).

Wassily Leontief war als Kollege und Nachfolger von Joseph Schumpeter einer der prägenden Ökonomen an der Harvard University nach dem Zweiten Weltkrieg. Als solcher gilt er heute als einer der Begründer des "`Operations Research"' - einer Spielart der modernen quantitativen Ökonomie. In diesem Zusammenhang begründete er neben der Input-Output-Analyse die wirtschaftliche Anwendung linearer Optimierungsmodelle. Einer seiner Studenten dort war ab 1945 Robert Solow. Die beiden verband eine väterliche Freundschaft, wie \textcite{Solow1987a} später schrieb. Dieser Robert Solow sollte nur wenige Jahr später die neoklassischen Wachstumstheorie begründen.

\subsection{Solow: Technischer Fortschritt als Wachstumsquelle} \label{sec: Solow-Modell}
Noch bevor er seine Dissertation abschloss, wurde Robert Solow angeworben. Er wechselte 1949 von Harvard an das benachbarte Massachusetts Institute of Technology (MIT) und startete dort seine post-graduale akademische Karriere. Dort sollte er nicht nur über 40 Jahre, von 1949 bis zu seiner Emeritierung im Jahre 1995 \parencite{Solow1987a}, bleiben, sondern gemeinsam mit - dem uns schon bekannten - Paul Samuelson den Ruf des MIT als eine der führenden Ökonomie-Hochschulen der Welt begründen. Nicht nur der bahnbrechenden Forschungsleistungen von Samuelson und Solow, sondern auch wegen derer enormem didaktischen Leistungen. \textcite{Samuelson1998} ist den meisten Ökonomie Studierenden als \textit{das} Ökonomie-Lehrbuch schlechthin bekannt, das schon 1948 das erste Mal erschien. Am MIT wurden in weiterer Folge eine ganze Generation von Top-Ökonomen durch Solow und Samuelson geprägt, darunter einige spätere Nobelpreisträger.

Seinen wesentlichsten Beitrag lieferte Solow bereits in den Jahren 1956 und 1957 als er in zwei Journal-Beiträgen eben die neoklassische Wachstumstheorie begründete \parencite{Solow1956, Solow1957}. Diese wird auch häufig exogene Wachstumstheorie genannt, bzw. das entsprechende Modell als "`Solow-Wachstumsmodell"' bezeichnet. Im selben Jahr wie Solow publizierte übrigens auch der australische Ökonom Trevor Swan ein praktisch identisches Modell \parencite{Swan1956}. Vereinzelt spricht man daher auch vom "`Solow-Swan-Wachstumsmodell"'. Insgesamt blieb Trevor Swan allerdings der Ruhm verwehrt \parencite{Dimand2009}, den Robert Solow seine lange Karriere genoss. Im Jahr 1987 wurde nur Robert Solow der Nobelpreis für Wirtschaftswissenschaften verliehen. 

\textcite{Solow1956} bezieht sich bei seiner Entwicklung der neoklassischen Wachstumstheorie zunächst direkt auf das damals vorherrschende "`Harrod-Domar-Modell"' (vgl. Kapitel \ref{Post-Keynes}). Er kritisierte die Annahme, dass darin der Produktionsfaktor Kapital in keinem Fall durch den Produktionsfaktor Arbeit (oder umgekehrt) ersetzt werden kann, als unrealistisch \parencite[S. 65]{Solow1956}. Alle anderen Annahmen des Harrod-Domar-Modells werden beibehalten. Das heißt insbesondere, dass es sich um ein Modell des \textit{langfristigen} Wachstums handelt. Obwohl Solow als einer der Hauptvertreter der neoklassischen Synthese gilt, bezieht sich sein Modell damit ausschließlich auf neoklassische Annahmen \parencite[S. 91]{Solow1956} und lässt damit die kurzfristigen, keynesianischen Instabilitäten, aber auch den Staat als wirtschaftspolitischen Player außen vor. Es gilt stets die Identität von Investition und Sparen. Geld- und Fiskalpolitik gibt es in diesem Modell nicht. Ausgangspunkt ist - wie eben in Kapitel \ref{sec: Cobb-Douglas-Produktionsfunktion}) dargestellt - eine Cobb-Douglas-Produktionsfunktion in der schon oben dargestellten Form:

$$P = b*L^k*K^{1-k}$$

Die Produktion$P$ wird makroökonomisch als gesamtwirtschaftlicher Output, also das Bruttoinlandsprodukt (BIP) $Y$ interpretiert. Die bisher einfach als Konstante angenommene Variable $b$, wird nun als Technologie zu einem bestimmten Zeitpunkt $A_t$ interpretiert. Manchmal auch als "`Totale Faktor-Produktivität"' bezeichnet. Dieser Technologie-Parameter ist exogen vorgegeben und sollte sich im Rahmen von Solow's Analyse als \textit{der} entscheidende Parameter erweisen. Das Ausgangsmodell der Exogenen Wachstumstheorie wird dementsprechend meist wie folgt dargestellt:

$$Y = A_t*L^k*K^{1-k}$$

Das heißt, das Modell geht von konstanten Skalenerträgen aus \parencite[S. 67]{Solow1956}. Das ist eine wichtige Einschränkung, weil man damit davon ausgeht, dass Wirtschaftswachstum nicht einfach dadurch erreicht werden kann, dass durch größere/kleiner Unternehmen eine nachhaltige Produktivitätsverbesserung, und dadurch Wirtschaftswachstum, generiert werden kann. Angenommen werden außerdem positive aber abnehmende Grenzproduktivität. Auch dies ist eine wichtige, aber recht unumstrittene Annahme. Wenn man einen Produktionsfaktor erhöht, erhöht sich auch stets der Gesamtoutput, allerdings bei fortlaufender Erhöhung nur eines Produktionsfaktors in immer geringer werdendem Ausmaß. Das heißt aber auch, dass man das BIP nicht unendlich erhöhen kann, wenn man nur \textit{einen} der Produktionsfaktoren immer weiter, also unendlich, erhöht. Gesamtwirtschaftlich könnte man zum Beispiel argumentieren, dass der Produktionsfaktor Arbeit durch die Bevölkerungszahl begrenzt ist. Erhöht man als zum Beispiel bei gleichbleibender Mitarbeiterzahl ständig das Kapital – zum Beispiel die Anzahl der Computer – dann wird der erste eingesetzte Computer einen hohen Zuwachs an Produktivität bringen. Mit jedem weiteren Computer wird die Produktivität zwar weiter steigen, allerdings mit immer geringerer Zuwachsrate. Wenn jeder Mitarbeiter mehr als einen Computer besitzt, wird der Produktivitätszuwachs verschwindend gering werden.

Als nächsten Schritt analysiert \textcite{Solow1956} daher die Kapitalakkumulation. Tatsächlich geht er davon aus, dass die Arbeitsbevölkerung mit einer konstanten Rate langsam wächst – eine Annahme, die man zumindest für die mittlere Frist in Industriestaaten, bedenkenlos machen kann. Zusätzlich geht man von einer gewissen Abschreibungsrate des Kapitals aus, also die durchaus üblich Annahme, dass der Kapitalstock durch Benutzung an Wert verliert. Ein stabiles Wachstums gibt es als Folge nur dann, wenn die Investitionsrate den durch Abschreibung "`verloren"' gegangenen Kapitalstock ersetzt und zusätzlich das Bevölkerungswachstum durch Kapitalstockwachstum genau ausgeglichen wird \parencite[S. 73]{Solow1956}. Im Umkehrschluss heißt das aber auch, dass Wirtschaftswachstum - das über jenes, das durch Bevölkerungswachstum erklärt wird, hinausgeht - durch Kapitalakkumulation, also den höheren Einsatz von Maschinen etc. pro Arbeitskraft, nicht erklärt werden kann. Höhere Sparquoten und damit höhere Investitionsquoten alleine führen also nicht zu langfristig höheren Wachstumsraten. Dieses formal hergeleitete Ergebnis aus \textcite{Solow1956} war einigermaßen überraschend und eine neue Erkenntnis. Es löst aber gewissermaßen einen logischen Widerspruch zwischen dem damals vorherrschenden Harrod-Domar-Modell und der kurzfristigen keynesianischen Standardlösung: Ersteres geht davon aus, dass langfristiges Wirtschaftswachstum durch höhere Sparquoten erreicht wird. Keynes hingegen hebt den kurzfristigen Schaden höherer Sparquoten auf das Wachstum hervor, weil dadurch eben die aggregierte Nachfrage sinkt \parencite[S. 612]{Snowdon2005}.

\textcite{Solow1956} war damit mit seiner Analyse aber noch nicht am Ende, im Gegenteil. Der eigentlich wesentliche Aspekt des neoklassischen Wachstumsmodells, ist die Rolle des technologischen Fortschritts. Natürlich wurde technologischer Fortschritt schon vor \textcite{Solow1956} in der Ökonomie thematisiert. Man denke nur an das Werk Joseph Schumpeter's, dessen Lebenswerk sich praktisch um technologischen Fortschritt dreht (vgl. Kapitel \ref{Schumpeter}). In der Neoklassik wurde dessen Bedeutung aber tatsächlich erst durch \textcite[S. 85ff]{Solow1956} modelliert. Das Ergebnis ist einigermaßen skurril. Denn weder war die Analyse des technischen Fortschritts das eigentliche Ziel von \textcite{Solow1956}, noch wird technologischer Fortschritt im Modell erklärt \parencite[S. 610]{Snowdon2005}. Tatsächlich ist diese Variable eine exogene Variable, die eben von "`als von außen bestimmt"' und "`nicht durch das Modell erklärt"' akzeptiert wird. Nichtsdestotrotz wird der technische Fortschritt von \textcite{Solow1956} formal-mathematisch als \textit{der} wesentliche Treiber von Wirtschaftswachstum identifiziert.

Grundsätzlich ist die Bedeutung des technologischen Fortschritts intuitiv verständlich: Die Leistungsfähigkeit eines Unternehmens, oder aggregiert eben einer ganzen Volkswirtschaft, kann nur in gewissem Ausmaß dadurch verbessert werden, dass die Arbeitnehmer immer \textit{mehr} (gleiche) Schreibmaschinen zur Verfügung gestellt bekommen. Eine wesentliche Verbesserung wird aber erreicht, wenn Schreibmaschinen durch Computer ersetzt werden. Stetiges Wirtschaftswachstum ist also nur dann möglich ist, wenn sich die eingesetzten Kapitalgüter – also zum Beispiel Maschinen, Computer, Transportmittel, Kommunikationsmittel – immer leistungsfähiger werden. Der Inhalt des "`technischen Fortschritts"', also was sich wie verbessert – ist allerdings ist nicht Teil der Theorie von \textcite{Solow1956}. Die neoklassische Wachstumstheorie hat sich also damit abgefunden festzustellen, dass technischer Fortschritt für Wachstum notwendig ist, dieser selbst allerdings nicht durch ökonomisches Handeln beeinflusst werden kann. Der technische Fortschritt wurde also als "`exogen"' betrachtet.  Auch die alternativ häufig verwendete Bezeichnung \textit{exogene} Wachstumstheorie macht in diesem Zusammenhang Sinn. 

Ein Jahr später untersuchte \textcite{Solow1957} seine neoklassische Wachstumstheorie empirisch und zeigte, dass in den USA zwischen 1909 und 1949 fast 90\% des Wirtschaftswachstums aufgrund technologischen Fortschritts zustande kam \parencite[S. 320]{Solow1957}. Damit wurde das Solow-Swan-Modell endgültig als \textit{das} Standardmodell der Wachstumstheorie etabliert. Dieses blieb es unumstritten mehrere Jahrzehnte lang. Was aber auch daran lag, dass die Wirtschaftsforschung zwischen 1970 und 1985 praktisch keinerlei Interesse an Wachstumstheorien zeigte \parencite[S. 586]{Snowdon2005}. 

Ab Mitte der 1980er Jahre kam das Thema Wachstumstheorien wieder in Mode. Zwei wesentliche Punkte waren dafür ausschlaggebend: Erstens, die Erklärung des Wachstums primär durch eine exogene Variable wurde zunehmend unbefriedigend. Zweitens, die exogene Wachstumstheorie kann das Divergenz-Problem nicht erklären. Im Solow-Swan-Modell wird Technologie nicht nur als exogen vorgegeben angenommen, sondern damit auch als öffentliches Gut. Das heißt, in allen Ländern der Erde steht grundsätzlich die gleiche Technologie zur Verfügung. In Verbindung mit der unumstrittenen Annahme abnehmender Grenzproduktivität bedeutet dies, dass in Staaten mit geringerer Kapitalausstattung Investitionen ertragreicher sind, als in Staaten, in denen bereits eine hohe Kapitalquote vorherrscht. Mit anderen Worten: In armen Ländern, in denen zwar viel Arbeitskräftepotential vorhanden ist, aber der Kapitalstock gering ist, führen Investitionen zu höheren Wachstumsraten als in reichen Ländern, in denen der Kapitalstock bereits hoch ist. Als Konsequenz daraus müsste Kapital von reichen in arme Länder fließen. In der Folge müsste das Wachstum dort höher sein und es zu einer Angleichung des Wohlstandes kommen. Als erster lieferte \textcite{Gerschenkron1962} dieses Argument bereits in den 1960er Jahren. Richtig in Fahrt kam die Diskussion aber erst Mitte der 1980er Jahre. Unter anderem stellte \textcite{Lucas1990} genau die Frage, warum Kapital nicht von armen in reiche Länder fließt. Das exogene Wachstumsmodell hatte genau dies vorhergesagt, aber empirisch hatte man in den letzten Jahrzehnten eher das Gegenteil, nämlich die steigende Divergenz der Wachstumsraten zwischen Entwicklungsländern und entwickelten Ländern beobachtet. Das Problem wurde in weiterer Folge als "`Lucas-Paradoxon"' bekannt und umfangreich debattiert. Der wissenschaftliche Ausweg aus diesem Paradoxon lieferten \textcite{Romer1986, Romer1990}, sowie \textcite{Lucas1988}. Sie begründeten eine ganz neue Wachstumstheorie, die heute mehr und mehr zum State of the Art wird: Die sogenannten "`Endogene Wachstumstheorie"'. Darin argumentieren die beiden, dass technologischer Fortschritt nicht zufällig irgendwo "`vom Himmel fällt"', sondern, dass die Lebensumstände der Bevölkerung technologischen Fortschritt begünstigen oder eben hemmen. Damit wird zum einen der technologische Fortschritt als wichtigster Wachstumsfaktor "`endogenisiert"', also innerhalb der Modelle erklärt. Zum anderen kann man dadurch eben die (teilweise) fehlende Konvergenz der Wachstumsraten zwischen Entwicklungsländern und Industriestaaten erklären. Die "`Endogene Wachstumstheorie"' wird ausführlich in Kapitel \ref{sec: endogene} behandelt. In den letzten wurde zudem ein weiterer Ansatz zur Erklärung wirtschaftlichen Wachstums, beziehungsweise zur Erklärung \textit{fehlenden} wirtschaftlichen Wachstums im Mainstream etabliert: In Kapitel \ref{sec: Neue Inst} wird näher auf den aufstrebenden "`Neuen Institutionalismus"' eingegangen. 

Trotz der aufstrebenden "`Endogenen Wachstumstheorie"', findet auch die "`Exogene Wachstumstheorie"' durchaus noch berühmte Befürworter, die deren Erklärungswert auch heute noch für hoch halten. Die Arbeiten von \textcite{Mankiw1992} und \textcite{Mankiw1995} werden häufig als "`Erweitertes Solow Modell"' bezeichnet. Der wesentliche Schwachpunkt des Solow-Modells, nämlich dass aufgrund der abnehmenden Grenzproduktivität des Kapitals Entwicklungsländer durch höheres Wachstum allmählich zu Industriestaaten aufschließen sollten, wird im Erweiterten Solow Modell entschärft. \textcite{Mankiw1992} führen dazu den zusätzlichen Produktionsfaktor Human-Kapital ein. Technologischer Fortschritt bleibt in diesem Modell der wesentliche langfristige Wachstumstreiber, der auch weiterhin nicht durch das Modell erklärt wird. Auch wird Technologie weiterhin als öffentliches Gut betrachtet. Allerdings können die unterschiedlichen, nicht konvergierenden Wachstumsraten zwischen bestimmten Entwicklungsländern und Industriestaaten in diesem Modell durch das unterschiedlich ausgeprägte Human-Kapital, also der unterschiedliche Bildungsgrad und Ausbildungsstand der Menschen in verschiedenen Ländern erklärt werden. Wie in den Endogenen Wachstumsmodellen wird dadurch die Bedeutung von Bildung und deren Institutionen in den Vordergrund gerückt, aber eben ohne deren Entwicklung selbst zu erklären \parencite[S. 150]{Romer2019}. 

Der Vollständigkeit halber, aus Platzgründen aber nur ganz kurz, wird auch auf das Ramsey–Cass–Koopmans Modell verwiesen. Die grundlegende Arbeit dazu \parencite{Ramsey1928} lieferte bereits 1928 das junge Mathe-Genie Frank Ramsey. Dieser war ein Student Keynes' und eng mit dem österreichischen Philosophen Ludwig Wittgenstein befreundet. Leider starb Ramsey bereits 1930 mit nur 26 Jahren. Im Jahre 1965 griffen \textcite{Cass1965} and \textcite{Koopmans1965} das Konzept unabhängig voneinander auf und erweiterten es zu einer Wachstumstheorie. Es ähnelt vom Ergebnis bezüglich des Wachstumsaspekts dem Solow-Modell stark, unterscheidet sich davon allerdings in einem entscheidenden Punkt: Es ist mikrofundiert \parencite[S. 50]{Romer2019}. Das heißt das gesamtwirtschaftliche Wachstum wird durch die Optimierungsentscheidungen von Haushalten und Firmen bestimmt. Paul Romer hob in einem Interview \parencite[S. 675]{Snowdon2005} die Bedeutung des Ramsey-Cass-Koopmans Modells als Vorläufer der Arbeiten der "`Neuen Klassiker"' hervor, die in Kapitel \ref{Neue Makro} betrachtet werden. Davor werfen wir im nächsten Kapitel aber noch einen Blick auf eine weitere neoklassische Forschungsrichtung, die um die Zeit des Zweiten Weltkriegs einen entscheidenden Schub erlebte: Die Allgemeine Gleichgewichtstheorie.


\section{Die Welt im Arrow-Debreu-Gleichgewicht}
\label{Arrow-Debreu}

Die Allgemeine Gleichgewichtstheorie ist bis heute untrennbar mit dem Namen Leon Walras verbunden, wie bereits in Kapitel \ref{Walras} ausführlich beschrieben. Bis heute bezeichnet man Modelle, die auf allgemeine Gleichgewichtszustände referenzieren als "`walrasianisch"' (vgl. z. B. Kapitel \ref{Neue Neoklassische Synthese}). Tatsächlich war \textcite{Walras1874} der Ursprung der Forschung zur allgemeinen Gleichgewichtstheorie. Walras' Ansatz die Lösbarkeit des Gleichungssystems durch "`Abzählen"' der Gleichungen und Variablen zu beweisen, war im Jahre 1874 originell und revolutionär. Aber schon in der ersten Hälfte des 20. Jahrhunderts waren die mathematischen Methoden wesentlich ausgereifter und der Ansatz von Walras gilt heute als unzureichend um als mathematischer Beweis der Existenz eines Allgemeinen Gleichgewichts aufrechterhalten zu bleiben. Weitere Schwächen von \textcite{Walras1874} waren, dass theoretisch negative Preise mögliche Lösungen wären und Preise von Null für jedes Gut ebenfalls eine gültige Lösung darstellen würde.

Ab 1918 wurde die Theorie des allgemeinen Gleichgewichts Schritt für Schritt auf ein formal-methodisch solideres Fundament zu stellen. Interessant ist in diesem Zusammenhang, wie zu jener Zeit noch das ehemalige Österreich-Ungarn ein Zentrum der formalen Wissenschaften war und insbesondere auch ein Österreich-Bezug bei der historischen Entwicklung der allgemeinen Gleichgewichtstheorie immer wieder auftritt. Zunächst griff der schwedische Ökonom \textcite{Cassel1918} die Ideen von Walras auf. Er stellte Überlegungen zu den Preisen in den Mittelpunkt des Gleichungssystems und klammerte Überlegungen des Nutzens, die bei \textcite{Walras1874} ein wichtige Rolle spielten, aus. Entscheidend ist, dass sein System der Preisbestimmung interagierende Angebots- und Nachfragefunktionen beinhaltete, er das System auf nicht-negative Preise einschränkte und nicht länger auf das "`Abzählen von Gleichungen"' beim Beweis der Existenz des Gleichgewichts abstellte \parencite[S. 4]{Weintraub1983}. Das Werk \textcite{Cassel1918} wurde in deutscher Sprache verfasst und war laut \textcite[S. 4]{Weintraub1983} im deutschsprachigen Raum schon vor 1930 Lehrbuchwissen. 

So richtig ins Rollen kam die Forschung zum Thema Allgemeine Gleichgewichtstheorie im Rahmen des "`Wiener Kolloquiums"', dass der Mathematiker Karl Menger - der Sohn des uns schon bekannten Ökonomen Carl Menger (vgl. Kapitel \ref{Wiener Schule}) -  in Wien ab 1928 abhielt und das als regelmäßige Treffen zwischen Ökonomen, Mathematikern und Logikern (eigentlich Philosophen) abgehalten wurde. Dieses Kolloquium dürfte die - für die Allgemeine Gleichgewichtstheorie so wichtige Verbindung zwischen Ökonomie und Mathematik - erfolgreich befruchtet haben. Die Termine fanden am Mathematik Institut der Universität Wien statt und entwickelte sich von reinen Mathematik-Veranstaltungen im Laufe der Zeit in Richtung formal-ökonomische Fragestellungen. Dies brachte den Mathematiker und Statistiker Abraham Wald und den Banker Karl Schlesinger - der nie eine akademische Position innehatte - dazu sich gemeinsam mit den Fragen des Allgemeinen Gleichgewichts zu beschäftigen. In ihren Werken \textcite{Wald1934} und \textcite{Schlesinger1934} diskutierten sie zunächst die Unzulänglichkeiten des Ansatzes von Cassel. \textcite{Wald1935} lieferte schließlich den Existenzbeweis eines eindeutigen Gleichgewichts in einem allgemeinen Gleichgewichtsmodell.

Der endgültige Durchbruch gelang aber schließlich dem Mathematik-Genie John von Neumann. Dieser wird später auch in den Kapiteln \ref{Spieltheorie} und \ref{sec: Neumann} eine gewichtige Rolle einnehmen. Er präsentierte und Veröffentlichte seinen Beitrag ebenfalls im Rahmen des Wiener-Kolloquiums \parencite{Neumann1937}, später wurde der Artikel in englischer Sprache im "`Review of Economic Studies"' abgedruckt \parencite{Neumann1945}. Der nur neun Seiten lange, dafür hoch-mathematische Beitrag wird von einigen Historikern als für die quantitative Ökonomie bahnbrechend eingestuft. \textcite[S. 13]{Weintraub1983} bezeichnete ihn als den "`allerwichtigsten Artikel in mathematischer Ökonomie"'. Er stellt die mathematisch-methodologische Grundlage der modernen Gleichgewichtstheorie dar. Aber auch für die Wachstumstheorie und die Kapitaltheorie waren die mathematischen Methoden des Artikels wegweisend. Einer Anekdote von Jacob Marschak zufolge, wiedergegeben in \textcite[S. 13]{Weintraub1983}, regte sich ein junger Mathematiker während eines Vortrages über die walrasianische Gleichgewichtstheorie von Marschak vor Physikern und Mathematikern in Berlin 1928 furchtbar über die dort verwendete mathematische Methodik auf: Anstatt eines Gleichungssystems müsse doch ganz offensichtlich ein System aus Ungleichungen verwendet werden, um ein allgemeines Gleichgewichtssystem darzustellen. Es stellte sich heraus, dass es sich bei dem entrüsteten jungen Mathematiker um John von Neumann handelte. Wahrscheinlich wurde schon damals sein Interesse an dem Thema geweckt.
Ausgangspunkt für \textcite{Neumann1937, Neumann1945} war wohl das Modell von \textcite{Cassel1918}. Davon kann man zumindest ausgehen, auch wenn von Neumann dieses Werk nicht explizit zitierte \parencite[S. 129]{Kurz1993}. In seiner Ausarbeitung führt der Mathematiker auf präzise Art und Weise die Konzepte der Dualität und Konvexitätsannahmen ein. Für seinen mathematischen Beweis der Existenz eines Gleichgewichts in seinem System aus Ungleichungen, verwendete er ein Konzept aus der Topologie und verallgemeinerte das Brouwer'sche Fixpunkt-Theorem \parencite[S. 1]{Neumann1945}. Die Existenz eines Sattelpunktes ist äquivalent mit der Existenz eines allgemeine Gleichgewichts \parencite[S. 14]{Weintraub1983}. Praktisch als Nebenprodukt daraus stellt \textcite{Neumann1945} fest, dass im allgemeinen Gleichgewicht der Zinssatz der BIP-Wachstumsrate entspricht. Ein Ergebnis, dass viele Jahre später \textcite{Phelps1961} als "`Goldene Regel der Akkumulation"' etablierte (vgl. Kapitel \ref{micmac}). Die mathematischen Konzepte aus \textcite{Neumann1945} wurden in der Ökonomie nach dem Zweiten Weltkrieg rasch als State of the Art etabliert. Das Zentrum der mathematischen Ökonomie wanderte mit dem Krieg über den Atlantik. Davor waren speziell in den USA mathematische Ökonomen die Ausnahme. Danach wuchs die Community mathematischer Ökonomen vor allem mit der Cowles-Commission, die 1932 von Geschäftsmann Alfred Cowles gegründet wurde und war in ihren Anfangsjahren vor allem von den beiden österreichischen Emigranten Gerhard Tintner und dem uns schon bekannten Abraham Wald geprägt, später von Jacob Marschak und jalling Koopmans. 1939 übersiedelte die Cowles-Commission nach Chicago. Innerhalb der Vereinigung wurden in Chicago vor allem die Themen Allgemeines Gleichgewicht und Ökonometrie beforscht, um nicht zu sagen für die folgenden Jahre geprägt. In der Cowles-Commission - allerdings zu unterschiedlichen Zeitpunkten - kamen schließlich die drei jungen Ökonomen Lionel McKenzie, Kenneth Arrow und Gerard Debreu in Kontakt mit der Allgemeinen Gleichgewichtstheorie.  

Die Krönung dieser folgte dann im Jahr 1954 durch das "`Arrow-Debreu-McKenzie-Modell"'. Meist wird es nur als "`Arrow-Debreu-Gleichgewichtsmodell"' bezeichnet, denn auch in diesem Zusammenhang tritt das Phänomen auf, dass einer der Namensgeber zu deutlich weniger Ruhm gelangte, als die anderen beiden: Die Rede ist in diesem Fall von Lionel McKenzie. Eigentlich war es nämlich \textcite{McKenzie1954}, der erstmals den bahnbrechenden Fixpunkt-Ansatz anwendete um die Existenz eines allgemeinen Gleichgewichts in einem ökonomischen Modell zu etablieren \parencite[S. 199]{Weintraub2011}. Sein Journal-Beitrag erschien in der April-Ausgabe von Econometrica im Jahr 1954, McKenzie war zu diesem Zeitpunkt noch Doktorats-Student. Der heute, zumindest unter Ökonomen, weltweit bekannte Journal-Artikel von \textcite{Arrow1954} erschien in der darauf folgenden Juli-Ausgabe des gleichen wissenschaftlichen Journals. Zwischen McKenzie und Debreu entbrannte auch tatsächlich - bereits 1952 im Rahmen eines wissenschaftlichen Kongresses, also vor der finalen Publikation der beiden Artikel - eine Diskussion darüber, welches Paper zuerst entstanden und der Öffentlichkeit zugänglich gemacht worden sei \parencite[S. 206]{Weintraub2011}. Aus heutiger Sicht ist diese Frage der "`Priorität"' laut \textcite[S. 210]{Weintraub2011} eindeutig so zu beantworten, dass es sich um eine "`gleichzeitige Entdeckung"' handelt. Etwas, das in der Wissenschaft überraschend häufig vorkommt, im konkreten Fall aber nicht sonderlich ungewöhnlich ist, da zu dieser Zeit die höhere Mathematik in den Wirtschaftswissenschaften ihren ersten Boom erlebte und mehrere junge Ökonomen zum Thema "`Allgemeine Gleichgewichtstheorie"' forschten. Im Detail ist die Bewertung der Bedeutung der beiden Artikel nicht ganz einfach. So umfasst \textcite{Arrow1954} wohl einen allgemeiner gültigen Ansatz. Nur \textcite{McKenzie1954} verwendete allerdings ursprünglich den später weitgehend etablierten "`Kakutani-Fixpunkt-Theorem"' \parencite{Kakutani1941} in seiner Beweisführung. Faktum ist auf jeden Fall, dass der Artikel \textcite{Arrow1954} wesentlich häufiger als \textcite{McKenzie1954} zitiert wird. Kenneth Arrow und Gerard Debreu erhielten außerdem - interessanterweise zu unterschiedlichen Zeitpunkten, aber doch für das gleiche Thema - den Nobelpreis für Wirtschaftswissenschaften. Die akademische Karriere von Lionel McKenzie war auch ohne diese Krönung beeindruckend. Er erhielt eine Professur an einer Universität und publizierte in seiner langen Karriere in mehreren Top-Journalen. Spät zeigten sich die Kontrahenten auch versöhnlich was das Thema der Priorität ihrer Forschungsergebnisse anbelangte. In seiner Nobel-Preis-Lecture \parencite{Debreu1983b} nannte Gerard Debreu die Arbeit von \textcite{McKenzie1954} als im Vergleich zu seiner "`unabhängig begonnen und gleichzeitig abgeschlossen"'.

Die Karriere des Franzosen Gerard Debreu verlief bekanntermaßen anders. Er kam interessanterweise das erste Mal bei den zweiten "`American Studies in Salzburg"' im Jahr 1948 in Kontakt mit dem Themenkomplex der "`Allgemeinen Gleichgewichtstheorie"' im weitesten Sinn. Im folgenden Jahr hatte er die Möglichkeit in den USA zu dem Thema zu forschen und blieb schließlich den Großteil seiner Karriere dort, wo er auch seine Hauptwerke \textcite{Arrow1954} und das umfassende mathematisch-ökonomische Buch \textcite{Debreu1959} verfasste.

Unumstritten der einflussreichste Ökonom von den dreien war aber Kenneth Arrow. Er ist uns schon aus der Wohlfahrtstheorie (vgl. Kapitel \ref{Wohlfahrt}) durch sein einflussreiches Unmöglichkeitstheorem bekannt. Außerdem begründete er ein wichtiges Konzept in der Nutzentheorie (vgl. Kapitel \ref{Finance}) und lieferte frühe theoretische Arbeiten zu den Problemen der Informationsasymmetrie (vgl. Kapitel \ref{Info}). Er kann zweifelsohne als einer der einflussreichsten Ökonomen in der Mikroökonomie nach 1945 angesehen werden. 

Was unterscheidet das "`finale"' Allgemeine Gleichgewichtstheorem nach \textcite{Arrow1954} vom bahnbrechenden mathematischen Framework von \textcite{Neumann1937}? Letztgenannter liefert quasi "`nur"' die mathematische Grundlage es Existenzbeweises, ohne auf die ökonomischen Rahmenbedingungen acht zu geben. Um den Unterschied herauszuarbeiten, müssen wir darauf eingehen, was denn überhaupt mit einem "`Allgemeinen Gleichgewicht"' gemeint ist. Ohne es bisher explizit anzuführen, sind wir immer davon ausgegangen, dass es sich um ein Marktgleichgewicht handelt, das in einer freien Marktwirtschaft erzielt wird. Das ist nicht selbstverständlich, denn man könnte auch einen zentralen Planer modellieren, der das Angebot vorgibt und so zu einem Gleichgewicht führt. In Zeiten, in denen der real existierende Sozialismus fast die halbe Welt beherrscht, ist dies nicht so abwegig. Tatsächlich gab es dazu natürlich auch Forschung, diese ist hier aber nicht abgebildet. Im Gegenteil, das Allgemeine Gleichgewichts-Modell verlangt sogar nach perfekten Wettbewerb, also vollkommene Information, homogene Güter, weder Eintritts- noch Austrittsschranken und eine unendliche Zahl von Anbietern und Nachfragern. Letzteres ist ein springender Punkt, in \textcite{Neumann1937} finden sich keine Annahmen zum Verhalten der Nachfrager, sowie der Produzenten - eben ein mathematisches Modell, nicht wirklich ein ökonomisches. Im "`Arrow-Debreu"'-Framework wird zunächst in \textcite{Debreu1952} die damals noch junge Spieltheorie - konkret \textcite{Nash1950b} - herangezogen um in Verbindung mit dem Fixpunkt-Ansatz von \textcite{Kakutani1941} zu zeigen, dass es ein "`Soziales Gleichgewicht"' \parencite[S. 90]{Debreu1983b}. Dabei handelt es sich um einen Existenz-Beweis für ein Gleichgewicht, in dem kein Marktteilnehmer einen Anreiz hat von seinen eigenen Entscheidungen abzuweichen. In \textcite{Arrow1954} wird dieses Konzept in der Folge herangezogen um eine Marktwirtschaft zu modellieren. Die Marktteilnehmer darin sind die Konsument, die Produzenten, sowie ein fiktiver Preissetzer \parencite[S. 266]{Arrow1954}, deren Verhalten an  vier formale Existenzbedingungen geknüpft ist. Konsumenten und Produzenten maximieren ihren Nutzen, bzw. Gewinn. Für die Bestimmung eines Gleichgewichts wird schließlich auf die Erkenntnisse aus der Wohlfahrtstheorie auf (vgl. Kapitel \ref{Wohlfahrt}) zurückgegriffen. Ein Gleichgewicht ist demzufolge im Pareto-Optimum erreicht, der Situation, in der niemand besser gestellt werden kann, ohne jemand anderen schlechter zu stellen. Die Existenz eines solchen Gleichgewichts - eines, das wenn es existiert, auch ein Pareto-effizientes Wettbewerbsgleichgewicht ist -  wird schließlich bewiesen \parencite[S. 274]{Arrow1954}. 

Das "`Arrow-Debreu-Allgemeine Gleichgewichtstheorem"' war in den frühen 1950er Jahren zweifellos ein bis dahin unerreichter Meilenstein der formal-mathematischen Ökonomie. Es war den neoklassischen Ansätzen (vgl. Kapitel \ref{Neoklassik}), die vor dem Zweiten Weltkrieg entstanden, in dieser Hinsicht weit überlegen. Von der mathematischen Komplexität her war es auch wesentlich anspruchsvoller als die keynesianischen Gleichgewichtsmodelle (vgl. Kapitel \ref{Synthese}), die zu dieser Zeit ebenfalls entstanden. Ihre Bedeutung außerhalb der akademischen Welt war hingegen enden wollend. Die Voraussetzung, dass alle Märkte vollkommene Konkurrenzmärkte sind, ist natürlich utopisch. In der wirtschaftspolitischen Realität war dieses mikroökonomische Allgemeine Gleichgewichtsmodell nicht anwendbar. Stattdessen begann die Hochphase der makroökonomischen, keynesianischen Gleichgewichtsmodelle, die wir bereits im letzten Kapitel kennen gelernt haben (vgl. Kapitel \ref{Synthese}).

Die Neoklassik könnte man mit der "`Allgemeinen Gleichgewichtstheorie"' als abgeschlossen betrachten. Natürlich werden in diesem Bereich bis heute Forschungsarbeiten durchgeführt, außerdem wird die Mainstream-Ökonomie häufig heute noch als Neoklassik bezeichnet. Aber die eigentliche neoklassische, mikroökonomische Wirtschafts\textit{theorie} kann mit dem Ende der 1950er Jahre als ausgereift angesehen werden. Was dann folgte waren "`Speziaformen"' der Neoklassik, wie die Neoklassische Finance (vgl. Kapitel \ref{Finance}), die Spieltheorie (vgl. Kapitel \ref{Spieltheorie}), die Teilbereiche der mikroökonomischen Wirtschafts\textit{politik}, allen voran die Marktversagensformen (vgl. Kapitel \ref{cha: Neu Keynes}), sowie die Umweltökonomie (vgl. Kapitel \ref{Umwelt}).