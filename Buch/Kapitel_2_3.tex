%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Das Ende der Ökonomie?}
\label{Neoklassik}

Die - wie wir mittlerweile wissen nur sogenannte - Marginalistische Revolution brauchte einige Zeit um sich zu etablieren. Gegen Ende des 19. Jahrhunderts setzten sich aber schließlich einige Gegebenheiten durch, die bis heute von Einfluss sind. Erstens, verlagerte sich die ökonomische Forschung fast ausschließlich in den universitären Bereich. Ökonomen wie Thünen oder Gossen, die gänzlich außerhalb des wissenschaftlichen Apparates arbeiteten, aber auch solche wie Jevons und Walras, die erst nach Tätigkeiten in der Privatwirtschaft im universitären Bereich Fuß fassten, sind seither eher die Ausnahme. Damit verlagerte sich, zweitens, die Publikationstätigkeit von gesamtheitlichen Wälzern auf hochspezialisierte Journalbeiträge. Ein Prozess, der zwar recht langsam voranschritt, die Gründungsjahre der heute noch führenden Journale deuten dies aber an: American Economic Review in 1911, das Economic Journal seit 1891, das Quarterly Journal of Economics 1886 und das Journal of Political Economy 1892 \parencite[S. 340]{Rosner2012}.
Drittens etablierte sich in der Ökonomie eine weitgehend einheitliche Sprache mit einheitlichen, fachspezifischen Ausdrücken. Einen wesentlichen Beitrag dazu leistete der britische Ökonom und Nachfolger Jevons in Cambridge Alfred Marshall.

\section{Marshall: Der Beginn der modernen Ökonomie}

\textcite{Marshall1890}: \textit{Principles of Economics - An Introductory Volume} gilt bis heute als eines der prägendsten Lehrbücher aller Zeiten. Es fasst nicht nur den State of the Art der Ökonomie zusammen, sondern erweiterte denselben auch. Der durchschlagende Erfolg dieses Buches hängt sicherlich auch damit zusammen, dass sich die mikroökonomische Theorie seit damals kaum mehr verändert hat. Natürlich wurde sie entscheidend und an vielen Stellen erweitert. Aber die damals schon bestehenden Theorien zur Mikroökonomie gelten bis heute unverändert und sind tatsächlich in einführenden Lehrbüchern praktisch identisch abgedruckt.

Was sein Privatleben anging, stammte Marshall aus einer hoch-religiösen Familie und war in seiner Kindheit vom "`tyrannischen"' Vater \parencite[S. 313]{Keynes1924} geprägt. Gegen den Willen seines Vaters und mit der Hilfe eines Darlehens von seinem Onkel studierte er Mathematik in Cambridge. Nach dem Abschluss seines Studiums nahm er 1868 bereits seine Lehrtätigkeit dort auf. Er publizierte in seinen frühen Jahren wenig, obwohl er viel verfasst und sich auf Reisen auch viel Wissen in den USA und Kontinental-Europa aneignete. 1877 musste er Cambridge verlassen, weil er eine ehemalige Studentin heiratete. 1885 aber konnte er zurückkehren. Ab dann stieg er rasch zum führenden englischen Ökonomen auf. Bereits 1879 hatte sein Mentor Henry Sidgwick einige seiner Werke veröffentlicht, die viel Aufsehen erregten. 1890 folgte sein Hauptwerk, die "`Principles of Economics"'. In weiterer Folge war er auch als Berater öffentlicher Stellen in England tätig. Ständig erweiterte er nebenbei seine "`Principles"'. Als Person, die empfindlich auf Kritik reagierte, wurde er zum "`Workaholic"', was schließlich seine Gesundheit angriff. 1908 wurde er emeritiert. In seinen letzten eineinhalb Lebensjahrzehnten publizierte er weiter, getrieben von seinem Perfektions-Drang \parencite[S. 145f]{Rieter1989}. Die inhaltliche Bedeutung seiner späten Arbeiten blieb aber beschränkt.

Mit Marshall änderte sich der Blick auf die Ökonomie grundlegend. Er gilt daher nicht umsonst als der "`Vollender der Neoklassik"'. Neben inhaltlicher Punkte, revolutionierte Marshall vor allem auch die ökonomische Methodologie und allgemein das Bild der Ökonomie als Wissenschaft:

Erstens, zunächst besticht sein Hauptwerk durch die Kombination von sprachlicher Verständlichkeit und mathematischer Präzision. Vergleicht man sein Werk mit jenen von, zum Beispiel, Menger, Walras oder Böhm-Bawerk, so merkt man sofort deutliche Unterschiede in Aufbau, sowie ein bessere Verständlichkeit. Dies ist zwar nicht unbedingt ausschließlich der Arbeit Marshalls eigen, sondern eher dem Zeitgeist zuzuschreiben und so auch zum Beispiel bei Irving Fisher zu finden, aber es besticht dennoch in \textcite{Marshall1890}. 

Zweitens, Marshall grenzte als erster die Volkswirtschaftslehre ("`Economics"') als Wissenschaft von den übrigen Sozialwissenschaften ab. Dazu "`kürzte"' er die politischen Entscheidungs- und Machtstrukturen aus seinem wirtschaftswissenschaftlichem Werk. Die bis dahin übliche Bezeichnung der "`Politischen Ökonomie"', wurde zur "`Volkswirtschaftslehre"'. Dies war für sein Werk notwendig. Die von ihm so elegant durchgezogene formale Herangehensweise ist nur dann möglich, wenn man Nebenbedingungen definiert und diese als gegeben annimmt. Dies ist eine der Stärken der Neoklassik. Es brachte ihr aber auch harsche Kritik ein, weil sich die Volkswirtschaftslehre damit von der erlebten Realität ein gutes Stück entfernt. Diese Kritik ist im Falle Marshalls ungerechtfertigt. Er war sich durchaus bewusst, dass seine "`Principles"' eine zu starke Vereinfachung der wirtschaftlichen Realität darstellen. Dementsprechend plante er, ja rang förmlich, um die Entstehung einer Fortsetzung seines Werkes \parencite[S. 146]{Rieter1989}. Tatsächlich ließ er erst mit der sechsten Auflage der "`Principles of Economics"' den Zusatz "`Volume I"' streichen. Mit 80 Jahren - 1920: "`Industry and Trade"', sowie 1923: "`Money Credit and Commerce"' - publizierte er schließlich seine Vorarbeiten zu weiteren Bänden, diese können aber nur mehr als fragmentierte Beiträge zu einzelnen Themenkomplexen gesehen werden. Die Abgrenzung der Volkswirtschaftslehre betrieb Marshall nicht nur inhaltlich, sondern auch organisatorisch. An der Universität von Cambridge setzte er durch, dass sie aus der Fakultät für "`Moral Science"' herausgelöst und stattdessen eine eigene Fakultät, mit eigenem Studiengang wurde \parencite[S. 141]{Rieter1989}. Nicht nur \textcite[S. 365]{Keynes1924} bezeichnete Marshall daher als "`Begründer der Cambridge School of Economics"'.

Drittens, Methodisch gilt die Arbeit Marshall's heute vielen als bahnbrechender als seine inhaltlichen Beiträge. Er führte die Ceteris-Paribus-Betrachtung in die Ökonomie ein. Also die Auswirkungen der Änderung eines einzelnen Einflussfaktors auf das Gesamtergebnis. Diese Betrachtung von statischen Gleichgewichten blieb in der Ökonomie lange der Standard. Marshall selbst sah darin einen Ausgangspunkt, war sich aber bewusst, dass eine dynamische Betrachtung besser wäre, aber in Modellen auch schwerer zu erfassen \parencite[S. 153]{Rieter1989}.

Viertens, Er war als einer der ersten Ökonomen extrem gut informiert über den "`State of the Art"' der Ökonomie. Heute ist es unumgänglich die wissenschaftlichen Arbeiten des Forschungszweiges, in welchem man selbst publizieren möchte, umfänglich zu kennen. Im 19. Jahrhundert allerdings sorgten Probleme der Sprache, Verfügbarkeit und bloßen Kenntnis dafür, dass oft Theorien entwickelt wurden, ohne dass die Urheber den Stand der Wissenschaft kannten. Wir erinnern uns zum Beispiel an Gossen. Aber auch Walras verfasste zunächst sein Hauptwerk und trat erst gegen Ende seiner Karriere in intensiven Austausch mit zeitgenössischen Ökonomen. Ganz anders war dies bei Marshall: Wie \textcite{Groenewegen1995} beschreibt, beschäftigte sich dieser bereits in den späten 1860er Jahren mit den deutschen Ökonomen wie Thünen, Roscher und Rau und kannte die Arbeiten der Franzosen Cournot und Dupuit. Selbstverständlich waren ihm die englischen Klassiker bekannt, aber auch die Historische Schule der Deutschen war ihm nicht fremd \parencite[S. 140]{Rieter1989}. Aufbauend auf all dem Wissen publizierte er sein Hauptwerk. Dies verhältnismäßig spät mit 48 Jahren im Jahr 1890. Die Erkenntnisse waren zu dieser Zeit bereits allesamt weitgehend bekannt und zirkulierten als Mitschriften aus seinen Vorlesungen. Er zitierte in den "`Principles"' auch nur wenig seinen Zeitgenossen Jevons, sondern eben vor allem die "`Vorläufer"'. Es gilt auch bis heute als umstritten, ob Marshall tatsächlich primär der \textit{Entwickler} bedeutender ökonomischer Theorien ist, oder doch eher der \textit{Vereiniger} ökonomischer Elemente, die bereits jeweils jemand anderer entwickelt hatte \parencite[S. 207ff]{Ekelund2002}. Marshall selbst behauptete in hohem Alter, dass er den Rahmen für seine "`Principles"' schon vor 1871, also dem Erscheinungsjahr der Arbeiten von Jevons und Menger, fertig hatte und er aus diesem Grund mehrheitlich die "`Vorläufer"' und kaum das Trio Jevons, Menger und Walras, zitiert hatte \parencite[S. 140]{Rieter1989}. 

Erst jetzt kommen wir zu seinen inhaltlichen Beiträgen: Allseits bekannt ist sicherlich das "`Marshall'sche Kreuz"', also seine Darstellung von Angebot und Nachfrage und dem daraus entstehenden Gleichgewicht. Darüber ob Marshall damit eine bahnbrechende Leistung erbracht habe, diskutierten Generationen von Ökonomen. Bekannt ist, dass die erste Darstellung von Angebot und Nachfrage als sich schneidende Kurven um das Jahr 1840 entstand und auf Antoine-Augustin Cournot zurückgeht \parencite[S. 3]{Humphrey1992}, oder eventuell von Karl Rau sogar schon noch etwas früher so dargestellt wurde \parencite[S. 159]{Blaug2001}. Definitiv auf Marshall geht damit aber die Verbindung zwischen marginalistischer Nachfragefunktion und eher klassischer Angebotsfunktion zurück. Die negativ verlaufende Nachfragefunktion leitet Marshall aus dem sinkenden Grenznutzen beim Konsum von Gütern ab. Die steigende Angebotsfunktion von den mit steigender Menge steigenden Produktionskosten. Dies findet sich bereits bei Ricardo. Wenn dieser auch davon ausging, dass die Produktionskosten deshalb stiegen, weil knappe Ressourcen in abnehmender Qualität zur Verfügung stehen würden.

Quasi als Nebenprodukte seiner Herleitung von Nachfrage- und Angebotsfunktion, schuf er Instrumente, die heute noch gängige Praxis sind. So zum Beispiel die Elastizität, die er konkret als relative Änderungen der nachgefragten Menge bei Änderung des Preises definiert. Schon Marshall führt ein Beispiel von Robert Giffen an, der darlegte, dass die Brot-Nachfrage in Irland im 19. Jahrhundert trotz steigender Preise anstieg. Bis heute lernt man das seltene Phänomen, dass trotz steigender Preise die Nachfrage steigt, als Giffen-Paradoxon (bzw. Giffen-Gut) kennen. Ebenfalls direkt auf \textcite{Marshall1890} geht die Analyse der Produzenten- und Konsumentenrente in heute üblicher Form zurück. Also die Differenz zwischen Preis, zu dem ein Produzent anbieten würde und dem Marktpreis, bzw. dem Preis, den ein Nachfragender bereit wäre zu bezahlen und dem Marktpreis. Marshall behandelte auch schon die Auswirkungen von Steuern auf diese die Konsumentenrente \parencite[S. 351]{Rosner2012}. 

Das Gesamtwerk Marshalls bildet bis heute die Grundlage der Mikroökonomie. Im idealisierten Bild des vollkommenen Marktes, auf dem vollständige Konkurrenz herrscht, ohne Zugangsschranken, mit streng nutzen-maximierenden Teilnehmern und ohne jegliches Marktversagen, gilt noch heute: "`It's all in Marshall!"'\footnote{Ein Zitat, das Arthur C. Pigou zugeschrieben wird, aber wohl nur sinngemäß tatsächlich geäußert wurde. \parencite{Audretsch2007, Pigou1925}}. Tatsächlich war die ursprüngliche "`neoklassische Theorie"' mit Marshall gewissermaßen abgeschlossen, wenn man dies in dem Sinne versteht, dass seine Erkenntnisse wie in seinem Werk "`Principles of Economics"' auch in modernen Mikroökonomie Büchern praktisch identisch dargestellt werden\footnote{Wichtige Ergänzungen etwa im Hinblick auf die Nutzendarstellung kamen etwas später noch von Vilfredo Pareto und Francis Edgeworth.}. Diese "`Vollendung der Neoklassik"' kann deshalb vielleicht als Ausgangspunkt der \textit{modernen} Ökonomie gelten.

Wie soeben dargestellt: Welcher Umfang von bedeutenden Beiträgen erst durch Marshall bekannt wurde, ist unumstritten, worin seine bahnbrechende Leistung nun tatsächlich lag, kann hingegen nicht eindeutig beantwortet werden. Nicht durch die \textit{eine} bedeutende Leistung gilt Marshall als einer der bedeutendsten Ökonomen, sondern durch sein Gesamtwerk. Wie \textcite{Rieter1989} es ausdrückt: "`Man empfindet ihn als Ganzes [...]. Ein komfortabler Neubau, errichtet auf alten Fundamenten."' 

Marshall war von verschiedensten Richtungen Kritik ausgesetzt. Wenig überraschend lehnten ihn die Sozialisten praktisch einfach grundsätzlich ab. Aber auch seine Zeitgenossen aus der "`Historischen Schule"'(vgl. Kapitel \ref{Historisch}) kritisierten ihn heftig, vor allem für die unrealistischer Annahmen, die für seine Modell notwendig sind und die gesetzmäßigen Zusammenhänge, die diese Modelle liefern. Die Amerikaner um Veblen (vgl. Kapitel \ref{Institut}) kritisierten, dass Marshall das Marktwirtschaftliche Wirtschaftssystem implizit als effizientes und gerechtes System akzeptierte. Sogar die österreichische Schule hielt wenig von Marshall's Lehre \parencite[S. 151]{Rieter1989}. Die Kritik traf Marshall hart und ungerechtfertigt. Er selbst stellte "`wirkliche"' ökonomische Probleme in den Vordergrund, hielt wenig vom starren Rationalprinzip und nannte die Bekämpfung der Armut als zentrales Ziel der Volkswirtschaftslehre \parencite{Rieter1989}. Heute wissen wir, dass sich Marshall's Lehren, entgegen aller Kritik, als Mainstream durchgesetzt haben. Interessant ist in diesem Zusammenhang, dass die Kritik an der Neoklassik bis heute eine ähnliche geblieben  ist. Auf wissenschaftlich stärker fundierter Ebene wurde und wird nach wie vor primär die Realität verschiedener Modellannahmen in Zweifel gezogen. Auch heute gibt es kaum einen Ökonomen, der die Modellannahmen für 100\% richtig hält und die gesetzmäßige Gültigkeit der Modellergebnisse als gegeben annimmt. Aber auf der anderen Seite hat sich bis heute in der Mikroökonomie keine umfassende Alternative zur Neoklassik durchgesetzt.  

Die moderne ökonomische Forschung \parencite{Ekelund2002, Blaug2001, Humphrey1992} sieht Marshall eher als "`Synthesizer"', denn als Entwickler, \parencite[S. 212]{Ekelund2002} der ökonomischen Theorien zum neoklassischen Gesamtwerk. Aber er gilt auch als Entwickler der modernen wirtschaftswissenschaftlichen Methodik. So schuf er den Rahmen für die lange vorherrschende statisch-komparative Analyse und verband induktive Theoriebildung mit deduktiver empirischen Überprüfung \parencite[S. 212]{Ekelund2002}. Insgesamt war er auf jeden Fall \textit{die} prägende Figur der frühen Neoklassik im England des späten 19. Jahrhunderts. Seine Arbeiten sind wohl die frühesten, die noch heute fast unverändert Teil der Mainstream-Ökonomie sind. Konkret wenn es um die mikroökonomische Analyse auf vollständigen Konkurrenzmärkten geht. 

Wir wissen aber natürlich, dass sich die Wirtschaftswissenschaften seither vielfältig weiterentwickelt haben. Sein Nachfolger in Cambridge, Arthur C. Pigou, machte sich als einer der ersten Gedanken über "`Marktversagen"', also Situationen, in dem eine rein rationale-mikroökonomische Analyse unerwünschte Marktergebnisse zum Vorschein bringt (vgl. Kapitel \ref{sec: Pigou}. Die Verbindung seiner Arbeiten mit der Gleichgewichtstheorie von Walras (vgl. Kapitel \ref{Arrow-Debreu}) und nicht zuletzt die neoklassische Wachstumstheorie (vgl. Kapitel \ref{sec: Solow-Modell}), sind Forschungsgebiete, die die Marshall'sche Neoklassik nach 1945 wesentlich weiterentwickelten.


\section{Edgeworth und Pareto: Die Lösung des Nutzenproblems, die bis heute hinkt}

Fassen wir zusammen: Eines, wenn nicht \textit{das}, die Neoklassik auszeichnende Element, ist das Nutzenkonzept, bzw. die Theorie des abnehmenden Grenznutzens. Dieses Konzept ist an und für sich intuitiv verständlich und leicht nachzuvollziehen: Nach einer langen Wanderung liefert mir das erste Bier einen enormen Nutzen, das fünfte Bier liefert ebenfalls einen Nutzen, doch ist dieser zweifelsohne deutlich geringer. Alleine dieses Konzept des abnehmende Grenznutzens lässt eine grafische Transformation von Geldeinheiten in Nutzeneinheiten schon zu: Der erste Euro liefert den höchsten Nutzen, der zweite einen etwas geringeren, der dritte Euro eine wiederum etwas geringeren, usw. Wenn man den Nutzen auf der y-Achse und die Geldeinheiten auf der x-Achse abträgt, erhält man eine Funktion die vom Ursprung ausgehend durchgehend einen positiven Anstieg aufweist. Der Anstieg verläuft dabei aber immer flacher. Vorausgesetzt eine Nutzenfunktion erfüllt diese Anforderung, dann können Geldeinheiten einfach in Nutzeneinheiten "`umgerechnet"' werden. Der entsprechende Nutzen wird dann in Zahlen ausgedrückt. Man spricht vom "`kardinalen Nutzenprinzip"'. Genau hier liegt aber ein Problem, das die frühen Neoklassiker\footnote{Walras und Menger behandelten das Problem tatsächlich nicht, Jevons meinte zwar, Nutzen sei nicht direkt messbar, er akzeptierte aber den Umweg über Geldeinheiten. Der Nutzen verschiedener Güter kann demnach im äquivalenten Geldwert ausgedrückt werden.} schlicht ignoriert haben \parencite[S. 328]{Blaug1962}: Nutzen ist in Wirklichkeit nicht direkt messbar. Es gibt keine sinnvolle Einheit in der man Nutzen quantifizieren könnte. Dementsprechend sind Rechenoperationen mit Nutzeneinheiten sinnlos.

Der erste, der dies ausführlich thematisierte war Francis Ysidro Edgeworth. Er war sowohl mit Stanley Jevons als auch mit Alfred Marshall befreundet und auch ein früher Verfechter der Mathematik in der Ökonomie. In seinen "`Mathematical Psychics"' \parencite{Edgeworth1881} kritisierte er, dass die Neoklassiker Nutzen unzulässigerweise als quantitative Variable behandelten. \textcite{Edgeworth1881} schlug vor nach Wegen zu suchen, den Nutzen tatsächlich direkt zu messen. Er verfolgte also auch ein kardinales Nutzenkonzept. Dazu wollte er einen "`Hedonimeter"' entwickeln, also ein Messgerät, dass den Nutzen direkt messen kann.  Übrigens verfolgte wenig später auch der junge Irving Fisher - der uns noch mehrmals unterkommen wird - in seiner Dissertation das Ziel einer kardinalen Nutzenmessung, allerdings schlug er vor diese indirekt vorzunehmen, also von getätigten Handlungen auf deren Nutzen zu schließen\parencite{Colander2007}. Das Konzept blieb schließlich in der Ökonomie ohne wesentliche Resonanz\footnote{Moderne Ansätze der Neuro-Ökonomie gehen allerdings wieder in Richtung kardinaler Nutzenmessung. Dabei wird in Magnetresonanz-Tomographen versucht Gehirnströme hinsichtlich Glücksgefühle zu messen}. Allerdings lieferte \textcite{Edgeworth1881} dennoch wichtige Bausteine für die Nutzentheorie. So entwickelte er darin das Konzept der - heute in der Ökonomie-Lehre nach wie vor omnipräsenten und ebenso beliebten - Indifferenzkurven. Abgeleitet können diese aus einer, wie oben beschriebenen, Nutzenfunktion. Interessant sind die Indifferenzkurven im Zwei-Güter-Fall. Angenommen ich bilde in einem Koordinatensystem die Menge von Gut A auf der x-Achse und die Menge von Gut B auf der y-Achse ab. Wenn ich mein ganzes Geld für Gut A ausgebe erreiche ich einen bestimmten Punkt direkt auf der x-Achse (und vice versa). Aus dem Konzept des abnehmenden Grenznutzens wissen wir, dass eine Güterkombination aus A und B gegenüber nur A (oder nur B) vorteilhaft ist. Oder mit anderen Worten: Ich bekomme für eine geringere Geldmenge, die ich für eine Güterkombination ausgebe den gleichen Nutzen, wie für ein höhere Geldmenge, die ich ausschließlich nur für A (oder nur für B) ausgebe. Verbindet man alle Güterkombinationen aus A und B, die den identischen Nutzen liefern, miteinander spricht man von einer Indifferenzkurve. Diese beginnt jeweils an einem Punkt auf der x- und y-Achse und ist zum Ursprung geneigt \parencite[S. 21ff]{Edgeworth1881}. Bildet also eine Linkskurve ab, bzw. verläuft konvex. \textcite{Edgeworth1881} beschreibt in weiterer Folge, wie zwei Personen miteinander über das Austauschverhältnis dieser zwei Güter verhandeln. Man stelle sich nun das soeben beschriebene Koordinatensystem mit zwei Personen vor. Zusätzlich zur Person A, deren Ausgangspunkt der Ursprung, also "`links unten"' ist, eine zweite Person B, deren Ausgangspunkt "`rechts oben"' ist. Seine Indifferenzkurven verlaufen spiegelverkehrt zu jenen von A, das heißt diese sind zum Punkt "`rechts oben"' geneigt. B hält in diesem Fall alle Güter 1 (aber kein 2), A hält die gesamte Menge 2 (aber keine 1). Beide wollen nun in einen Tauschprozess kommen. Mögliche "`Tauschpunkte"' sind überall dort wo sich die Indifferenzkurven der beiden Personen schneiden. Ein Tauschgleichgewicht und gleichzeitig eine maximale aggregierte Wohlfahrt (welfare) wird aber bei Edgeworth nur in einem Punkt erreicht, nämlich wo sich die Indifferenzkurve von A und B genau tangieren. Dies ist aber eine falsche Annahme seitens Edgeworth - es gibt mehrere Tangentialpunkte und vor allem keine Möglichkeit eine "`allemeines Nutzenmaximum"' zu identifizieren \parencite[S. 49]{Humphrey1996}. Das soeben beschriebene Tool hat dennoch extreme Bedeutung erlangt ist heute als die "`Edgeworth-Box"' bekannt\footnote{Edgeworth selbst stellte die Box leicht abweichend dar, nämlich mit den Personen A und B "`rechts unten"', bzw. "`links oben"'. Details zur reichhaltigen Geschichte der Edgeworth-Box sind in \textcite{Humphrey1996} dargestellt}. Sie ist ein wichtiges Element in der allgemeinen Gleichgewichtstheorie zu der wir gleich wieder kommen werden. In \textcite{Edgeworth1881} sind beide Konzepte, also Indifferenzkurven und Edgeworth-Box, mathematisch und verbal beschrieben. Bekannt gemacht und angewendet hat beides schließlich Vilfredo Pareto, wobei er der erste war, der beide Konzepte grafisch wie heute üblich darstellte.

Was Marshall nicht beachtete waren die Fragen nach dem Allgemeinen Gleichgewicht im Sinne Walras'. Laut \textcite[S. 360]{Rosner2012} war Marshall das Werk von \textcite{Walras1874} zwar bekannt, aber er hatte ihm offenbar nicht den Stellenwert beigemessen, des es später erhalten sollte. Damit die Verbindung zu einem weiter wichtigen Wegbereiter der "`älteren"' Neoklassik vollends hergestellt: Vilfredo Pareto. Interessanterweise, die Gründe sind unbekannt, wurde er auf den deutschen Namen Fritz Wilfried getauft \parencite[S. 158]{Eisermann1989}. Bekannt wurde der Sohn eines nach Frankreich emigrierten Italieners und einer Französin allerdings unter dem Namen Vilfredo. Er war zunächst als Techniker tätig, veröffentlicht aber immer wieder Artikel in einem italienischen, ökonomischen Journal. Er wird daraufhin, von einem italienischen Wirtschaftsprofessor empfohlen, 1893 nach Lausanne als Nachfolger des kränklichen Walras berufen. Mit Walras überwirft er sich in weiterer Folge allerdings rasch. Als Person wird er als radikaler Liberaler bezeichnet \parencite{Cirillo1983}, seine wissenschaftliche Herangehensweise als streng logisch-deduktiv.  Pareto's Werk ist insgesamt geprägt von seiner technischen Ausbildung. Seine Arbeiten behandeln recht enge ökonomische Themen. Insgesamt ähnelt sein wissenschaftlicher Stil damit bereits jenem der modernen Wirtschaftswissenschaften \parencite[S. 362]{Rosner2012}. Politisch wird er häufig als Vorläufer des Faschismus bezeichnet, da er dessen Aufstieg begrüßt haben soll. Vor alle sein soziologisches Werk - Pareto veröffentlichte neben wirtschaftswissenschaftlichen auch bedeutende soziologische Arbeiten - wurde häufig mit dem Faschismus in Verbindung gebracht. Dies wurde in der Literatur häufig diskutiert. \textcite[S. 162]{Eisermann1989} - offenbar ein großer Bewunderer Paretos - argumentierte dieser habe den Faschismus stets abgelehnt, vor allem mit dem Verweis er wäre ein großer Liberaler gewesen, dessen logisch deduktiver Zugang so weit gegangen sei, \textit{jegliche} politische Entwicklung stets nur als distanzierter Beobachter zu analysieren. \textcite{Cirillo1983} betrachtet Pareto's politische Haltung durchaus kritischer, so war dieser zweifellos kein Anhänger der Demokratie. Aber auch er meint, die italienischen Faschisten, insbesondere Mussolini selbst, seine Anhänger Pareto's gewesen und wollten sein Werk für sich vereinnahmen. Eine endgültige "`Wahrheit"' kann wohl auch hierzu nicht ermittelt werden. Aus wirtschaftswissenschaftlicher Sicht lieferte Pareto auf jeden Fall in mehrere Gebieten bedeutende Beiträge. Interessant ist, dass einige seiner Entdeckungen heute über die Wirtschaftswissenschaften hinaus verwendet werden und weithin bekannt sind. So wird er wohl in der Allgemeinheit am ehesten mit dem  "`Pareto-Prinzip"' in Verbindung gebracht: Diese 80-20-Regel, wonach 80\% der Leistung durch 20\% des Aufwands erbracht werden, lässt sich auf seine Untersuchung der Einkommensverteilung in Italien zurückführen \parencite{Pareto1896}. Er fand heraus, dass diese nicht normalverteilt, sondern rechtsschief ist. Die von ihm abgeleitete Verteilung wird seither als "`Pareto-Verteilung"' bezeichnet und spielt, nicht zuletzt durch das wieder aufgeflammte Interesse an Fragen der Einkommensverteilung\parencite{Persky1992}, heute noch eine große Rolle in Verteilungsanalysen.

Sein aus wirtschaftshistorischer bedeutendster Beitrag folgte aber ein Jahrzehnt später. In seiner heute als "`Theorie der Wahlakte"' bezeichneten hat Pareto das hinkende Nutzenkonzept aufgegriffen und in Verbindung mit den Elementen von Edgeworth auf gänzlich neue Beine gestellt. Wie gesagt sind Jevons, Walras, Menger aber auch noch Marshall implizit von einem kardinalen Nutzenkomzept ausgegangen. Betrachtet man ein einzelnes Gut, so spielt diese Betrachtung keine große Rolle, weil es egal ist, ob man dem Nutzen in diesem Fall einen numerischen Wert zuweist. Betrachtet man aber ein Güterbündel führt die numerische Betrachtung des Nutzens zu Problemen. Es verleitet nämlich dazu anzunehmen, die Nutzenwerte der einzelnen Güter im Bündel ließen sich zum Beispiel zu einem Gesamtnutzen addieren. Dies macht aber keinen Sinn, weil Nutzen eben nicht direkt gemessen werden kann. Edgeworth hatte dieses Problem erkannt und mit der Entwicklung von Indifferenzkurven wichtige Vorarbeiten geleistet. Er arbeitet aber daran einen Weg zu finden, wie man Nutzen doch direkt messen kann und dann gültig in Zahlen ausdrücken kann. \textcite{Pareto1906} griff nun die Idee der Indifferenkurven auf, wendete sie aber praktisch "`aus einer anderen Richtung kommend"' an: Wir wissen, dass auf einer Indifferenzkurve liegende Punkte jeweils denselben Nutzen liefern. "`Höher"' liegende Indifferenzkurven liefern aber immer einen höheren Nutzen, als "`tiefer"' Indifferenzkurve. Diese Information aus Indifferenzkurven ist bei Pareto die einzig gültige und die einzig wichtige. Der Nutzen wird also nicht mehr kardinal gemessen, sondern ordinal: Indifferenzkurven können nur hinsichtlich ihrer Rangfolge sortiert werden. Man kann aber nach wie vor bestimmen welches Güterbündel besser als ein anderes ist, verwendet dazu aber keine irreführenden Zahlenwerte für den Nutzen mehr. Pareto revolutionierte mit der Einführung des ordinalen Nutzen das Nutzenkonzept nachhaltig, es wird bis heute in ökonomischen Modellen verwendet. In \textcite{Pareto1906} wurde übrigens auch erstmals der Begriff "`homo \oe conomicus"' verwendet.

Den ordinalen Nutzen führte Pareto in die "`Edgeworth-Box"' über, die durch seine Art der Darstellung schließlich popularisiert wurde. Darin kommt es zu einem Güteraustausch, wenn die Indifferenzkurven von zwei Individuen sich tangieren. Dies lässt sich wiederum verallgemeinern zum Haushaltsoptimum: Das Verhältnis der Grenznutzen zweier Güter ist gleich dem Preisverhältnis\footnote{Dies kennen wir schon aus dem Kapitel \ref{Vorläufer}, entscheidend ist die Herleitung über das ordinale Nutzenkonzept.}. Nebenbei wird durch diese Herangehensweise auch das bis heute häufig verwendete "`Pareto-Effizienz"' begründet: Wenn niemand besser gestellt werden kann, ohne gleichzeitig jemand anderen schlechter zu stellen. Dies wird häufig als der Ausgangspunkt der "`Wohlfahrtsökonomie"', mit der sich später Arthur Cecil Pigou explizit ausführlich beschäftigte (vgl. Kapitel \ref{sec: Pigou}). 

Man könnte nun davon ausgehen, dass Pareto's Nutzenkonzept in der neoklassischen Theorie sofort zum "`State of the Art"' wurde. Dem ist aber nicht so. Stattdessen verschwand die Diskussion über das Nutzenkonzept für längere Zeit \parencite[S. 148]{Blaug2001} aus dem Fokus der ökonomischen Forschung. Zu einer erneuten Überarbeitung kam es erst in den 1930er Jahren. \textcite{Hicks1934b} bzw. \textcite{Hicks1934a} griffen ausdrücklich \textcite{Pareto1906} noch einmal auf und deckten kleinere mathematische Inkonsistenzen darin auf. Die wesentliche Änderung war, dass bei der Bestimmung des Haushaltsoptimums das "`Verhältnis der Grenznutzen"' zweier Güter durch deren "`Austauschverhältnis"' ersetzt wurde. Der "`Abnehmende Grenznutzen"' verliert dadurch an Bedeutung und wird durch das Konzept der "`Grenzrate der Substitution"' ersetzt. Eine im wesentlichen rein formale, weniger eine inhaltliche Änderung. Die Frage, die nun beantwortet wird lautet: \textit{Welche Menge an X kompensiert meinen Verlust der letzten Einheit Y?} statt: \textit{Das wievielte Stück X liefert mir den gleichen Nutzen wie das wievielte Stück Y?}.

Das neoklassische Nutzenkonzept fand damit einen Abschluss. Es wird in der Form wie von \textcite{Hicks1934b} dargestellt auch in modernen Mikroökonomie-Büchern noch verwendet. Fazit: Für die Gleichgewichtsfindung benötigt man keine kardinale Nutzenmessung. Es reicht das Konzept des ordinalen Nutzen, in dem man nur eine Rangordnung der individuell präferierten Güterbündel angeben kann, aber keinerlei Aussagen treffen kann \textit{um wie viel höher} der Nutzen von Güterbündel A gegenüber jenem von B ist. Diese Erkenntnis war auch wichtig für die nach dem Zweiten Weltkrieg aufkommende Forschung zur Theorie des "`Allgemeinen Gleichgewichts"' (vgl. Kapitel \ref{Arrow-Debreu}). 

Es gibt bis heute Stimmen, die meinen die ordinale Nutzenmessung sei nur eine Notlösung. Wie wir wissen ist das gesamte Konzept des homo \oe conomicus fortlaufender Kritik aus verschiedenen Richtungen ausgesetzt. Dem kann man entgegenhalten, dass jeder Versuch einer kardinalen Nutzenmessung bislang gescheitert ist. Ein besseres Konzept ist bislang nicht greifbar. Aber man kann durchaus die damit verbunden Konsequenzen durchdenken: Bei ordinalen Nutzen macht zum Beispiel die Betrachtung der Konsumentenrente keinen Sinn \parencite[S: 400]{Rosner2012}. Schließlich handelt es sich dabei um die Summe der Nutzen verschiedener Personen. Noch einschneidender: Sämtliche Aussagen zur gesamtwirtschaftlichen Wohlfahrt haben damit keine Grundlage. Wenn interpersonelle Nutzenvergleiche unzulässig sind, macht es keinen Sinn darüber zu diskutieren, ob Verteilungspolitik zu Verbesserungen führen. Demnach lässt sich nicht sagen, ob 100EUR einer alleinerziehenden Mutter mehr Nutzen stiften als dem wohlhabenden Millionär. Die Neoklassik mit ihrem ordinalen Nutzenkonzept lässt diesbezüglich keine wissenschaftlich fundierte objektive Aussage zu, es handelt sich immer Werturteile. Wirtschaftspolitisch ist dies natürlich unbefriedigend. Man könnte damit sogar in die Leibnitz'sche Interpretation verfallen, wir lebten in der besten aller Welten: Wäre nicht jedes Individuum in seinem Nutzenmaximum, würde kein Gleichgewicht bestehen und die Besitzverhältnisse würden sich ändern. Mit Blick auf die Realität kann dies aber nur eine zynische Aussage sein.

In einem Bereich wurden Nutzenfunktionen noch später neu definiert. Ohne es zu erwähnen handelte es sich bisher stets um die Abbildung von Nutzen unter Sicherheit. Das heißt, wenn ich Gut A gegen Gut B tausche, weiß ich, dass ich tatsächlich das mir bekannte Gut B in gewohnter Qualität erhalte. In einige Bereichen, vor allem in der Finanzwissenschaft, wurde nach dem Zweiten Weltkrieg eine Nutzentheorie unter Risiko von nöten. Hier reicht eine ordinale Nutzenbetrachtung nicht aus. \textcite{VonNeumann1944} zeigten allerdings, dass sich unter gewissen Voraussetzungen aus einer Lotterie ein kardinaler Nutzen ableiten lässt. Dies wird in Kapitel \ref{Erwartungsnutzen} detailliert dargestellt.

\section{Fisher and Clark: Economics goes USA}

Die Volkswirtschaftslehre wird heute - mehr noch als andere Disziplinen - von US-amerikanischen Beiträgen dominiert. Vor allem mit dem Beginn des Aufstiegs des Faschismus in Kontinentaleuropa und durch den Zweiten Weltkrieg erfuhr die Verlagerung der Wirtschaftswissenschaften von Europa in den Angel-sächsischen Raum Auftrieb. Was die Zeit vor 1900 angeht, ist es aber doch überraschend, dass in den USA praktisch keinerlei, aus heutiger Sicht, bedeutende Beiträge entstanden. Vor allem wenn man beachtet, dass die heute führenden Journale schon deutlich vor 1900 in den USA gegründet wurden:  Quarterly Journal of Economics 1886, das Journal of Political Economy 1892. Die American Economic Association wurde 1885 gegründet und publiziert seit 1911 den American Economic Review. Tatsächlich waren die - wiederum aus heutiger Sicht - ersten bedeutenden amerikanischen Ökonomen John Bates Clark und Irving Fisher.

Clark ist heute vor allem durch die John-Bates-Clark-Medaille bekannt, die mittlerweile jedes Jahr - bis 2009 war der Vergaberhythmus zwei Jahre - an einen herausragenden, in den USA tätigen Ökonomen oder eine Ökonomin unter 40 Jahren vergeben wird. Er wurde 1946 in den USA geboren und ging nach dem Abschluss seines Studiums nach Europa, wo er allerdings vor allem mit sozialistischen Ideen und der Historischen Schule in Kontakt kam. So war Karl Knies, ein Vertreter letztgenannter Schule, einer seiner Professoren, der ihn laut \textcite{Tobin1985} auch stark beeinflusste. Nach seiner Rückkehr in die USA vertrat Clark dann auch Kapitalismus-kritische Positionen. In \textcite{Clark1886} kritisierte er die Klassiker um Ricardo und auch deren Individualismus und die Verherrlichung des Wettbewerbsgedanken \parencite[S. 29]{Tobin1985}. Erst in weiterer Folge änderte er seine Ansichten um 180 Grad und wurde zu einem der führenden Neoklassiker jener Zeit. Dies ist bemerkenswert, da er zu diesem Zeitpunkt schon an die 50 Jahre alt war. Bekannt wurde er schließlich durch die "`Grenzproduktivitätstheorie der Einkommensverteilung"'. Diese stellte er bereits in \textcite{Clark1891} weitgehend dar, bekannt wurde aber vor allem sein Hauptwerk \textcite{Clark1899}: "`The Distribution of Wealth"'. Die frühen Neoklassiker hatte mit ihrer Nutzentheorie vor allem die Nachfrageseite behandelt. Die Angebotsseite blieb auch bei \textcite{Marshall1890} unzureichend behandelt: Zwar gab es bei ihm schon den abnehmenden Grenzertrag bei der Produktion, allerdings noch keine Aussagen zur Verteilung der Produktionsfaktoren. Auch in der allgemeinen Gleichgewichtstheorie, bei Walras und auch bei Pareto war die Angebotsseite - also die Produktion - noch unbefriedigend dargestellt. Dies änderte sich mit dem Beitrag von John Bates Clark. Der aufmerksame Leser wird bemerken, dass auch einer der Vorläufer der Neoklassik vor allem die Produktionsseite betrachtet hat, nämlich Johann Heinrich von Thünen (vgl. Kapitel \ref{Vorläufer}). Clark war sich dessen bewusst und ging auch auf die Arbeit von Thünen ein, erkannte darin aber so manche Unzulänglichkeit \parencite[S. 31]{Tobin1985}. Zur Theorie der Grenzproduktivität selbst: Clark argumentierte, dass ein Unternehmen nur solange zusätzliche Arbeitskräfte einstellen wird, bis die letzte eingestellte Arbeitskraft einen zusätzlichen Gewinn liefert. Im Optimum entspricht der Grenzprodukt der Arbeit also der Lohnhöhe. Gleiches gilt für den zweiten Produktionsfaktor, das Kapital. Das Grenzprodukt des Kapitals entspricht den Kapitalkosten (Zinssatz)- 

Clark schuf damit im Wesentlichen für die Produktionsseite ein Äquivalent zur "`Grenzrate der Substitution"', die wir gerade bei Pareto kennen gelernt haben: Die Produktionsfaktoren werden solange gegeneinander ausgetauscht, bis sie alle das gleiche Grenzprodukt liefern. Das heißt ein weiterer Tausch nicht mehr zu höherem Gewinn führen. Dies erklärt also primär wie bei gegebenen Faktorpreisen die Produktionsfaktoren im optimalen Verhältnis zueinander eingesetzt werden. Das Ergebnis ist auch deshalb so interessant, weil als Nebenprodukt daraus die funktionale Einkommensverteilung erklärt wird.

Die funktionale Einkommensverteilung zeigt, welchen Anteil des Nationaleinkommens der Faktor Arbeit im Form von Löhnen und welchen Anteil der Faktor Kapital im Form von Unternehmensgewinnen erhält. In der klassischen Ökonomie ging man davon aus - was zur damaligen Zeit auch noch eine realistische Annahme war -, dass Arbeitnehmer ausschließlich Löhne als Einkünfte generieren, während Arbeitgeber ausschließlich Gewinne als Einkünfte erhalten. Die Diskussion um die funktionale Einkommensverteilung war seit jeher eine Diskussion um Fairness und Gerechtigkeit und eine der zentralen Streitfragen zwischen Klassischen Ökonomen und den Marxisten (vgl. Kapitel \ref{Klassik} und Kapitel \ref{Marx}). Die gerade vorgestellt neoklassische Grenzproduktivitätstheorie der Verteilung war nun insofern auch bahnbrechend anders, als sie die funktionale Einkommensverteilung als reines Ergebnis der Marktkräfte darstellt und nicht mehr - wie in der Klassik üblich - als Ergebnis politischer und wirtschaftlicher Marktverhältnisse. Im Vorwort von \textcite{Clark1899} heißt es dementsprechend auch: "`Die Einkommensverteilung in der Gesellschaft wird durch ein Naturgesetz bestimmt, wenn [der Markt] ohne Friktionen arbeiten kann, erhält jeder Agent im Produktionsprozess jenen Anteil am Wohlstand, den er selbst kreiert."' Clark war sich aber sehr wohl bewusst, dass die tatsächliche funktionale Einkommensverteilung ungerecht sein könnte, nämlich dann wenn die Voraussetzung der vollkommenen Konkurrenz auf Märkten nicht erfüllt ist. Er setzte sich daher stets dafür ein, dass der Staat primär dafür sorgen muss, dass es zu keiner Monopolbildung auf Märkten kommt \parencite{Clark1907}. Der Ansatz, dass vor allem mangelnde Konkurrenzverhältnisse zu schlechten Verteilungsergebnisse führen, spielt auch heute noch eine gewichtige Rolle und wird unter anderem von Philippe Aghion vertreten.

Man darf nicht vergessen, dass diese Grenzproduktivitätstheorie der Verteilung die selben strengen Annahmen voraussetzt, wie die bereits bekannten neoklassischen Ansätze: Neben den perfekten Konkurrenzmärkten auch die Homogenität der Produktionsfaktoren, sowie deren perfekte Substituierbarkeit. Das heißt insbesondere, das die einzelnen Arbeitnehmer gleich effizient sind und sowohl untereinander als auch gegen Kapitalgüter jederzeit austauschbar sind. Zudem werden konstante Skalenerträge angenommen. Das heißt eine gleichzeitige Verdoppelung aller Produktionsfaktoren führt zu einer Verdoppelung des Outputs. Gleichzeitig nahm \textcite{Clark1899} abnehmende Grenzerträge an - also unterproportional steigenden Output, wenn nur ein Produktionsfaktor steigt. Gerade den letzten Punkt nahm Clark als ein "`Naturgesetz"' hin, ohne einen Beweis für die Richtigkeit zu liefern, wie \textcite[S. 32]{Tobin1985} kritisiert. Diese mikroökonomische Theorie auf die gesamtwirtschaftliche Kennzahl "`funktionale Einkommensverteilung"' überzuführen ist dementsprechend problematisch. Die gerade aufgezeigten Voraussetzungen kann man wahrscheinlich für einzelne Branchen rechtfertigen, aber wohl kaum für eine gesamte Ökonomie. Dies wurde dann in den 1960er Jahren in der "`Kapitaltheoretischen Kontroverse"' auch bekannt als "`War of the two Cambridges"' thematisiert (vgl. Kapitel \ref{Post-Keynes}).

Insgesamt wurde das Werk von Clark kontrovers diskutiert. Seine Zeitgenossen in den USA, allen voran \textcite{Veblen1909} lehnten die Neoklassische Schule noch lange Zeit ab \parencite[S. 97]{Persky2000}. Die Grenzproduktivitätstheorie wird heute fast immer \textcite{Clark1899} zugewiesen, obwohl Knut Wicksell und Philip Wicksteed diese - unabhängig von ihm - ebenso entwickelt hatten. \textcite{Clark1891, Clark1899} stellte seine Ausführungen gänzlich ohne mathematische Formeln dar, was für Neoklassiker sehr untypisch ist. \textcite{Tobin1985} kritisiert, dass Clark eine ähnliche und sehr ausführliche Arbeit von Stuart Wood (1889), die ihm bekannt sein musste, gänzlich ignorierte. Die Grenzproduktivitätstheorie stieß aber auch die Türe auf für viele wichtige Forschungslinien des 20. Jahrhunderts. Die Produktionsfunktion von Cobb und Douglas der 1920er Jahre schloss in neoklassischer Tradition an Clark's Grenzproduktivitätsmodell an (vgl. Kapitel \ref{sec: Cobb-Douglas-Produktionsfunktion}). Die späteren  Wachstumsmodelle von Harrod und Domar auf keynesianische Seite, bzw. das Solow-Wachstumsmodell in neoklassischer Tradition, waren - wie \textcite{Tobin1985} schreibt - die selben "`Ein Produkt, zwei Faktoren"'-Modelle wie jenes von Clark.

Wenn Clark heute als der erste bedeutende amerikanische Ökonom angesehen wird, dann gilt aus heutiger Sicht Irving Fisher als der bedeutendste amerikanische Ökonom des frühen 20. Jahrhunderts. Fisher wurde zwanzig Jahre später als Clark geboren. Obwohl Clark erst recht spät, nämlich wie beschrieben mit über 50 Jahren sein Hauptwerk vorlag, stand Fisher zu dessen Lebzeiten eher im Schatten von Clark. Er hat von seinen Zeitgenossen weit weniger Beachtung erlangt, als nach seinem Ableben. In historischen Beiträgen wird lebhaft darüber spekuliert, warum Fisher - der heutzutage als moderner und bahnbrechender Ökonom angesehen wird - von seinen Zeitgenossen kaum als solcher wahrgenommen wird. Einzig \textcite[S. 872]{Schumpeter1954} prognostizierte, dass "`Zukünftige Historiker Fisher als größten aller bisherigen Amerikanischen Ökonomen"' feiern werden. Derselbe nannte auch zwei Gründe für die erst später aufkommende Bewunderung von Fisher's Werken: Erstens, seine "`verrückten"' Ideen - dazu gleich mehr und zweitens, seine mathematischen Ansätze, für damalige Zeit in den USA noch immer die absolute Ausnahme.

Was sicherlich stimmt, ist, dass Fisher - ebenso wie Marshall - als einer der ersten Wirtschaftswissenschaftler moderner Prägung angesehen werden kann und zwar in vielerlei Hinsicht. Zum einen verwendete er eine klare, einwandfreie Mathematik gepaart mit eindeutigen sprachlichen Formulierungen. Wie später in Kapitel \ref{FisherundKnight} dargestellt, gelten Eugen von Böhm-Bawerk und Irving Fisher als Vorläufer der modernen Finanzierungstheorie. Tatsächlich liefern deren Arbeiten ähnliche Inhalte. Vergleich man aber \textcite{BohmBawerk1888} und \textcite{Fisher1930} direkt miteinander, merkt man sofort die Überlegenheit und methodische Modernität des Letztgenannten \parencite[S. 33 ]{Tobin1985}. Er war aber auch einer der ersten Ökonomen, der nicht nur theoretische Ansätze schuf, sondern diese auch empirisch überprüfte. Dazu entwickelte er auch eigens statistische Verfahren. Außerdem war er nicht nur als Wissenschaftler und Universitätsprofessor tätig, sondern er vermarktete seien Ideen auch als Unternehmer und Erfinder mit mehreren Dutzend Arbeitnehmern und Büros in New York und Washington \parencite[S. 215]{Monissen1989}. Heute würde man ihn wohl als (ersten) Begründer eines "`Think Tanks"' bezeichnen. Seine Geschäftstätigkeit führte aber auch zu einem prägenden Rückschlag für Fisher: Noch Mitte Oktober 1929 beteuerte er, dass die "`Aktienmärkte ein permanent hohes Niveau"' erreicht hätten \parencite[S. 11]{Dimand2005}. Dieses Zitat sollte ihm lang anhaften. Auch in den Monaten nach dem "`Schwarzen Donnerstag"' am 24. Oktober 1929 und trotz der fortlaufenden Kursverlusten, beteuerte Fisher stets seinen Glauben an eine rasche Erholung der Märkte. Fisher's eigene Vermögensverluste bezifferte sein Sohn später auf "`acht bis zehn Millionen Dollar"' \parencite[S. 216]{Monissen1989}. \textcite[S. 30]{Tobin1985} schreibt sogar davon, dass die Universität Yale - an der er sowohl studierte, als auch später lehrte, "`sein Haus für ihn retten"' musste. 
Sein grandioser Fehlschlag bei der Einschätzung der "`Great Depression"' war eine jener "`Spinnereien"', die \textcite{Schumpeter1954} meinte und Fisher's öffentlichem Ansehen als Wissenschaftler stark schadete. Außerdem wurde er von manchen dadurch als Geschäftsmann und "`skrupelloser Spekulant diskreditiert"' \parencite[S. 216]{Monissen1989}. Fisher war aber auch in mancherlei anderer Hinsicht speziell. Mit 30 Jahren erkrankte er an Tuberkulose, überwand diese allerdings durch einen dreijährigen Kuraufenthalt. Danach beschäftigte und engagierte er sich für öffentliche Gesundheit. Er lehnte jegliche Genussmittel ab und war Vegetarier, aber auch überzeugter Eugeniker. Die Prohibition unterstützte er nicht nur, er verfasst auch Arbeiten, die deren Sinnhaftigkeit im Sinne daraus resultierender höherer Produktivität zeigen sollten. Später stellte man darin grobe Datenmängel fest \parencite[S. 215]{Monissen1989}. Außerdem unterstützte er die Idee des "`Schwundgeldes"', also Geld, das mit fortlaufender Zeit an Wert verlieren sollte, sowie 100\% gedecktes Geld, was die Geldschöpfung durch Geschäftsbanken unmöglich machen sollte \parencite[S. 30]{Tobin1985}.

Sein Erklärungsansatz zur "`Great Depression"', die "`Debt-Deflation-Theory"' \parencite{Fisher1933} reihte sich ein in die zahlreichen Krisenerklärungstheorien, die nach der "`Great Deprssion"' publiziert wurden. Der Ansatz ist durchaus interessant und wurde im zeitlichen Umfeld der "`Great Recession"' nach 2007 wieder verstärkt zitiert. \textcite{Fisher1933} argumentiert darin, dass durch die Deflation während der Krisenjahre Anfang der 1930er Jahre die realen  Schuldenwerte deutlich stiegen. Die Verbindlichkeiten von Schuldnern nahmen durch die Deflation also selbst bei aufrechter Rückzahlung real zu. Das führte zu verstärkten Ausfällen und verstärkte in weiterer Folge Kreditzinsen, Vertrauensverlust und schlussendlich die gesamtwirtschaftliche Entwicklung. Der Zeitgeist - in Form des Aufstiegs des Keynesianismus - bescherte der "`Debt-Deflation-Theorie"' aber ein Dasein als eher wenig beachtetes Werk.

Kommen wir endlich zu den erfolgreichen wissenschaftlichen Beiträgen Fisher's. Seine bewundernswerte, enorme Schaffenskraft wurde schon angedeutet, und tatsächlich war Fisher in verschiedenen Bereichen tätig. Sein Beitrag zur Nutzenmessung, den er mit seiner Dissertation \parencite{Fisher1892} vorlag, wurde schon erwähnt. Heute gilt vor allem seine Arbeiten zur Kapitaltheorie als wichtigster Beitrag. Dazu legte \textcite{Fisher1907} "`The Rate of Interest"' vor. Die Erweiterung \textcite{Fisher1930} "`The Theory of Interest"' wird noch heute zitiert. Diese Arbeiten begründen die "`intertemporale Konsumentscheidung"', sowie das sogenannten Fisher-Separations-Theorem. Beide Theoreme stellen wichtige Grundlagen bei der Betrachtung der modernen Finanzierungstheorie dar und werden in Kapitel \ref{FisherundKnight} detaillierter dargestellt.

Zu seinen Lebzeiten galt Fisher's Arbeit zur Quantitätstheorie des Geldes \textcite{Fisher1911} als sein Hauptwerk. Darin behandelt er diese in jener Form, die noch bis heute üblich ist. Sie wird uns noch des öfteren unterkommen, weil sie bis in die Gegenwart eine Rolle spielt. Die Quantitätsgleichung des Geldes wurde dabei schon viel früher sinngemäß formuliert und zwar bereits als erste Erklärungstheorie von Inflation. Im 16. Jahrhundert wurde in Spanien und Frankreich ein Preisanstieg bei praktische allen verfügbaren Waren beobachtet. Geld bestand damals in Europa noch fast ausschließlich aus Gold- oder Silbermünzen. Es wurde als reines Tauschmittel angesehen, dessen Wert als konstant angenommen wurde. Die erste Erklärung für einen Wertverlust bei Münzen war, dass der Edelmetall-Anteil der Münzen abgenommen hatte. Dieses Phänomen, dass Münzen bewusst verschlechtert wurden, indem man deren Edelmetall-Anteil senkte, war bereits damals bekannt. Der französische Gelehrte Jean Bodin erwiderte dieser Theorie im Jahr 1568, dass nicht die Münzverschlechterung an den Preissteigerungen Schuld war, sondern der hohe Import von Edelmetallen aus Amerika durch die Spanier \parencite{OBrien2000}. Es verschlechterte sich also nicht das Geld, sondern es erhöhte sich die Geldmenge. Der uns bereits bekannte Klassiker David Hume (vgl. \ref{Klassik}) formulierte schließlich die erste ausdrückliche Form der Quantitätsgleichung. Aber es war \textcite{Fisher1911} der die \textit{moderne} Form der Quantitätsgleichung formulierte und daraus eine Quantitäts\textit{theorie} des Geldes machte. Geld spielte bei den Klassikern nur als Tauschmittel eine Rolle und auch die frühen Neoklassiker hatten Geld nicht jene Bedeutung zugestanden, die es in der modernen Wirtschaftswissenschaft hat. Kurzum: Die Geldtheorie war am Anfang des 20. Jahrhunderts noch wenig entwickelt \parencite[S. 32]{ Tobin2005}. Weitgehend etabliert und unumstritten war Anfang des 20. Jahrhunderts die Bindung von Gold an ein Edelmetall - konkret der "`klassische Goldstandard"', der zwischen 1873 und 1914 gut funktionierte. Die Quantitätsgleichung des Geldes kann in diesem Sinn einfach als nicht widerlegbare Geldtheorie interpretiert werden. Die Formel wird bis heute als $$M*V = P*T$$ dargestellt \footnote{Die Urheberschaft dieser Formel ist häufig diskutiert. Inwieweit die Aussagen von Bodin und Hume schon als äquivalent zur Formel betrachtet werden können ist umstritten. Bekannt ist auf jeden Fall, dass Simon Newcomb, ein amerikanischer Astronom und Ökonom, die Formel bereits ausformuliert hatte \parencite[S. 33]{Tobin2005}.}. Die Geldmenge$M$ mal der Umlaufgeschwindigkeit$V$, also die Häufigkeit mit der Geld den Besitzer wechselt, entspricht dem Preisniveau$P$ mal der Summe aller Transaktionen$T$, heute könnte man dies als Bruttoinlandsprodukt bezeichnen. Dazu ein einfaches Beispiel: Stellen sie sich eine beliebige Ware vor. Angenommen die Geldmenge$M$ würde sich verdoppeln. Und zwar in dem Sinn, dass alle Geldbestände plötzlich verdoppelt wären. Wie würde sich dann der Preis$P_i$ dieser "`beliebigen Ware"' (also auch aller Waren$P$) entwickeln? Rein intuitiv antwortet jeder, dass sich deren Preis ebenso plötzlich verdoppeln würde. Man spricht in diesem Fall von "`klassischer Dichotomie"'. Die Mengenänderung des Geldes hat keine reale Auswirkung auf die Wirtschaft. Es ändert sich einfach das Preisniveau. Die Geldtheorie Fisher's geht aber über die Simple Identität im Sinne der Quantitätsgleichung hinaus. Er analysierte die Preise und Mengen der einzelnen Waren und wie diese gewichtet werden müssten um eine realistische Darstellung zu erlangen. Seine bekannten Vorarbeiten zu Indexzahlen und statistischen Methoden fanden hierbei Anwendung. Seine Fragestellungen zur Quantitätgleichung, die diese schließlich zur Quantitätstheorie machten, sind teilweise bis heute nicht unumstritten. So kann die Gesamt-Geldmenge einer Ökonomie durch eine zentrale Institution - normalerweise eine Zentralbank - nur dann direkt gesteuert werden, wenn Geld zu 100\% gedeckt ist. Dies ist eben in modernen Ökonomien nicht der Fall. Geschäftsbanken können durch Kreditvergabe neues Geld erschaffen. Erhöht diese Giralgeldschöpfung nun die Geldmenge, oder wird dadurch einfach die Umlaufgeschwindigkeit erhöht? Fisher plädiert für zweiteres. Die Giralgeldschöpfung erkennt Fisher als problematisch für die Anwendbarkeit der Quantitätsgleichung. Dadurch wird das Geldangebot exogen durch Institutionen - nämlich Geschäftsbanken - gesteuert, wenn diese nicht ihren Rahmen zur Giralgeldschöpfung schon vollkommen ausgenutzt haben \parencite[S. 35]{Tobin1985}. Interessant ist aber, dass ausgerechnet Fisher, für dessen Werk Zinsen \parencite{Fisher1906, Fisher1930} von zentraler Bedeutung waren, deren Bedeutung bei in der Interpretation der Quantitätstheorie völlig vernachlässigte \parencite{Tobin2005}. Schließlich können bestimmte Wertpapiere zum Preis eines gewissen Zinsaufwandes, wie Geldmittel verwendet werden. Dies wirkt sich auf die Geldmenge aus. So gilt es heute als umstritten, welches der Geldmengenaggregate (M0, M1, M2, M3)\footnote{Die Geldmengenaggregate bestimmen welche Wertpapiere als Geld bezeichnet werden. Die Geldmenge M0 zum Beispiel besteht ausschließlich aus gedrucktem Geld und Münzen, sowie Geldbeständen der Geschäftsbanken bei der Zentralbank. M3 umfasst auch bestimmte Wertpapiere, die innerhalb einer bestimmten Frist zu Geld gemacht, also als Zahlungsmittel dienen, können.} in der Quantitätsgleichung herangezogen werden soll. 
Was ist nun die \textit{Theorie} in \textcite{Fisher1911}? Er leitet aus der Quantitätsgleichung eine Zyklentheorie ab. Eine steigende Geldmenge würde demnach schon in der kurzen Frist zu steigenden Preisen, aber noch schneller steigenden Unternehmensgewinnen führen \parencite[S. 59]{Fisher1911}. In der Folge investieren die Unternehmen mehr und die Banken vergeben mehr Kredite, bis sich herausstellt, dass es zu einer Abweichung vom Gleichgewicht kommt und der Wirtschaftszyklus sich ins negative dreht. Die Auslöser werden bei Fisher nicht weiter hinterfragt, sondern "`ad hoc"' angenommen. Auch spielen, wie bereits erwähnt, die Zinssätze keine Rolle. Insgesamt wirkt das Ganze aus heutiger Sicht recht konstruiert. Aber diese Geldtheorie im Sinne der Quantitätstheorie, sowie die daraus abgeleitete Zyklentheorie wird als eine der ersten makroökonomischen Theorien angesehen\footnote{Ob die Theorien zur Quantitätstheorie des Geldes des frühen 20. Jahrhunderts, oder doch erst die Publikation von Keynes' "`General Theory"' als Geburtsstunde der modernen Makroökonomie angesehen werden sollen, wird im nächsten Unterkapitel kurz andiskutiert.}. Fisher wird damit häufig als Vorläufer zum Monetarismus (vgl. Kapitel \ref{Monetarismus}) bezeichnet. Allerdings leitet er weit weniger stark wirtschaftspolitische Empfehlungen aus seiner Quantitätsgleichung ab als später Milton Friedman.

Der Lebensweg des überaus talentierten Irving Fishers ist in so vielen Facetten interessant. Vor allem \textcite{Dimand2005} übertreibt es aber wohl etwas bei seiner Lobhymne auf Fisher. Demnach könnte man in seinen Arbeiten praktisch alle bahnbrechenden Konzepte der Ökonomie bei Fisher finden: Die "`Phillips-Kurve"' und die Bedeutung der Geldpolitik während der "`Great Depression"' \parencite[S. 7]{Dimand2005}. Überhaupt den ganzen Monetarismus, sowie eigentlich vieles der modernen Makroökonomie  \parencite[S. 10]{Dimand2005}. Selbst sein großes persönliches Waterloo, die Fehleinschätzung der Entwicklung der Kapitalmärkte Ende der 1920er Jahre wird von \textcite[S. 9]{Dimand2005} als Entdeckung des "`Equity Premium Puzzles"' - ein Paradoxon bei dem es darum geht, dass die Differenz zwischen risikolosen und risikobehafteten Renditen zu unterschiedlich ist - interpretiert. Etwas realistischer ist wohl die Einschätzung, dass er vor allem mit seinen Arbeiten zu Kapitaltheorie, insbesondere zur intertemporalen Konsumentscheidungen und zur Zinstheorie, tatsächlich bahnbrechende Beiträge lieferte. Dass diese bis heute häufig zitiert werden, zeugt davon, dass er seiner Zeit weit voraus war. Bemerkenswert ist insbesondere sein moderner Stil, der sich kaum vom heute üblichen unterscheidet und jenem seiner Zeitgenossen deutlich überlegen ist. Was seine Quantitätstheorie des Geldes angeht hatte er mit Knut Wicksell einen intellektuellen Gegenspieler, für den Fisher's Quantitätstheorie viel zu einfach formuliert war. Wicksell's Ideen werden im nächsten Kapitel behandelt.

\section{Wicksell: Eine überraschend moderne Theorie}

Johan Gustav Knut Wicksell wird allgemein als Begründer der "`Schwedischen Schule"' - häufig auch "`Stockholmer Schule"' genannt - angesehen. Deren frühe Vertreter, wie eben Wicksell, werden oftmals auch der "`Österreichischen Schule"' zugerechnet. Dies scheint im Hinblick auf die gleich dargestellte Bedeutung der Zinsen zwar stimmig, insgesamt waren die abgeleiteten wirtschaftspolitischen Empfehlungen der Österreicher doch wesentlich andere als jene der Schwedischen Schule. So war Knut Wicksell ein überzeugter Sozialdemokrat und er wurde als "`politisch links"' bezeichnet \parencite[S. 210, S. 192]{Grossekettler1989}. Für die Vertreter der Österreichischen Schule wäre es eine Beleidigung als politisch links zu gelten. Seinen neoklassischen Arbeiten fügte er hinzu, dass zwar die Einkommensverteilung in der neoklassischen Theorie vom Markt bestimmt sei, dass man dies aber in der Realität nicht akzeptieren müsse und durch Vermögens- und Einkommenssteuern eine gerechtere Verteilung anstreben sollte \parencite[S. 196]{Grossekettler1989}.In gewisser Hinsicht dürfte Wicksell recht radikal gewesen sein, so wurde er nach einem gotteslästerlichen Vortrag zu zwei Monaten Gefängnis verurteilt. Verheiratet war er mit Anna Bugge, die eine der ersten Frauen in bedeutenden Positionen in Schweden war und Kopf der dortigen Emanzipationsbewegung war \parencite[S. 191]{Grossekettler1989} Interessant ist, dass er seine Arbeiten primär in deutscher Sprache veröffentlichte. Seine Arbeiten im Rahmen der Neoklassik wurden "`wiederentdeckt"', weniger bekannt sind seine Werke als Vorläufer der Neuen Politischen Ökonomie (vgl. Kapitel \ref{Pol_Econ}), die erst durch deren Hauptvertreter James M. Buchanan analysiert wurden. 

Wicksell wird häufig als Gegenspieler von Fisher im Hinblick auf die Quantitätstheorie bezeichnet. Dabei war Wicksell d'accord mit deren Grundannahmen. Allerdings geht Wicksell eben einen Schritt weiter und berücksichtigt Zinsen in seiner Form der Konjunkturtheorie (Zyklentheorie). Konkret schafft er das Konzept des "`natürlichen Zinssatzes"', das noch heute verwendet und diskutiert wird. Heute gibt es leider viele moderne Abhandlungen von selbsternannten "`Ökonomen"', die das Konzept des Zinses als Ganzes in Frage stellen. Die Herleitung des natürlichen Zinssatzes von \textcite{Wicksel1898} - die er vor über 100 Jahren verfasst hat - ist im Vergleich dazu ein wahrer Wohlgenuss. Auch wenn diese Herleitung, nicht sehr präzise und formal eindeutig ist. Zudem ist sie deutlich zu vereinfacht und verwirrend dargestellt\footnote{Im Rahmen der bereits erwähnten "`Kapitaltheoretischen Kontroverse"' wurde festgehalten, dass aus einzelwirtschaftlichen Analysen nicht auf gesamtwirtschaftliche Zinsen schließen kann ("`Wicksell Effekt"')  (vgl. Kapitel \ref{Post-Keynes})} \parencite[S. 639]{Blaug1962}: Vom Sozialprodukt zieht man die Löhne, Unternehmerlöhne und Abgeltungen für die Bodenbesitzer ab. Der Rest ist die Vermehrung des Kapitals, davon muss man noch die Abschreibungen abziehen - also welcher Teil davon muss investiert werden um den Kapitalstock zu erhalten - und man kommt auf die Verzinsung des Kapitals (\textcite[S. 113ff]{Wicksel1898} nach \textcite[S. 407]{Rosner2012}). Dies ist die natürliche Verzinsung des realen Kapitals (natürlicher Zinssatz). Daneben betrachtete \textcite{Wicksel1898} aber zusätzlich den Finanzmarkt. Auf diesem wird ebenfalls ein Zinssatz gebildet, man könnte diesen den Marktzins nennen.
Diese beiden Zinssätze wirken auf unterschiedlichen Märkten. Bekannt war vor Wicksell bereits, dass höhere Zinsen die Produktionskosten erhöhen, weil der Faktor Kapital teurer wird. Aber es gibt auch die Wirkung der Zinsen über die Finanzmärkte. \textcite[S. 73f]{Wicksel1898} beschreibt, dass niedrigere Zinsen zu höherer Kreditnachfrage und in weiterer Folge höherer Güternachfrage und steigender Inflation führen. Damit schafft es Wicksell die Geldtheorie in die Konjunkturtheorie, die zuvor ausschließlich den Gütermarkt betrachtete, einzubinden \parencite[S. 5]{Blanchard2000}. Oder mit anderen Worten er schafft es die Kapitaltheorie mit der Grenzproduktivitätstheorie der Einkommensverteilung zu verbinden. 

Demnach ist die Gesamtwirtschaft im Gleichgewicht, wenn die Kapitalgeber (Unternehmer) tatsächlich mit dem natürlichen Zins entlohnt werden. Ist der Marktzins niedriger als der natürliche Zinssatz, so werden die Unternehmen die Produktion ausweiten und in weiterer Folge werden die Preise steigen. Ein Marktzins, der über dem natürlichen Zinssatz liegt führt zu sinkenden Preisen. Zinserhöhungen (Zinssenkungen) wirken also bereits bei Wicksell erst dann konjunkturdämpfend (konjunkturfördernd), wenn der Marktzinssatz höher (niedriger) als der natürliche Zinssatz ist\footnote{Zinserhöhungen (Zinssenkungen) isoliert betrachtet haben also noch keine konjunkturdämpfende (konjunkturfördernde) Wirkung. Dieser Effekt wirkt nur im Verhältnis zum natürlichen Zinssatz.}. Wicksell hat diese Konjunkturtheorie übrigens nicht abschließend in \textcite{Wicksel1898} dargestellt, sondern laufend erweitert, sodass man deren Gesamtausmaß nur aus der Literatur seines Hauptwerkes und \textcite{Wicksell1922}: "`Vorlesungen über Nationalökonomie, Band 2: Geld und Kredit"' vollständig erfasst werden kann. 

Der Ansatz wirkt gerade heute ungemein modern. Wo doch die meisten Zentralbanken dazu übergegangen sind die Konjunktursteuerung mittels Inflation-Targeting vorzunehmen und dafür mutmaßlich einer "`Taylor-Rule"' folgen (vgl. Kapitel \ref{Taylor}), die genau das Verhältnis zwischen Marktzins, natürlichen Zins, neben erwarteter und tatsächlicher Inflation beinhaltet. Auch die Überinvestitionstheorie von Hayek (vgl. Kapitel \ref{Austria}) zur Erklärung der "`Great Depression"' ähnelt dem Konzept stark \parencite[S. 201]{Grossekettler1989}. Man darf aber nicht vergessen, dass Wicksell's Arbeit "`Geldzins und Güterpreise"' bereits 1898 publiziert wurde. Also noch lange vor der "`Great Depression"' (1929 - 1933) und auch lange vor Keynes' "`General Theorie"' (1936), die schließlich das Konzept der aggregierten Nachfrage in den Vordergrund rückte (vgl. Kapitel \ref{Keynes}). Dementsprechend geht Wicksell auch nicht weiter auf die Nachfrage-seitigen Effekte der Zinsänderungen ein, bzw. auf die Möglichkeit, dass der Marktzinssatz über längere Zeit unter dem natürlichen Zinssatz liegen könnte und dementsprechend in größeren Wirtschaftskrisen münden könnte.

Interessant ist der Vergleich zwischen Fisher's Ansatz und jenem von Wicksell aus moderner makroökonomischer Sicht. Während die Quantitätstheorie von den Monetaristen um Milton Friedman wiederbelebt wurde (vgl. Kapitel \ref{Monetarismus}), erlebten die Ideen von Wicksell in der "`Neuen neoklassischen Synthese"' (vgl. Kapitel \ref{Neue Neoklassische Synthese}) eine Renaissance. Einer deren Hauptvertreter, Michael Woodford, geht sogar soweit, diese Schule als "`Neo-Wicksellianische Schule"' zu bezeichnen. Dies ist auch der Grund für die "`Wiederentdeckung"' Wicksell's Ende des 20. Jahrhunderts. Mit der aufkommen der DSGE-Modelle und insbesondere der "`Zinssteuerung"' durch Zentralbanken (vgl. Kapitel \ref{Neue Neoklassische Synthese}) wurden die frühen Konzepte vom Wicksell wieder verstärkt beachtet.

Von Mark Blaug, einem der profiliertesten Wirtschaftshistoriker des 20. Jahrhunderts, wurde Wicksell sogar als der Begründer der modernen Makroökonomie angesehen \parencite[S. 274]{Blaug1986}. Die meisten führenden Lehrbücher (\textcite[S. 13]{Snowdon2005}, \textcite[S. 795]{Blanchard2003}, \textcite[S. 29]{Samuelson1998}) sprechen diese Rolle allerdings uneingeschränkt John Maynard Keynes zu

In diesem Buch wird dem pragmatische Ansatz von \textcite{Blanchard2000} gefolgt, wenn es darum geht, wann den nun die Makroökonomie entstanden ist: Vor 1936 gab es natürlich schon makroökonomische Ansätze. Abgesehen von früheren Arbeiten durch Ricardo, Marx und anderen, waren Wicksell und Fisher die zwei dominierenden Köpfe in diesem Bereich. Insgesamt gab es vor allem zur Geldtheorie in Form der Interpretation der Quantitätstheorie, aber auch zur Zyklentheorie, Forschungsansätze, die aber kein einheitliches Rahmenwerk darstellten \parencite[S. 2f]{Blanchard2000}. \textcite[S. 1]{Blanchard2000} nennt diese Phase: "`Eine Periode der Entdeckungen, in der Makroökonomie noch nicht Makroökonomie war."' Deren Geburt folgte eben erst 1936 mit der Veröffentlichung von Keynes' "`General Theory"'. Auf dieses bahnbrechende Werk kommen wir in Kürze, nämlich in Kapitel \ref{Keynes} zu sprechen. 
